"""
geofractal.router.components.utility.aggregation_utility
========================================================

Aggregation utilities for combining walker interpolation steps.

Walker produces stepped outputs [B, S, T, D] where S is num_steps.
Aggregations combine these into final [B, T, D] output.
ALL aggregations are BaseUtility (static math, no learnable params).

Aggregation Types (all static):
    Simple:
        - MeanAggregation:     Simple average across steps
        - SumAggregation:      Sum (no normalization)
        - MaxAggregation:      Element-wise maximum
        - MinAggregation:      Element-wise minimum
        - FirstStepAggregation: Take first step only
        - LastStepAggregation:  Take last step only

    Weighted:
        - WeightedMeanAggregation: Alpha-weighted mean
        - TriangularAggregation:   Peak at center
        - TopKAggregation:         Average top-k steps by magnitude
        - BottomKAggregation:      Average bottom-k steps

    Softmax-based:
        - SoftmaxAggregation:  Softmax over step magnitudes
        - SoftminAggregation:  Softmin (favors lower magnitudes)
        - MinPAggregation:     Min-p threshold gating

    Similarity-based:
        - SimilarityAggregation:      Weight by similarity to reference
        - CrossSimilarityAggregation: Weight by avg similarity to all steps
        - SimilarityTreeAggregation:  Hierarchical similarity merging (WINNER)

    Interpolation:
        - SlerpAggregation: Spherical interpolation across steps

Learnable aggregation weights are generated by Inception layer.
See: router/components/walker_component.py

Mask Support:
    All aggregations support masking for fingerprint preservation:
    - mask=0: Position excluded from aggregation
    - mask=1: Position included fully
    - original: Tensor to preserve where mask=0

Copyright 2025 AbstractPhil
Licensed under the Apache License, Version 2.0
"""

from abc import abstractmethod
from typing import Optional, Dict
import math

import torch
import torch.nn.functional as F
from torch import Tensor

from geofractal.router.base_utility import BaseUtility

# NOTE: Aggregations are STATIC mathematical reductions - no learnable parameters.
# Learnable aggregation weights belong in Inception layer.
# See: router/components/walker_component.py


# =============================================================================
# MASK UTILITIES
# =============================================================================

def normalize_mask(mask: Tensor, target_shape: tuple, device, dtype) -> Tensor:
    """
    Normalize mask to target shape for broadcasting.

    Handles:
        - Expanding smaller masks to target shape
        - Reducing larger masks by averaging extra dimensions
        - Returning mask unchanged if already correct shape
    """
    mask = mask.to(device=device, dtype=dtype)

    # Already correct shape
    if mask.shape == target_shape:
        return mask

    target_dims = len(target_shape)

    # Mask is smaller - expand
    if mask.dim() < target_dims:
        while mask.dim() < target_dims:
            mask = mask.unsqueeze(-1)
        return mask.expand(target_shape)

    # Mask is larger - reduce by averaging leading extra dims
    while mask.dim() > target_dims:
        mask = mask.mean(dim=1)  # Average over steps dimension

    # Shape mismatch after reduction - try to match
    if mask.shape != target_shape:
        # Expand any remaining dimensions that need it
        if mask.dim() == target_dims:
            return mask.expand(target_shape)

    return mask


def apply_mask_blend(blended: Tensor, original: Tensor, mask: Tensor) -> Tensor:
    """Apply mask: 0=keep original, 1=use blended."""
    return mask * blended + (1 - mask) * original


# =============================================================================
# BASE AGGREGATION UTILITY
# =============================================================================

class AggregationUtility(BaseUtility):
    """
    Base class for aggregation utilities.

    Aggregates stepped walker outputs [B, S, T, D] → [B, T, D].

    S = num_steps (interpolation steps)
    T = sequence length
    D = features
    """

    aggregation_name: str = "base"

    def __init__(self, name: Optional[str] = None, **kwargs):
        super().__init__(name or self.aggregation_name, **kwargs)

    def _compute_scores(
        self,
        stepped: Tensor,
        alphas: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Compute per-step scores for weighted aggregation.

        Args:
            stepped: [B, S, T, D]
            alphas: [S] or [B, S]

        Returns:
            [B, S] scores (unnormalized)
        """
        B, S, T, D = stepped.shape
        return torch.ones(B, S, device=stepped.device, dtype=stepped.dtype)

    def _aggregate_impl(
        self,
        stepped: Tensor,
        weights: Tensor,
    ) -> Tensor:
        """
        Apply weighted aggregation.

        Args:
            stepped: [B, S, T, D]
            weights: [B, S] normalized weights

        Returns:
            [B, T, D]
        """
        weights = weights.unsqueeze(-1).unsqueeze(-1)  # [B, S, 1, 1]
        return (stepped * weights).sum(dim=1)

    def __call__(
        self,
        stepped: Tensor,
        alphas: Optional[Tensor] = None,
        mask: Optional[Tensor] = None,
        original: Optional[Tensor] = None,
    ) -> Tensor:
        """
        Aggregate stepped outputs.

        Args:
            stepped: [B, S, T, D] stepped interpolation outputs
            alphas: [S] or [B, S] alpha values (for weighted methods)
            mask: Aggregation mask where 0=exclude, 1=include
            original: [B, T, D] for fingerprint preservation

        Returns:
            [B, T, D] aggregated output
        """
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        # Compute scores and normalize
        scores = self._compute_scores(stepped, alphas)  # [B, S]

        # Apply step-level masking
        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            step_mask = mask_norm.mean(dim=(-1, -2))  # [B, S]
            scores = scores * step_mask

        # Normalize weights
        weights = scores / scores.sum(dim=1, keepdim=True).clamp_min(1e-8)

        # Aggregate
        result = self._aggregate_impl(stepped, weights)

        # Apply position-level masking for fingerprint preservation
        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# MEAN AGGREGATION
# =============================================================================

class MeanAggregation(AggregationUtility):
    """Simple mean across steps."""

    aggregation_name = "mean"

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape
        return torch.ones(B, S, device=stepped.device, dtype=stepped.dtype)


# =============================================================================
# SUM AGGREGATION
# =============================================================================

class SumAggregation(AggregationUtility):
    """Sum across steps (no normalization)."""

    aggregation_name = "sum"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            stepped = stepped * mask_norm

        result = stepped.sum(dim=1)

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# MAX AGGREGATION
# =============================================================================

class MaxAggregation(AggregationUtility):
    """Element-wise maximum across steps."""

    aggregation_name = "max"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            stepped = stepped.masked_fill(mask_norm < 0.5, float('-inf'))

        result = stepped.max(dim=1).values
        result = torch.where(torch.isinf(result), torch.zeros_like(result), result)

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# MIN AGGREGATION
# =============================================================================

class MinAggregation(AggregationUtility):
    """Element-wise minimum across steps."""

    aggregation_name = "min"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            stepped = stepped.masked_fill(mask_norm < 0.5, float('inf'))

        result = stepped.min(dim=1).values
        result = torch.where(torch.isinf(result), torch.zeros_like(result), result)

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# FIRST/LAST STEP AGGREGATION
# =============================================================================

class FirstStepAggregation(AggregationUtility):
    """Take only the first step."""

    aggregation_name = "first"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        result = stepped[:, 0]

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


class LastStepAggregation(AggregationUtility):
    """Take only the last step (default walker behavior)."""

    aggregation_name = "last"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        result = stepped[:, -1]

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# WEIGHTED MEAN AGGREGATION
# =============================================================================

class WeightedMeanAggregation(AggregationUtility):
    """Alpha-weighted mean across steps."""

    aggregation_name = "weighted_mean"

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        if alphas is None:
            return torch.ones(B, S, device=stepped.device, dtype=stepped.dtype)

        if alphas.dim() == 1:
            return alphas.unsqueeze(0).expand(B, -1)
        return alphas


# =============================================================================
# TRIANGULAR AGGREGATION
# =============================================================================

class TriangularAggregation(AggregationUtility):
    """
    Triangular weighting - peak at center, tapering to edges.

    Good for emphasizing middle interpolation steps.
    """

    aggregation_name = "triangular"

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape
        positions = torch.linspace(0, 1, S, device=stepped.device, dtype=stepped.dtype)
        weights = torch.minimum(positions, 1 - positions) * 2
        return weights.unsqueeze(0).expand(B, -1)


# =============================================================================
# TOP-K / BOTTOM-K AGGREGATION
# =============================================================================

class TopKAggregation(AggregationUtility):
    """
    Average top-k steps by magnitude.

    Selects steps with highest mean absolute value.
    """

    aggregation_name = "top_k"

    def __init__(self, k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.k = k

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        k = min(self.k, S)

        # Compute step magnitudes
        magnitudes = stepped.abs().mean(dim=(-1, -2))  # [B, S]

        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            step_mask = mask_norm.mean(dim=(-1, -2))
            magnitudes = magnitudes * step_mask

        # Get top-k indices
        _, indices = magnitudes.topk(k, dim=1)  # [B, k]

        # Gather and average
        indices_expanded = indices.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, T, D)
        selected = stepped.gather(1, indices_expanded)  # [B, k, T, D]
        result = selected.mean(dim=1)

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


class BottomKAggregation(AggregationUtility):
    """
    Average bottom-k steps by magnitude.

    Selects steps with lowest mean absolute value.
    """

    aggregation_name = "bottom_k"

    def __init__(self, k: int = 3, **kwargs):
        super().__init__(**kwargs)
        self.k = k

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        k = min(self.k, S)

        magnitudes = stepped.abs().mean(dim=(-1, -2))  # [B, S]

        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            step_mask = mask_norm.mean(dim=(-1, -2))
            magnitudes = magnitudes.masked_fill(step_mask < 0.5, float('inf'))

        # Get bottom-k indices (smallest magnitudes)
        _, indices = magnitudes.topk(k, dim=1, largest=False)

        indices_expanded = indices.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, T, D)
        selected = stepped.gather(1, indices_expanded)
        result = selected.mean(dim=1)

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# SOFTMAX/SOFTMIN AGGREGATION
# =============================================================================

class SoftmaxAggregation(AggregationUtility):
    """
    Softmax-weighted aggregation over step magnitudes.

    Higher magnitude steps get more weight.
    """

    aggregation_name = "softmax"

    def __init__(self, temperature: float = 1.0, **kwargs):
        super().__init__(**kwargs)
        self.temperature = temperature

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        magnitudes = stepped.abs().mean(dim=(-1, -2))  # [B, S]
        scores = F.softmax(magnitudes / self.temperature, dim=1)

        # Return unnormalized so base class normalizes (already normalized here)
        return scores


class SoftminAggregation(AggregationUtility):
    """
    Softmin-weighted aggregation.

    Lower magnitude steps get more weight.
    """

    aggregation_name = "softmin"

    def __init__(self, temperature: float = 1.0, **kwargs):
        super().__init__(**kwargs)
        self.temperature = temperature

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        magnitudes = stepped.abs().mean(dim=(-1, -2))
        scores = F.softmax(-magnitudes / self.temperature, dim=1)

        return scores


# =============================================================================
# MIN-P AGGREGATION
# =============================================================================

class MinPAggregation(AggregationUtility):
    """
    Min-P threshold gating aggregation.

    Only includes steps whose magnitude exceeds threshold
    relative to maximum magnitude.
    """

    aggregation_name = "min_p"

    def __init__(self, threshold: float = 0.1, **kwargs):
        super().__init__(**kwargs)
        self.threshold = threshold

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        magnitudes = stepped.abs().mean(dim=(-1, -2))  # [B, S]
        max_mag = magnitudes.max(dim=1, keepdim=True).values.clamp_min(1e-8)

        # Gate: only include steps above threshold
        gate = (magnitudes > self.threshold * max_mag).float()

        return gate


# =============================================================================
# SIMILARITY AGGREGATION
# =============================================================================

class SimilarityAggregation(AggregationUtility):
    """
    Similarity-weighted aggregation.

    Weights each step by cosine similarity to reference.
    Higher similarity to reference = higher weight.
    """

    aggregation_name = "similarity"

    def __init__(self, reference: str = 'last', **kwargs):
        """
        Args:
            reference: 'first', 'last', or 'mean'
        """
        super().__init__(**kwargs)
        self.reference = reference

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        if self.reference == 'first':
            ref = stepped[:, :1]
        elif self.reference == 'mean':
            ref = stepped.mean(dim=1, keepdim=True)
        else:  # 'last'
            ref = stepped[:, -1:]

        stepped_flat = stepped.view(B, S, -1)
        ref_flat = ref.view(B, 1, -1)

        # Cosine similarity: [-1, 1] → map to [0, 1] for positive weights
        cos_sim = F.cosine_similarity(stepped_flat, ref_flat, dim=-1, eps=1e-8)
        scores = (cos_sim + 1) / 2

        return scores


class CrossSimilarityAggregation(AggregationUtility):
    """
    Cross-similarity aggregation.

    Each step weighted by average similarity to all other steps.
    Higher similarity to others = higher weight.
    """

    aggregation_name = "cross_similarity"

    def _compute_scores(self, stepped, alphas=None):
        B, S, T, D = stepped.shape

        stepped_flat = stepped.view(B, S, -1)
        stepped_norm = F.normalize(stepped_flat, dim=-1, eps=1e-8)

        # All-pairs similarity: [B, S, S]
        sim_matrix = torch.bmm(stepped_norm, stepped_norm.transpose(1, 2))

        # Average similarity to other steps (exclude self)
        eye_mask = 1 - torch.eye(S, device=stepped.device, dtype=stepped.dtype).unsqueeze(0)
        avg_sim = (sim_matrix * eye_mask).sum(dim=-1) / max(S - 1, 1)  # [B, S]

        # Map from [-1, 1] to [0, 1] for positive weights
        scores = (avg_sim + 1) / 2

        return scores


# =============================================================================
# SIMILARITY TREE AGGREGATION (WINNER CONFIG)
# =============================================================================

class SimilarityTreeAggregation(AggregationUtility):
    """
    Hierarchical similarity-based aggregation.

    Recursively merges most similar adjacent pairs until single output.
    Part of the winning preset: shiva + cosine + similarity_tree (98.21%)

    Fully vectorized - uses log2(S) merge operations.
    """

    aggregation_name = "similarity_tree"

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        # Apply mask to input if provided
        if mask is not None:
            mask_norm = normalize_mask(mask, stepped.shape, device, dtype)
            stepped = stepped * mask_norm

        # Simple case: 1 or 2 steps
        if S == 1:
            result = stepped[:, 0]
        elif S == 2:
            # Similarity-weighted merge of 2 steps
            flat = stepped.view(B, S, -1)
            sim = F.cosine_similarity(flat[:, 0], flat[:, 1], dim=-1, eps=1e-8)
            w = (sim + 1) / 2  # Map [-1, 1] to [0, 1]
            w = w.view(B, 1, 1)
            result = stepped[:, 0] * w + stepped[:, 1] * (1 - w)
        else:
            # Pad to power of 2 for clean binary tree merging
            next_pow2 = 2 ** math.ceil(math.log2(S))
            if S < next_pow2:
                # Pad with last step repeated
                pad_count = next_pow2 - S
                padding = stepped[:, -1:].expand(B, pad_count, T, D)
                current = torch.cat([stepped, padding], dim=1)
            else:
                current = stepped

            n = current.shape[1]

            # Binary tree reduction: log2(n) levels
            while n > 1:
                # Reshape to pairs: [B, n//2, 2, T, D]
                n_pairs = n // 2
                pairs = current.view(B, n_pairs, 2, T, D)

                # Compute similarity between each pair
                # Flatten spatial dims for cosine similarity
                flat_pairs = pairs.view(B, n_pairs, 2, -1)  # [B, n_pairs, 2, T*D]
                sims = F.cosine_similarity(
                    flat_pairs[:, :, 0],  # [B, n_pairs, T*D]
                    flat_pairs[:, :, 1],  # [B, n_pairs, T*D]
                    dim=-1, eps=1e-8
                )  # [B, n_pairs]

                # Convert similarity to weight: higher sim = more equal blend
                # sim=1 → w=0.5, sim=0 → w=0.5, sim=-1 → w=0.5
                # Actually use sim as weight toward first element
                w = (sims + 1) / 2  # Map [-1, 1] to [0, 1]
                w = w.view(B, n_pairs, 1, 1)  # [B, n_pairs, 1, 1]

                # Weighted merge of pairs
                current = pairs[:, :, 0] * w + pairs[:, :, 1] * (1 - w)  # [B, n_pairs, T, D]
                n = n_pairs

            result = current.squeeze(1)  # [B, T, D]

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# SLERP AGGREGATION
# =============================================================================

class SlerpAggregation(AggregationUtility):
    """
    Spherical interpolation aggregation.

    Progressively slerps from first to last step.
    """

    aggregation_name = "slerp"

    def __init__(self, eps: float = 1e-8, **kwargs):
        super().__init__(**kwargs)
        self.eps = eps

    def __call__(self, stepped, alphas=None, mask=None, original=None):
        B, S, T, D = stepped.shape
        device, dtype = stepped.device, stepped.dtype

        # Progressive slerp: start from first, interpolate toward each subsequent
        result = stepped[:, 0]

        for i in range(1, S):
            alpha = i / (S - 1) if S > 1 else 1.0
            target = stepped[:, i]

            # Normalize
            result_norm = F.normalize(result, dim=-1, eps=self.eps)
            target_norm = F.normalize(target, dim=-1, eps=self.eps)

            # Angle
            dot = (result_norm * target_norm).sum(dim=-1, keepdim=True).clamp(-1 + self.eps, 1 - self.eps)
            theta = torch.acos(dot)
            sin_theta = torch.sin(theta) + self.eps

            # Slerp
            w1 = torch.sin((1 - alpha) * theta) / sin_theta
            w2 = torch.sin(alpha * theta) / sin_theta

            # Preserve magnitudes
            result_mag = result.norm(dim=-1, keepdim=True)
            target_mag = target.norm(dim=-1, keepdim=True)
            new_mag = result_mag * (1 - alpha) + target_mag * alpha

            result = (w1 * result_norm + w2 * target_norm) * new_mag

        if mask is not None and original is not None:
            pos_mask = normalize_mask(mask, (B, T, D), device, dtype)
            result = apply_mask_blend(result, original, pos_mask)

        return result


# =============================================================================
# NOTE: Learnable aggregation variants belong in inception layer, not utilities.
# Utilities are STATIC mathematical primitives.
# See: router/components/walker_component.py for aux-modulated aggregations
# =============================================================================


# =============================================================================
# FACTORY
# =============================================================================

AGGREGATION_REGISTRY: Dict[str, type] = {
    'mean': MeanAggregation,
    'sum': SumAggregation,
    'max': MaxAggregation,
    'min': MinAggregation,
    'first': FirstStepAggregation,
    'last': LastStepAggregation,
    'weighted_mean': WeightedMeanAggregation,
    'weighted': WeightedMeanAggregation,  # alias
    'triangular': TriangularAggregation,
    'top_k': TopKAggregation,
    'bottom_k': BottomKAggregation,
    'softmax': SoftmaxAggregation,
    'softmin': SoftminAggregation,
    'min_p': MinPAggregation,
    'similarity': SimilarityAggregation,
    'cross_similarity': CrossSimilarityAggregation,
    'similarity_tree': SimilarityTreeAggregation,
    'slerp': SlerpAggregation,
}


def create_aggregation(name: str, **kwargs) -> AggregationUtility:
    """
    Create static aggregation by name.

    All aggregations are static mathematical reductions.
    Learnable aggregation weights are generated by WalkerInception.
    See: router/components/walker_component.py
    """
    if name not in AGGREGATION_REGISTRY:
        raise ValueError(f"Unknown aggregation: {name}. Available: {list(AGGREGATION_REGISTRY.keys())}")
    return AGGREGATION_REGISTRY[name](**kwargs)


# =============================================================================
# TESTS
# =============================================================================

if __name__ == '__main__':

    def section(title):
        print(f"\n{'=' * 60}")
        print(f"  {title}")
        print('=' * 60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    section("AGGREGATION INSTANTIATION")

    mean_agg = MeanAggregation()
    last_agg = LastStepAggregation()
    print(f"MeanAggregation: {mean_agg}")
    print(f"LastStepAggregation: {last_agg}")

    section("BASIC AGGREGATION")

    B, S, T, D = 4, 8, 16, 64
    stepped = torch.randn(B, S, T, D, device=device)

    print(f"Input shape: {stepped.shape}")
    print(f"{'Aggregation':<18s} | Output shape | Mean | Std")
    print("-" * 60)

    for name in ['mean', 'sum', 'max', 'min', 'first', 'last']:
        agg = create_aggregation(name)
        result = agg(stepped)
        print(f"{name:<18s} | {str(result.shape):<12s} | {result.mean().item():>6.3f} | {result.std().item():.3f}")

    section("WEIGHTED AGGREGATIONS")

    alphas = torch.linspace(0, 1, S, device=device)

    for name in ['weighted_mean', 'triangular']:
        agg = create_aggregation(name)
        result = agg(stepped, alphas=alphas)
        print(f"{name:<18s} | mean={result.mean().item():.3f}")

    section("TOP-K / BOTTOM-K")

    for k in [2, 4]:
        top_k = create_aggregation('top_k', k=k)
        bottom_k = create_aggregation('bottom_k', k=k)

        top_result = top_k(stepped)
        bottom_result = bottom_k(stepped)

        print(f"top_{k}: mean={top_result.mean().item():.3f}, std={top_result.std().item():.3f}")
        print(f"bottom_{k}: mean={bottom_result.mean().item():.3f}, std={bottom_result.std().item():.3f}")

    section("SOFTMAX / SOFTMIN / MIN-P")

    for name in ['softmax', 'softmin', 'min_p']:
        agg = create_aggregation(name)
        result = agg(stepped)
        print(f"{name:<12s} | mean={result.mean().item():.3f}")

    section("SIMILARITY-BASED")

    for ref in ['first', 'last', 'mean']:
        agg = create_aggregation('similarity', reference=ref)
        result = agg(stepped)
        print(f"similarity({ref:<5s}) | mean={result.mean().item():.3f}")

    cross_sim = create_aggregation('cross_similarity')
    result = cross_sim(stepped)
    print(f"cross_similarity    | mean={result.mean().item():.3f}")

    section("SLERP AGGREGATION")

    slerp = create_aggregation('slerp')
    result = slerp(stepped)
    print(f"slerp | shape={result.shape}, mean={result.mean().item():.3f}")

    section("MASK SUPPORT")

    original = torch.zeros(B, T, D, device=device)
    mask = torch.ones(B, S, T, D, device=device)
    mask[:, :, :T//2, :] = 0  # Mask out first half of sequence

    mean_agg = create_aggregation('mean')
    result = mean_agg(stepped, mask=mask, original=original)

    print(f"With mask (first half = 0):")
    print(f"  Result first half mean: {result[:, :T//2].abs().mean().item():.6f} (should be ~0)")
    print(f"  Result second half mean: {result[:, T//2:].abs().mean().item():.3f} (should be non-zero)")

    section("SIMILARITY TREE AGGREGATION (Winner Config)")

    sim_tree = create_aggregation('similarity_tree')
    result = sim_tree(stepped)
    print(f"similarity_tree | shape={result.shape}, mean={result.mean().item():.3f}")
    print(f"  (Hierarchical pairwise merging based on cosine similarity)")

    section("NOTE: LEARNABLE AGGREGATIONS")

    print("Learnable aggregation weights are generated by the Inception layer,")
    print("not stored in utility classes. See: router/components/walker_component.py")
    print("  - WalkerInception.compute_agg_weights(aux, stepped) → [B, S] weights")

    section("ALL AGGREGATIONS")

    print(f"{'Aggregation':<18s} | Mean | Std")
    print("-" * 40)
    for name in AGGREGATION_REGISTRY:
        agg = create_aggregation(name)
        try:
            result = agg(stepped)
            print(f"{name:<18s} | {result.mean().item():>6.3f} | {result.std().item():.3f}")
        except Exception as e:
            print(f"{name:<18s} | ERROR: {e}")

    section("ALL TESTS PASSED")