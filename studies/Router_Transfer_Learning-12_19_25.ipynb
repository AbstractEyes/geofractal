{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "FnSKpuKIG4V6",
        "2vajKVjtIUj6",
        "OvNNSDuoP6Zv",
        "B9XO-3qCTWlu",
        "CWbgylj4oqE8",
        "MLqW2rrjotql",
        "qNGyPmKhsLDK",
        "LHYpUlaCsNqd",
        "lZFLktRL-gjs",
        "640jOqQHA2RK",
        "vpBbtMymPvK-",
        "PKr4pkuhU7jS",
        "buqt9zeYjuoo",
        "ag-vfpYox8gq",
        "4I6ZJ5WvDmMh",
        "n8uc7Qe3cpI8",
        "WqeCR2XnQ96d",
        "0joFOO5x50qL",
        "-2_GU3rHCKub"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93623e56458041f8a5f3ea1184525b10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d996bfbf08134faca1ac87d5f0707cab",
              "IPY_MODEL_3970bfb7034f4a308c9cbc1f0edc9919",
              "IPY_MODEL_e62b34cde51e424c8673337d85cff1f4"
            ],
            "layout": "IPY_MODEL_276b330ece8f4e2497214890d31a6216"
          }
        },
        "d996bfbf08134faca1ac87d5f0707cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_687cc8ab87fc4021bbf565a9b2f9c968",
            "placeholder": "​",
            "style": "IPY_MODEL_e00b52c7c93348588211881521383738",
            "value": "tokenizer_config.json: "
          }
        },
        "3970bfb7034f4a308c9cbc1f0edc9919": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e0de8e72a24bb78afc35163c620b26",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cff9d83cb8064af2b970374ccfe247db",
            "value": 1
          }
        },
        "e62b34cde51e424c8673337d85cff1f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82199acdbf2b41a09f792981496e17a2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a83271d941240edbf87c566e1483b36",
            "value": " 2.54k/? [00:00&lt;00:00, 227kB/s]"
          }
        },
        "276b330ece8f4e2497214890d31a6216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "687cc8ab87fc4021bbf565a9b2f9c968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00b52c7c93348588211881521383738": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6e0de8e72a24bb78afc35163c620b26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cff9d83cb8064af2b970374ccfe247db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82199acdbf2b41a09f792981496e17a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a83271d941240edbf87c566e1483b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea1c21ddc007443cb5032c1a0f1e1767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_949a0eb93ac94e44b5c93f54e49077d7",
              "IPY_MODEL_b3ca7540c13d435d8fa18c251a2dedd8",
              "IPY_MODEL_077ae272e6a84974be005ab79ab73854"
            ],
            "layout": "IPY_MODEL_8afe58320d514057a307a772ce824dd3"
          }
        },
        "949a0eb93ac94e44b5c93f54e49077d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea52fc950d864673b5d5e13499849f4c",
            "placeholder": "​",
            "style": "IPY_MODEL_87d1a943c54547fa8c0a92a3d920a88b",
            "value": "spiece.model: 100%"
          }
        },
        "b3ca7540c13d435d8fa18c251a2dedd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d12a934d319642d4ba434058f980dacd",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56e64e6200d64bb59c2725846e28f5a4",
            "value": 791656
          }
        },
        "077ae272e6a84974be005ab79ab73854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c9326cc60dd44cc90ae43608b523464",
            "placeholder": "​",
            "style": "IPY_MODEL_761f096f066c453081450c5b1ab84344",
            "value": " 792k/792k [00:01&lt;00:00, 666kB/s]"
          }
        },
        "8afe58320d514057a307a772ce824dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea52fc950d864673b5d5e13499849f4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87d1a943c54547fa8c0a92a3d920a88b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d12a934d319642d4ba434058f980dacd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56e64e6200d64bb59c2725846e28f5a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c9326cc60dd44cc90ae43608b523464": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "761f096f066c453081450c5b1ab84344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b7ac4d04de5e453ba3bc5fe9a06f3ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db3f8a3caa6a4f6fa21b80d31fc0f878",
              "IPY_MODEL_91f5f3d150034d0092c73b4cc73cb9bf",
              "IPY_MODEL_e90df34f8918438d8b5e529c965e1aad"
            ],
            "layout": "IPY_MODEL_3e94ec3248fc4509a34a05fd3934710f"
          }
        },
        "db3f8a3caa6a4f6fa21b80d31fc0f878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20a9082106544500a2c4f387fe0ed05c",
            "placeholder": "​",
            "style": "IPY_MODEL_11dbbce0789746a7a0b0fdd724033ead",
            "value": "special_tokens_map.json: "
          }
        },
        "91f5f3d150034d0092c73b4cc73cb9bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b6dc87c0dd040c19e216d4febb165a4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_624a6d97423c4df6aed87c603a5b58ab",
            "value": 1
          }
        },
        "e90df34f8918438d8b5e529c965e1aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ccc4325f6224db78097770f7125b3de",
            "placeholder": "​",
            "style": "IPY_MODEL_cebab80c7cdd4d39a8a61d9d51f2c09a",
            "value": " 2.20k/? [00:00&lt;00:00, 238kB/s]"
          }
        },
        "3e94ec3248fc4509a34a05fd3934710f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a9082106544500a2c4f387fe0ed05c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11dbbce0789746a7a0b0fdd724033ead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b6dc87c0dd040c19e216d4febb165a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "624a6d97423c4df6aed87c603a5b58ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0ccc4325f6224db78097770f7125b3de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cebab80c7cdd4d39a8a61d9d51f2c09a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7858ec8a37bc4b83972334f52965ece3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_107b2054347440b380b602c03ba59e65",
              "IPY_MODEL_f592738cf9cc4139a977d60a373d9022",
              "IPY_MODEL_5b68e12ed6e94fff9d80102d5427b13a"
            ],
            "layout": "IPY_MODEL_eb73c8bb34264f31944b593174946647"
          }
        },
        "107b2054347440b380b602c03ba59e65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d759220e474f088ea0e92522c77c6b",
            "placeholder": "​",
            "style": "IPY_MODEL_3673bee51ef34e20b89acfbd992833a5",
            "value": "tokenizer.json: "
          }
        },
        "f592738cf9cc4139a977d60a373d9022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3bcc1254fcc14992bf3d44feb29a9b1d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5620d9e5484f42e9be10ef400f53c623",
            "value": 1
          }
        },
        "5b68e12ed6e94fff9d80102d5427b13a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03ab363c3704b90844811aa1b773c41",
            "placeholder": "​",
            "style": "IPY_MODEL_39f0326be81a405692f8f50391a2bc70",
            "value": " 2.42M/? [00:00&lt;00:00, 3.69MB/s]"
          }
        },
        "eb73c8bb34264f31944b593174946647": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d759220e474f088ea0e92522c77c6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3673bee51ef34e20b89acfbd992833a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3bcc1254fcc14992bf3d44feb29a9b1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5620d9e5484f42e9be10ef400f53c623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f03ab363c3704b90844811aa1b773c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f0326be81a405692f8f50391a2bc70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50945a476fd149fc8665b8bfb7424b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e09deb1b562e4ea6b0df75b1604ea7a9",
              "IPY_MODEL_fc03de57e1d74a57825793e630881aa6",
              "IPY_MODEL_ed681c4b8a9241a2a5e7399e1034adaf"
            ],
            "layout": "IPY_MODEL_dcc3664e6c334315920f16704d7afd31"
          }
        },
        "e09deb1b562e4ea6b0df75b1604ea7a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_028efddb88ff42c3aa18e494a39f3823",
            "placeholder": "​",
            "style": "IPY_MODEL_663fa8918b4e4c70b2ad32f54306339a",
            "value": "config.json: "
          }
        },
        "fc03de57e1d74a57825793e630881aa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_081016f140694f3abe9651d6e071f1c3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3aad3cc1f7b408e8cb20b1d92561786",
            "value": 1
          }
        },
        "ed681c4b8a9241a2a5e7399e1034adaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84b31d8d40ee4cfc9afdc526fb83b496",
            "placeholder": "​",
            "style": "IPY_MODEL_a65368991ca94f5e83a8e85809e7f3df",
            "value": " 1.40k/? [00:00&lt;00:00, 178kB/s]"
          }
        },
        "dcc3664e6c334315920f16704d7afd31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028efddb88ff42c3aa18e494a39f3823": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "663fa8918b4e4c70b2ad32f54306339a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "081016f140694f3abe9651d6e071f1c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c3aad3cc1f7b408e8cb20b1d92561786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "84b31d8d40ee4cfc9afdc526fb83b496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a65368991ca94f5e83a8e85809e7f3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b305a3692e77402f81262cd1961e764e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43651cfb4dfa4577abf22009e12665d4",
              "IPY_MODEL_332ee6707bb04d16a64dc807c41063e2",
              "IPY_MODEL_a1b4e30f308941698a5fb1d0bfe53314"
            ],
            "layout": "IPY_MODEL_6c3ae5a0aad241c0a6c063f5a5da548b"
          }
        },
        "43651cfb4dfa4577abf22009e12665d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_331ee2ebdc56489c8c828da5412b2510",
            "placeholder": "​",
            "style": "IPY_MODEL_21cbf771b6d84787b085a5c6d6680747",
            "value": "model.safetensors: 100%"
          }
        },
        "332ee6707bb04d16a64dc807c41063e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26f31e2951fd466591736429abbb7cda",
            "max": 990345061,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd96306970ee4a0894267ac0fce91c71",
            "value": 990345061
          }
        },
        "a1b4e30f308941698a5fb1d0bfe53314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_739cb7090cad45418228941b968232a1",
            "placeholder": "​",
            "style": "IPY_MODEL_f8cad35261854c7abd8cdc90b9eb0901",
            "value": " 990M/990M [00:01&lt;00:00, 880MB/s]"
          }
        },
        "6c3ae5a0aad241c0a6c063f5a5da548b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "331ee2ebdc56489c8c828da5412b2510": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21cbf771b6d84787b085a5c6d6680747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26f31e2951fd466591736429abbb7cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd96306970ee4a0894267ac0fce91c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "739cb7090cad45418228941b968232a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8cad35261854c7abd8cdc90b9eb0901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5aa5506bc3c4efaa194901b4ed7b750": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b6b08ca1268d4396bd0a337c7f5621cd",
              "IPY_MODEL_08a8d150e2c14477b4617021172078b0",
              "IPY_MODEL_86118495fec54945bb926647716d657c"
            ],
            "layout": "IPY_MODEL_1f73298db35a4ee9990556a4d95bdc44"
          }
        },
        "b6b08ca1268d4396bd0a337c7f5621cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db93187a4d8a4c4d9fa6ba372387be98",
            "placeholder": "​",
            "style": "IPY_MODEL_9404cd41b3be4564bb50310a1d546493",
            "value": "README.md: "
          }
        },
        "08a8d150e2c14477b4617021172078b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f321be7795fa49e9b231428063a93a83",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bcad0da8e2ce40898f1dd2135f3c4f47",
            "value": 1
          }
        },
        "86118495fec54945bb926647716d657c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b04a114bd63243dc9220279d5f84a555",
            "placeholder": "​",
            "style": "IPY_MODEL_746bb8b9c4f940019ab9e47416822f51",
            "value": " 2.30k/? [00:00&lt;00:00, 288kB/s]"
          }
        },
        "1f73298db35a4ee9990556a4d95bdc44": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db93187a4d8a4c4d9fa6ba372387be98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9404cd41b3be4564bb50310a1d546493": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f321be7795fa49e9b231428063a93a83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bcad0da8e2ce40898f1dd2135f3c4f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b04a114bd63243dc9220279d5f84a555": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "746bb8b9c4f940019ab9e47416822f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9153378fdf65439f872a1e42fd317dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eafbf6b026a0469b8c08a11e38f6ffb0",
              "IPY_MODEL_60fffb20d20a4fa9b5c2e4bf4d7f5591",
              "IPY_MODEL_f786a560b59a46d0a5c100f7b4fa2c41"
            ],
            "layout": "IPY_MODEL_ed7eb04b31644424bd423810bad973a9"
          }
        },
        "eafbf6b026a0469b8c08a11e38f6ffb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e82a14425a443691712226c639d5ab",
            "placeholder": "​",
            "style": "IPY_MODEL_5c342dbd65bb40f699bc6b2abd82c6f1",
            "value": "train.txt.gz: 100%"
          }
        },
        "60fffb20d20a4fa9b5c2e4bf4d7f5591": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b1b68c4828a43759a608fd4305ba824",
            "max": 85467066,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04e711bdda434a6a970dd5346b8d8394",
            "value": 85467066
          }
        },
        "f786a560b59a46d0a5c100f7b4fa2c41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b531928d66cb4911a36b5fdd6f473ca2",
            "placeholder": "​",
            "style": "IPY_MODEL_1ffafa765b9c4a66bac16a545a6a0d03",
            "value": " 85.5M/85.5M [00:01&lt;00:00, 63.0MB/s]"
          }
        },
        "ed7eb04b31644424bd423810bad973a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4e82a14425a443691712226c639d5ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c342dbd65bb40f699bc6b2abd82c6f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7b1b68c4828a43759a608fd4305ba824": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e711bdda434a6a970dd5346b8d8394": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b531928d66cb4911a36b5fdd6f473ca2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ffafa765b9c4a66bac16a545a6a0d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "259ef01709d24d27aed3bc1247dc57bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4634bc8f69804cfa8262b1cf72604de8",
              "IPY_MODEL_3907b8cb9b6445ac9a4f09284628166a",
              "IPY_MODEL_f933996470394b6eb9a06bef9c4aabfc"
            ],
            "layout": "IPY_MODEL_eaba2e3dce324b1ebc08de60595cba16"
          }
        },
        "4634bc8f69804cfa8262b1cf72604de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab78cb7232d741b786e9879e8f6168f4",
            "placeholder": "​",
            "style": "IPY_MODEL_e0e2836bde1241a498f7bae238768696",
            "value": "test.txt.gz: 100%"
          }
        },
        "3907b8cb9b6445ac9a4f09284628166a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62e33c3f25594356b52517c56a03bf86",
            "max": 9488687,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd892461a7914ce2a895375478d9232a",
            "value": 9488687
          }
        },
        "f933996470394b6eb9a06bef9c4aabfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b72e0fa60e874ee5bc0b983cf32362d2",
            "placeholder": "​",
            "style": "IPY_MODEL_abe022163ed6453097af606abce5b022",
            "value": " 9.49M/9.49M [00:00&lt;00:00, 18.9MB/s]"
          }
        },
        "eaba2e3dce324b1ebc08de60595cba16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab78cb7232d741b786e9879e8f6168f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e2836bde1241a498f7bae238768696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "62e33c3f25594356b52517c56a03bf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd892461a7914ce2a895375478d9232a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b72e0fa60e874ee5bc0b983cf32362d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe022163ed6453097af606abce5b022": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6c143271f6a48e1b65af307643b5bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b8fbc6164c264f9c8ca96dc897e69019",
              "IPY_MODEL_ded397062cdf444787b4a0a598a97d79",
              "IPY_MODEL_c63f14456b374df3a5717c934c9249f0"
            ],
            "layout": "IPY_MODEL_9bb18995435640a98481c260778097d6"
          }
        },
        "b8fbc6164c264f9c8ca96dc897e69019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7e377dd0bb843dbb2f16df9a5d78b40",
            "placeholder": "​",
            "style": "IPY_MODEL_03888f66d621436288b083dab1364cd8",
            "value": "Generating train split: 100%"
          }
        },
        "ded397062cdf444787b4a0a598a97d79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30fb70f7d0544623a9c784c464b9c50d",
            "max": 1534699,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d4c4ca734b2c47f694610a6972ecefe2",
            "value": 1534699
          }
        },
        "c63f14456b374df3a5717c934c9249f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42ad895f932547ebb631382ed0fb139f",
            "placeholder": "​",
            "style": "IPY_MODEL_15a9fee3ef91466ba882ea866b0dc225",
            "value": " 1534699/1534699 [00:03&lt;00:00, 390903.47 examples/s]"
          }
        },
        "9bb18995435640a98481c260778097d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7e377dd0bb843dbb2f16df9a5d78b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03888f66d621436288b083dab1364cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30fb70f7d0544623a9c784c464b9c50d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c4ca734b2c47f694610a6972ecefe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42ad895f932547ebb631382ed0fb139f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a9fee3ef91466ba882ea866b0dc225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cfdbd674e74678a041baf744df3918": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2a1e33204fb74258ad99ddd14b866bfd",
              "IPY_MODEL_4a479e0f4e74465fab7b5b4b379039ff",
              "IPY_MODEL_40130299e30643b3acec03f11f798b8a"
            ],
            "layout": "IPY_MODEL_3e38088f31d646fcaa6938b0535cae76"
          }
        },
        "2a1e33204fb74258ad99ddd14b866bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_922794c4956b45f6b417b4dfb1870a17",
            "placeholder": "​",
            "style": "IPY_MODEL_f9a28cedd351432398345f8f06583c21",
            "value": "Generating test split: 100%"
          }
        },
        "4a479e0f4e74465fab7b5b4b379039ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ceb0f27f7664a8aa71ad3224ebae359",
            "max": 170522,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f081e0342234dfb9726411dbb869d60",
            "value": 170522
          }
        },
        "40130299e30643b3acec03f11f798b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_404d09baea2949a1ae2d10a4aacf6410",
            "placeholder": "​",
            "style": "IPY_MODEL_c218deb37e564b79821f44df1e3d72c3",
            "value": " 170522/170522 [00:00&lt;00:00, 407758.10 examples/s]"
          }
        },
        "3e38088f31d646fcaa6938b0535cae76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922794c4956b45f6b417b4dfb1870a17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9a28cedd351432398345f8f06583c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ceb0f27f7664a8aa71ad3224ebae359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f081e0342234dfb9726411dbb869d60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "404d09baea2949a1ae2d10a4aacf6410": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c218deb37e564b79821f44df1e3d72c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2d70ef21ab4787b9bf9d709ef29dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7593c4589f6f416184b8753d23ae25fb",
              "IPY_MODEL_0633ee4a7d6f4bb6a32fea745cbbb6a8",
              "IPY_MODEL_11c420f1a3e54ddfb4c6229d68d9ea76"
            ],
            "layout": "IPY_MODEL_887f7979aa15415486b70db951de4683"
          }
        },
        "7593c4589f6f416184b8753d23ae25fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eafec4d93dd54b2aaac17218f516c16b",
            "placeholder": "​",
            "style": "IPY_MODEL_82862d8d5aad4215bf1d6837dafd8643",
            "value": "100%"
          }
        },
        "0633ee4a7d6f4bb6a32fea745cbbb6a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a5a7d8530ef455abdbc425791584835",
            "max": 79,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ad38ededdf245168b6036a302659dc3",
            "value": 79
          }
        },
        "11c420f1a3e54ddfb4c6229d68d9ea76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d12c4d4b48646a09369adcefca4cf54",
            "placeholder": "​",
            "style": "IPY_MODEL_e269da2174f2462baa03bfc178935dba",
            "value": " 79/79 [00:04&lt;00:00, 15.86it/s]"
          }
        },
        "887f7979aa15415486b70db951de4683": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eafec4d93dd54b2aaac17218f516c16b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82862d8d5aad4215bf1d6837dafd8643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a5a7d8530ef455abdbc425791584835": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ad38ededdf245168b6036a302659dc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d12c4d4b48646a09369adcefca4cf54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e269da2174f2462baa03bfc178935dba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "558bad2f5b004575b86acbba908ad7bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2905ba759f944d1e89f947c7b38ac907",
              "IPY_MODEL_ad945a5144be429cbee5463e47a2ac91",
              "IPY_MODEL_4a51890b558c402fa7d67f797ce1312c"
            ],
            "layout": "IPY_MODEL_8f82d1b2b9ff49aea699736336495648"
          }
        },
        "2905ba759f944d1e89f947c7b38ac907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae48b85703dc458586e3f1192d2c930a",
            "placeholder": "​",
            "style": "IPY_MODEL_75d5a8a77b4a4b679a7e687288d82f44",
            "value": "100%"
          }
        },
        "ad945a5144be429cbee5463e47a2ac91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e678f5d8750448b6b260b48907d31c7f",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a969e8ad1e604e51af2734c9331c5420",
            "value": 500
          }
        },
        "4a51890b558c402fa7d67f797ce1312c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7836ec6a3d084a4f9d606c954a437389",
            "placeholder": "​",
            "style": "IPY_MODEL_180bb9264de64ef78c9cd8a09b098074",
            "value": " 500/500 [05:22&lt;00:00,  1.49it/s]"
          }
        },
        "8f82d1b2b9ff49aea699736336495648": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae48b85703dc458586e3f1192d2c930a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75d5a8a77b4a4b679a7e687288d82f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e678f5d8750448b6b260b48907d31c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a969e8ad1e604e51af2734c9331c5420": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7836ec6a3d084a4f9d606c954a437389": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "180bb9264de64ef78c9cd8a09b098074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89c108a2fc224b5582413b7ae90614c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3da9f058956142a9826488c92117b67c",
              "IPY_MODEL_ba0d732f8f334ddfb817185d2b2d41dd",
              "IPY_MODEL_0000018122944adbb520db629a37a1a7"
            ],
            "layout": "IPY_MODEL_34d08e26f5e14284916c2fb1637a92a3"
          }
        },
        "3da9f058956142a9826488c92117b67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_707d0d9ece794e96b0035b176d13e635",
            "placeholder": "​",
            "style": "IPY_MODEL_f67cd9a7bd6b4420946aa1e7bfb533ed",
            "value": "Uploading the dataset shards: 100%"
          }
        },
        "ba0d732f8f334ddfb817185d2b2d41dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6dc607cddd144647b4892fcda14a447c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_db6eb841e1064864a6d7639e7b686773",
            "value": 1
          }
        },
        "0000018122944adbb520db629a37a1a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e10747a80f441d8addee142235f86e3",
            "placeholder": "​",
            "style": "IPY_MODEL_e533ab05f6324f66897b729f11666abc",
            "value": " 1/1 [13:54&lt;00:00,  7.32s/ shards]"
          }
        },
        "34d08e26f5e14284916c2fb1637a92a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "707d0d9ece794e96b0035b176d13e635": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f67cd9a7bd6b4420946aa1e7bfb533ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6dc607cddd144647b4892fcda14a447c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db6eb841e1064864a6d7639e7b686773": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e10747a80f441d8addee142235f86e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e533ab05f6324f66897b729f11666abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27b2d7f3746749588fcddee8cd2ec35a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2659d507c135495d9ae0fca0b141bedc",
              "IPY_MODEL_41ba0fde77f046cc93d351fc27ed07f1",
              "IPY_MODEL_098eead2e888463ba451d3df23c9955c"
            ],
            "layout": "IPY_MODEL_da53343938464ac19321388801cc0968"
          }
        },
        "2659d507c135495d9ae0fca0b141bedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e96bd3caf2f4410eb1952fa8601020bd",
            "placeholder": "​",
            "style": "IPY_MODEL_3103c9fd9a254baeb81f8426c323989c",
            "value": "Creating parquet from Arrow format: 100%"
          }
        },
        "41ba0fde77f046cc93d351fc27ed07f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25098c0e52f64f3dbcdc14868dd88d23",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4105ef2d2e541cba6531514084326c8",
            "value": 5
          }
        },
        "098eead2e888463ba451d3df23c9955c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_039e2bed8341469cb3ebda053db46aef",
            "placeholder": "​",
            "style": "IPY_MODEL_e35c31d35f6b45ba907011a25e3071ad",
            "value": " 5/5 [00:00&lt;00:00, 16.83ba/s]"
          }
        },
        "da53343938464ac19321388801cc0968": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96bd3caf2f4410eb1952fa8601020bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3103c9fd9a254baeb81f8426c323989c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25098c0e52f64f3dbcdc14868dd88d23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4105ef2d2e541cba6531514084326c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "039e2bed8341469cb3ebda053db46aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e35c31d35f6b45ba907011a25e3071ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b324b5333e4c4abfb21edadb1cc7fbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_344390f41fa048b8a919249e09b471f3",
              "IPY_MODEL_64c8b35e57c14e6caca76a5ed3bcea6a",
              "IPY_MODEL_1615ca16f62b4381bdf473f7618ebc8f"
            ],
            "layout": "IPY_MODEL_b17a8dd8f423452f8dbe20e34b3a71b4"
          }
        },
        "344390f41fa048b8a919249e09b471f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63a490ba37f34e17823c7f4dd89cae4b",
            "placeholder": "​",
            "style": "IPY_MODEL_3f5e142645f14e33bca9afe53c0f724a",
            "value": "Processing Files (1 / 1)      : 100%"
          }
        },
        "64c8b35e57c14e6caca76a5ed3bcea6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71be5798d84d41c3a31e08d5d70bcd84",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00224f174f19478ea0ce74e24219c5cb",
            "value": 1
          }
        },
        "1615ca16f62b4381bdf473f7618ebc8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55533224b07c475c827f7fd53d890ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_886984036a9d45fe92b4248dceb0ee5b",
            "value": " 26.3MB / 26.3MB, 4.70MB/s  "
          }
        },
        "b17a8dd8f423452f8dbe20e34b3a71b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63a490ba37f34e17823c7f4dd89cae4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f5e142645f14e33bca9afe53c0f724a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71be5798d84d41c3a31e08d5d70bcd84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "00224f174f19478ea0ce74e24219c5cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "55533224b07c475c827f7fd53d890ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "886984036a9d45fe92b4248dceb0ee5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76990916b7d14e25b2a70a5dab467587": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e8e331cc19a4a5b8877eaed3172553b",
              "IPY_MODEL_d7005628cfb1401e8ccd9822c5de9194",
              "IPY_MODEL_fb1ad6fd777c482aa98508cd814b6948"
            ],
            "layout": "IPY_MODEL_a723daf59c6e4076aab910fd10052a80"
          }
        },
        "9e8e331cc19a4a5b8877eaed3172553b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e8ef30370864ef9a3062a42a48b7f58",
            "placeholder": "​",
            "style": "IPY_MODEL_2c40a326e2ec49cdae5b5b2376795d50",
            "value": "New Data Upload               : 100%"
          }
        },
        "d7005628cfb1401e8ccd9822c5de9194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0dc4cf2f5a63474a94128c7141f37583",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec4e511544a840e5b35d4a23e22e4ad7",
            "value": 1
          }
        },
        "fb1ad6fd777c482aa98508cd814b6948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3b8c202967e4039903228cf1b568b37",
            "placeholder": "​",
            "style": "IPY_MODEL_1be16488676a41fd8f0425d269b0a8e1",
            "value": " 26.2MB / 26.2MB, 4.67MB/s  "
          }
        },
        "a723daf59c6e4076aab910fd10052a80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8ef30370864ef9a3062a42a48b7f58": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c40a326e2ec49cdae5b5b2376795d50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dc4cf2f5a63474a94128c7141f37583": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ec4e511544a840e5b35d4a23e22e4ad7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3b8c202967e4039903228cf1b568b37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be16488676a41fd8f0425d269b0a8e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e1e40299acf47d48d7e9857578da3d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80fb6bccfeae4845b5d32eb6ea14bdfc",
              "IPY_MODEL_2974a38eedda43d78e76415a12d1564b",
              "IPY_MODEL_0e5e55c66417465d8f490b8918e7e6d8"
            ],
            "layout": "IPY_MODEL_c5232fcc6f6448d98d52c2a6f5f2b9c0"
          }
        },
        "80fb6bccfeae4845b5d32eb6ea14bdfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109ef0ac7a4b498d9aae6052fa095c3c",
            "placeholder": "​",
            "style": "IPY_MODEL_bf1b3b84ffaf44aebe67676c0ce67d0f",
            "value": "                              : 100%"
          }
        },
        "2974a38eedda43d78e76415a12d1564b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2b32f5916734dbe8271c0230feb0f84",
            "max": 26290294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b43e0666011c4443bccb9fb30546b7be",
            "value": 26290294
          }
        },
        "0e5e55c66417465d8f490b8918e7e6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b70983302b648459abebe22dee6fe9c",
            "placeholder": "​",
            "style": "IPY_MODEL_118817d4d6ad486989fdcb3d310f9623",
            "value": " 26.3MB / 26.3MB            "
          }
        },
        "c5232fcc6f6448d98d52c2a6f5f2b9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "109ef0ac7a4b498d9aae6052fa095c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf1b3b84ffaf44aebe67676c0ce67d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b32f5916734dbe8271c0230feb0f84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43e0666011c4443bccb9fb30546b7be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3b70983302b648459abebe22dee6fe9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "118817d4d6ad486989fdcb3d310f9623": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1c858c6ccf4d07bb636fd9e53f0da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea43eed4f2ee40ec95f105c7d9e42bd6",
              "IPY_MODEL_d71a7c484ff0403d978663a045dd1a08",
              "IPY_MODEL_0e652fe65f004d7986067bbcfc034b8a"
            ],
            "layout": "IPY_MODEL_4e457315be8e43d6871ef5c24a59fd87"
          }
        },
        "ea43eed4f2ee40ec95f105c7d9e42bd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d577fbfd6d84128ad625d402f363b96",
            "placeholder": "​",
            "style": "IPY_MODEL_34229ce0aca54ba1b4286db89c05d925",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "d71a7c484ff0403d978663a045dd1a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66d52246ced04920903b52e2230f1da3",
            "max": 585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_877cb4899abe45e88114817d83602c6c",
            "value": 585
          }
        },
        "0e652fe65f004d7986067bbcfc034b8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_712a4975276045509aaa4929e3fc8ce8",
            "placeholder": "​",
            "style": "IPY_MODEL_978f5f8bb54d41548ba7309b9a5278d4",
            "value": " 585/585 [00:00&lt;00:00, 63.2kB/s]"
          }
        },
        "4e457315be8e43d6871ef5c24a59fd87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d577fbfd6d84128ad625d402f363b96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34229ce0aca54ba1b4286db89c05d925": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66d52246ced04920903b52e2230f1da3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "877cb4899abe45e88114817d83602c6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "712a4975276045509aaa4929e3fc8ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "978f5f8bb54d41548ba7309b9a5278d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4fcee14038b044d6830aa905bf7d2269": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa18b5f77bc644f295d0db88bcef39a1",
              "IPY_MODEL_bd958f8ed1e043aa971a141b376ab9c3",
              "IPY_MODEL_bd8730ca482f4de48688a05f57f39d76"
            ],
            "layout": "IPY_MODEL_e5c920b4961b4a3dafda552c4424140f"
          }
        },
        "fa18b5f77bc644f295d0db88bcef39a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a342b7516dd04b36970cdaaffe567288",
            "placeholder": "​",
            "style": "IPY_MODEL_c269ab6ac0ab40eb844e64555c2c28ec",
            "value": "config.json: 100%"
          }
        },
        "bd958f8ed1e043aa971a141b376ab9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e426527ab2c44a2692edcf4891c1efb7",
            "max": 745,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_161fa9207bb64d1d9fa5841a303385fd",
            "value": 745
          }
        },
        "bd8730ca482f4de48688a05f57f39d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe18fb891e0543fea41353a310ccc558",
            "placeholder": "​",
            "style": "IPY_MODEL_726e9caa1e0146efb64d18967803efdd",
            "value": " 745/745 [00:00&lt;00:00, 82.4kB/s]"
          }
        },
        "e5c920b4961b4a3dafda552c4424140f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a342b7516dd04b36970cdaaffe567288": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c269ab6ac0ab40eb844e64555c2c28ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e426527ab2c44a2692edcf4891c1efb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "161fa9207bb64d1d9fa5841a303385fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fe18fb891e0543fea41353a310ccc558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726e9caa1e0146efb64d18967803efdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cd2e198926e409784f5eced56aed830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ba09ef63c28456a970982254677b5c9",
              "IPY_MODEL_85e5fef78a74447a90776201e8954db4",
              "IPY_MODEL_ea885bfc6ce3416aa76816dea0fc4c2f"
            ],
            "layout": "IPY_MODEL_53ded89e42a544c8aaffbf07e663fb17"
          }
        },
        "0ba09ef63c28456a970982254677b5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f8f1c69950e4d7cbf265dea213bad8d",
            "placeholder": "​",
            "style": "IPY_MODEL_d8c17eead38c4c5db4a586702bf5baa9",
            "value": "model.safetensors: 100%"
          }
        },
        "85e5fef78a74447a90776201e8954db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171f017e4bbd4e02a73a43ddba610080",
            "max": 1212559808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_58cb9170fdd74ab7a4e4dcf64a1b728d",
            "value": 1212559808
          }
        },
        "ea885bfc6ce3416aa76816dea0fc4c2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dfa1f4859fb41d6be223af70989b99f",
            "placeholder": "​",
            "style": "IPY_MODEL_0859b65c24684d2a898e974d047dca0e",
            "value": " 1.21G/1.21G [00:02&lt;00:00, 1.13GB/s]"
          }
        },
        "53ded89e42a544c8aaffbf07e663fb17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f8f1c69950e4d7cbf265dea213bad8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8c17eead38c4c5db4a586702bf5baa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "171f017e4bbd4e02a73a43ddba610080": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58cb9170fdd74ab7a4e4dcf64a1b728d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5dfa1f4859fb41d6be223af70989b99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0859b65c24684d2a898e974d047dca0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "51664daba73040b98318416a7e82417d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9cea022eed2548ba9548571abe8a297b",
              "IPY_MODEL_0b55f68bc128433386c93294f0c4e8c3",
              "IPY_MODEL_c6ae3e99dca54409baa39504e9a50f50"
            ],
            "layout": "IPY_MODEL_fc31644cee7345589989ef04318cbb93"
          }
        },
        "9cea022eed2548ba9548571abe8a297b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9b4cb8481010488e8ab07b24ccec1da8",
            "placeholder": "​",
            "style": "IPY_MODEL_0976d89565594f7681576d74c5fb2f4d",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "0b55f68bc128433386c93294f0c4e8c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0464c71aa4354777a273407a25ec43fd",
            "max": 10158382892,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_301591f6f12b4e96831e8ad0eee98746",
            "value": 10158382892
          }
        },
        "c6ae3e99dca54409baa39504e9a50f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bc7d0bb45344c53bbe9877c68ea170e",
            "placeholder": "​",
            "style": "IPY_MODEL_54f4cd5aa162474b91ad3393b25252ee",
            "value": " 10.2G/10.2G [00:23&lt;00:00, 313MB/s]"
          }
        },
        "fc31644cee7345589989ef04318cbb93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b4cb8481010488e8ab07b24ccec1da8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0976d89565594f7681576d74c5fb2f4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0464c71aa4354777a273407a25ec43fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "301591f6f12b4e96831e8ad0eee98746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1bc7d0bb45344c53bbe9877c68ea170e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54f4cd5aa162474b91ad3393b25252ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "54b93ce6dea141199ab7d9c71cc17bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a0b14c9237e145f194d3c7d15a01eb84",
              "IPY_MODEL_ba6e9c92994244479e809415efa56900",
              "IPY_MODEL_504ab435d23048d99660f382e3502a48"
            ],
            "layout": "IPY_MODEL_d8c012e0c7014b3591021e43bf66e45c"
          }
        },
        "a0b14c9237e145f194d3c7d15a01eb84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa5d30cc0cf64deb8eb77ab6d73bda76",
            "placeholder": "​",
            "style": "IPY_MODEL_06a2432f88c448f88fb86375c4610580",
            "value": "config.json: 100%"
          }
        },
        "ba6e9c92994244479e809415efa56900": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add5e130ff04473491695b38239ae6e1",
            "max": 744,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3afc3db163024cf5a3e9942caf2b30d6",
            "value": 744
          }
        },
        "504ab435d23048d99660f382e3502a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e086a5f71bce469a94313c3e381a3600",
            "placeholder": "​",
            "style": "IPY_MODEL_c4589dddcf1c442d921882f3c30f692b",
            "value": " 744/744 [00:00&lt;00:00, 82.7kB/s]"
          }
        },
        "d8c012e0c7014b3591021e43bf66e45c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5d30cc0cf64deb8eb77ab6d73bda76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a2432f88c448f88fb86375c4610580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "add5e130ff04473491695b38239ae6e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3afc3db163024cf5a3e9942caf2b30d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e086a5f71bce469a94313c3e381a3600": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4589dddcf1c442d921882f3c30f692b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6276e61f080c49cfa19510549e4c0b62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6af2639416314bfc95a94d2efb1cedf5",
              "IPY_MODEL_2d3e445f77814a9799fd7f844418516d",
              "IPY_MODEL_72e5d06d8aa144ac9e9b853299c4abb8"
            ],
            "layout": "IPY_MODEL_b3a9de287ff8424e8345bba52e972c59"
          }
        },
        "6af2639416314bfc95a94d2efb1cedf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3b018f1187a14e36a6b24a3702be25c4",
            "placeholder": "​",
            "style": "IPY_MODEL_7895574cf16d43c5ae8c41f27560d818",
            "value": "model.safetensors: 100%"
          }
        },
        "2d3e445f77814a9799fd7f844418516d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60ae15ff42714319b1343ef1c1761e8d",
            "max": 342662192,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0bf29967041f44c8b4c58b392b9cd069",
            "value": 342662192
          }
        },
        "72e5d06d8aa144ac9e9b853299c4abb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea3ac6ceb9d2414692055422541c381e",
            "placeholder": "​",
            "style": "IPY_MODEL_d6873a0816854ea4a1122615b3f43119",
            "value": " 343M/343M [00:02&lt;00:00, 236MB/s]"
          }
        },
        "b3a9de287ff8424e8345bba52e972c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b018f1187a14e36a6b24a3702be25c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7895574cf16d43c5ae8c41f27560d818": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60ae15ff42714319b1343ef1c1761e8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bf29967041f44c8b4c58b392b9cd069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea3ac6ceb9d2414692055422541c381e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6873a0816854ea4a1122615b3f43119": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2df6f47005ea429f93e6432215a3c3bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48acde64835a495f8aecb1ad3f0db273",
              "IPY_MODEL_c1f3166c160b40c190f5f9b55fe1a9f2",
              "IPY_MODEL_4d2bc4e79dc64f20aeba2e1b6a92bdf3"
            ],
            "layout": "IPY_MODEL_f6d263ce88124fb5af54fe317f3912c6"
          }
        },
        "48acde64835a495f8aecb1ad3f0db273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09be89632479470bb0b6a70f657fae62",
            "placeholder": "​",
            "style": "IPY_MODEL_ebe661bb07ee47ce8b8cf8522ac469c4",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "c1f3166c160b40c190f5f9b55fe1a9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0fb0d23de434088848a7f8f0a28c311",
            "max": 585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_def4f0babf4c4855be9e77ffecba561b",
            "value": 585
          }
        },
        "4d2bc4e79dc64f20aeba2e1b6a92bdf3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e23c99903b2a44be8f1aaf3114fa3513",
            "placeholder": "​",
            "style": "IPY_MODEL_996f8520f55b4ef6ba8327eed4ae36da",
            "value": " 585/585 [00:00&lt;00:00, 82.2kB/s]"
          }
        },
        "f6d263ce88124fb5af54fe317f3912c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09be89632479470bb0b6a70f657fae62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebe661bb07ee47ce8b8cf8522ac469c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0fb0d23de434088848a7f8f0a28c311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def4f0babf4c4855be9e77ffecba561b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e23c99903b2a44be8f1aaf3114fa3513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "996f8520f55b4ef6ba8327eed4ae36da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3be73c8da38b4e03ab4949459daff05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415aba6e6cfc4a649c3f703bbfb1b91c",
              "IPY_MODEL_7767048b604a4197bf1a7ba58e5eee0b",
              "IPY_MODEL_6834971356284080a44804d98189ee02"
            ],
            "layout": "IPY_MODEL_83ae42bc76f64956bff5d52cfaa7d1ab"
          }
        },
        "415aba6e6cfc4a649c3f703bbfb1b91c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c546441a5613405abf88f3c513a4a604",
            "placeholder": "​",
            "style": "IPY_MODEL_b8b03564053743a897930e7a76a0d341",
            "value": "config.json: 100%"
          }
        },
        "7767048b604a4197bf1a7ba58e5eee0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86eaa0c834284e7b8a8137325ea557d3",
            "max": 447,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90085490af674433a68a2322b560d0c4",
            "value": 447
          }
        },
        "6834971356284080a44804d98189ee02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e1bfb0fc5c04d95aca84ef23306603b",
            "placeholder": "​",
            "style": "IPY_MODEL_671ec51b1b874b5392ee554d6a05f66d",
            "value": " 447/447 [00:00&lt;00:00, 56.8kB/s]"
          }
        },
        "83ae42bc76f64956bff5d52cfaa7d1ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c546441a5613405abf88f3c513a4a604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8b03564053743a897930e7a76a0d341": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86eaa0c834284e7b8a8137325ea557d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90085490af674433a68a2322b560d0c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e1bfb0fc5c04d95aca84ef23306603b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "671ec51b1b874b5392ee554d6a05f66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d075d671557045fd81466639b5c69e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc77902e874948df90f7e52741ba8395",
              "IPY_MODEL_73371ab01fb5496abb5f4943e7767f4b",
              "IPY_MODEL_9d681e78661e437687f9773c2aa46f81"
            ],
            "layout": "IPY_MODEL_56152b5411934b4aa4fdfafea5595fc9"
          }
        },
        "cc77902e874948df90f7e52741ba8395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d1c4242ebb141c0b6633502de402d46",
            "placeholder": "​",
            "style": "IPY_MODEL_249bb040cf474ebabfe8661bfc6c460c",
            "value": "model.safetensors: 100%"
          }
        },
        "73371ab01fb5496abb5f4943e7767f4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b1a4e6673214bf5bfa58a66496ca9ed",
            "max": 197854928,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97041718eeb04037b85226691afed906",
            "value": 197854928
          }
        },
        "9d681e78661e437687f9773c2aa46f81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0045524095e4229bbf820413ae88fcf",
            "placeholder": "​",
            "style": "IPY_MODEL_0bada257179c47c9abec90a9e3b476c9",
            "value": " 198M/198M [00:07&lt;00:00, 29.0MB/s]"
          }
        },
        "56152b5411934b4aa4fdfafea5595fc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d1c4242ebb141c0b6633502de402d46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "249bb040cf474ebabfe8661bfc6c460c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b1a4e6673214bf5bfa58a66496ca9ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97041718eeb04037b85226691afed906": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a0045524095e4229bbf820413ae88fcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bada257179c47c9abec90a9e3b476c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b3de4823bf14d4f809a0edbf590d500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0a0b9132499a4b8186c83b49b20c73f1",
              "IPY_MODEL_30048f6492504356b4996d41620419f0",
              "IPY_MODEL_71ebffa1055948788bccec051d78b2b9"
            ],
            "layout": "IPY_MODEL_095ec648a0c14c8d9664e69324dfbe99"
          }
        },
        "0a0b9132499a4b8186c83b49b20c73f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85ed96f169c94bdabb6c3c43d3798db0",
            "placeholder": "​",
            "style": "IPY_MODEL_85d68d33994640509982cc6779e30bb6",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "30048f6492504356b4996d41620419f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865dd78ad416436aa62567b90f5e9d98",
            "max": 585,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_070aa84cb68c439382e888696b7c2daa",
            "value": 585
          }
        },
        "71ebffa1055948788bccec051d78b2b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c61b2b7899c4eebad7684899abdedcd",
            "placeholder": "​",
            "style": "IPY_MODEL_67daf02d32954e829b51b4ea15a11e55",
            "value": " 585/585 [00:00&lt;00:00, 74.1kB/s]"
          }
        },
        "095ec648a0c14c8d9664e69324dfbe99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85ed96f169c94bdabb6c3c43d3798db0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85d68d33994640509982cc6779e30bb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "865dd78ad416436aa62567b90f5e9d98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "070aa84cb68c439382e888696b7c2daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c61b2b7899c4eebad7684899abdedcd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67daf02d32954e829b51b4ea15a11e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# setup the geofractal repo"
      ],
      "metadata": {
        "id": "nwHcegtfP-bB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    !pip uninstall -qy geometricvocab geofractal\n",
        "except:\n",
        "    pass\n",
        "\n",
        "!pip install -q git+https://github.com/AbstractEyes/geofractal.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73hJjh45QAqR",
        "outputId": "9cce66a2-93dc-49f3-e0ff-0c340bd87d04"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for geofractal (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for geometricvocab (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# quick compile test"
      ],
      "metadata": {
        "id": "FnSKpuKIG4V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Quick torch.compile() test\"\"\"\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "def test_compile():\n",
        "    from geofractal.router.head.builder import HeadBuilder\n",
        "    from geofractal.router.head.components import HeadConfig\n",
        "    from geofractal.router.factory.prototype import LightweightPrototype, AssembledPrototype, PrototypeConfig\n",
        "    from geofractal.router.factory.protocols import StreamSpec\n",
        "\n",
        "    B, S, D = 4, 16, 256\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"  torch.compile() Compatibility Test\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"PyTorch version: {torch.__version__}\")\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print()\n",
        "\n",
        "    # 1. ComposedHead\n",
        "    print(\"1. ComposedHead...\")\n",
        "    try:\n",
        "        config = HeadConfig(feature_dim=D)\n",
        "        head = HeadBuilder.standard(config).build().to(DEVICE)\n",
        "        compiled_head = torch.compile(head)\n",
        "\n",
        "        x = torch.randn(B, S, D, device=DEVICE)\n",
        "        out = compiled_head(x)\n",
        "        assert out.shape == (B, S, D)\n",
        "\n",
        "        # Backward\n",
        "        out.sum().backward()\n",
        "        print(\"   ✓ Forward + backward works\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ Failed: {e}\")\n",
        "\n",
        "    # 2. LightweightPrototype\n",
        "    print(\"2. LightweightPrototype...\")\n",
        "    try:\n",
        "        prototype = LightweightPrototype(\n",
        "            stream_dims={\"a\": 256, \"b\": 256},\n",
        "            num_classes=10,\n",
        "            hidden_dim=128,\n",
        "        ).to(DEVICE)\n",
        "        compiled_proto = torch.compile(prototype)\n",
        "\n",
        "        inputs = {\n",
        "            \"a\": torch.randn(B, 256, device=DEVICE),\n",
        "            \"b\": torch.randn(B, 256, device=DEVICE),\n",
        "        }\n",
        "        logits, _ = compiled_proto(inputs)\n",
        "        assert logits.shape == (B, 10)\n",
        "\n",
        "        labels = torch.randint(0, 10, (B,), device=DEVICE)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        print(\"   ✓ Forward + backward works\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ Failed: {e}\")\n",
        "\n",
        "    # 3. AssembledPrototype\n",
        "    print(\"3. AssembledPrototype...\")\n",
        "    try:\n",
        "        config = PrototypeConfig(\n",
        "            num_classes=10,\n",
        "            stream_specs=[\n",
        "                StreamSpec.feature_stream(\"a\", input_dim=256, feature_dim=128),\n",
        "                StreamSpec.feature_stream(\"b\", input_dim=256, feature_dim=128),\n",
        "            ],\n",
        "            freeze_streams=False,\n",
        "        )\n",
        "        prototype = AssembledPrototype(config).to(DEVICE)\n",
        "        compiled_proto = torch.compile(prototype)\n",
        "\n",
        "        inputs = {\n",
        "            \"a\": torch.randn(B, 256, device=DEVICE),\n",
        "            \"b\": torch.randn(B, 256, device=DEVICE),\n",
        "        }\n",
        "        logits, _ = compiled_proto(inputs)\n",
        "        assert logits.shape == (B, 10)\n",
        "\n",
        "        labels = torch.randint(0, 10, (B,), device=DEVICE)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        print(\"   ✓ Forward + backward works\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ Failed: {e}\")\n",
        "\n",
        "    # 4. Training loop with compile\n",
        "    print(\"4. Training loop (10 steps)...\")\n",
        "    try:\n",
        "        prototype = LightweightPrototype(\n",
        "            stream_dims={\"a\": 128, \"b\": 128},\n",
        "            num_classes=5,\n",
        "            hidden_dim=64,\n",
        "        ).to(DEVICE)\n",
        "        compiled_proto = torch.compile(prototype)\n",
        "        optimizer = torch.optim.Adam(compiled_proto.parameters(), lr=1e-3)\n",
        "\n",
        "        for step in range(10):\n",
        "            inputs = {\n",
        "                \"a\": torch.randn(B, 128, device=DEVICE),\n",
        "                \"b\": torch.randn(B, 128, device=DEVICE),\n",
        "            }\n",
        "            labels = torch.randint(0, 5, (B,), device=DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, _ = compiled_proto(inputs)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(\"   ✓ Training loop works\")\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ Failed: {e}\")\n",
        "\n",
        "    # 5. Compile modes\n",
        "    print(\"5. Compile modes...\")\n",
        "    for mode in [\"default\", \"reduce-overhead\", \"max-autotune\"]:\n",
        "        try:\n",
        "            head = HeadBuilder.lightweight(HeadConfig(feature_dim=128)).build().to(DEVICE)\n",
        "            compiled = torch.compile(head, mode=mode)\n",
        "            x = torch.randn(B, S, 128, device=DEVICE)\n",
        "            out = compiled(x)\n",
        "            print(f\"   ✓ mode='{mode}'\")\n",
        "        except Exception as e:\n",
        "            print(f\"   ✗ mode='{mode}': {e}\")\n",
        "\n",
        "    print()\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test_compile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbuNARf1G2FI",
        "outputId": "222926aa-7e06-4c13-a322-d8276ae471f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  torch.compile() Compatibility Test\n",
            "============================================================\n",
            "PyTorch version: 2.9.0+cu126\n",
            "Device: cuda\n",
            "\n",
            "1. ComposedHead...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  return torch._C._get_cublas_allow_tf32()\n",
            "W1205 01:24:52.966000 1028 torch/_inductor/utils.py:1558] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ Forward + backward works\n",
            "2. LightweightPrototype...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ Forward + backward works\n",
            "3. AssembledPrototype...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ Forward + backward works\n",
            "4. Training loop (10 steps)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ Training loop works\n",
            "5. Compile modes...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ mode='default'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ mode='reduce-overhead'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.049984000623226166}\n",
            "AUTOTUNE addmm(64x512, 64x128, 128x512)\n",
            "strides: [0, 1], [128, 1], [1, 128]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0500 ms 100.0% \n",
            "  bias_addmm 0.0683 ms 73.2% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0368 seconds and 0.0002 seconds precompiling for 2 choices\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.04585599899291992}\n",
            "AUTOTUNE addmm(1x32, 1x64, 64x32)\n",
            "strides: [0, 1], [64, 1], [1, 64]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0459 ms 100.0% \n",
            "  bias_addmm 0.0686 ms 66.9% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0341 seconds and 0.0002 seconds precompiling for 2 choices\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.04707200080156326}\n",
            "AUTOTUNE addmm(1x128, 1x64, 64x128)\n",
            "strides: [0, 1], [64, 1], [1, 64]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0471 ms 100.0% \n",
            "  bias_addmm 0.0696 ms 67.6% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0335 seconds and 0.0001 seconds precompiling for 2 choices\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.04710400104522705}\n",
            "AUTOTUNE addmm(64x128, 64x128, 128x128)\n",
            "strides: [0, 1], [128, 1], [1, 128]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0471 ms 100.0% \n",
            "  bias_addmm 0.0696 ms 67.7% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0323 seconds and 0.0002 seconds precompiling for 2 choices\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.046431999653577805}\n",
            "AUTOTUNE addmm(1x16, 1x32, 32x16)\n",
            "strides: [0, 1], [0, 1], [1, 32]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0464 ms 100.0% \n",
            "  bias_addmm 0.0691 ms 67.2% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0338 seconds and 0.0002 seconds precompiling for 2 choices\n",
            "Autotune Choices Stats:\n",
            "{\"num_choices\": 2, \"num_triton_choices\": 0, \"best_kernel\": \"addmm\", \"best_time\": 0.0469760000705719}\n",
            "AUTOTUNE addmm(1x128, 1x128, 128x128)\n",
            "strides: [0, 1], [128, 1], [1, 128]\n",
            "dtypes: torch.float32, torch.float32, torch.float32\n",
            "  addmm 0.0470 ms 100.0% \n",
            "  bias_addmm 0.0696 ms 67.5% \n",
            "SingleProcess AUTOTUNE benchmarking takes 0.0324 seconds and 0.0002 seconds precompiling for 2 choices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ mode='max-autotune'\n",
            "\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# simple classifier test"
      ],
      "metadata": {
        "id": "2vajKVjtIUj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple classifier using current GeoFractal API.\n",
        "How hard is it really?\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "def test_simple_classifier():\n",
        "    \"\"\"Build a simple 2-stream classifier with current API.\"\"\"\n",
        "\n",
        "    from geofractal.router.factory.prototype import AssembledPrototype, PrototypeConfig\n",
        "    from geofractal.router.factory.protocols import StreamSpec, HeadSpec, FusionSpec\n",
        "\n",
        "    # =================================================================\n",
        "    # SETUP: Simulated feature extractors (frozen encoders)\n",
        "    # =================================================================\n",
        "\n",
        "    # Pretend these are CLIP and DINO\n",
        "    clip_dim = 512\n",
        "    dino_dim = 768\n",
        "    num_classes = 10\n",
        "\n",
        "    # =================================================================\n",
        "    # BUILD THE CLASSIFIER\n",
        "    # =================================================================\n",
        "\n",
        "    config = PrototypeConfig(\n",
        "        num_classes=num_classes,\n",
        "        stream_specs=[\n",
        "            StreamSpec.feature_vector(\"clip\", input_dim=clip_dim, feature_dim=256),\n",
        "            StreamSpec.feature_vector(\"dino\", input_dim=dino_dim, feature_dim=256),\n",
        "        ],\n",
        "        head_spec=HeadSpec.lightweight(feature_dim=256),\n",
        "        fusion_spec=FusionSpec.standard(output_dim=256),\n",
        "        freeze_streams=False,\n",
        "    )\n",
        "\n",
        "    model = AssembledPrototype(config).to(DEVICE).compile()\n",
        "\n",
        "    print(f\"Model parameters: {model.num_parameters:,}\")\n",
        "    print(f\"Streams: {model.stream_names}\")\n",
        "\n",
        "    # =================================================================\n",
        "    # TRAINING LOOP\n",
        "    # =================================================================\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(3):\n",
        "        epoch_loss = 0\n",
        "        for step in range(50):\n",
        "            # Simulate batch of features from frozen encoders\n",
        "            batch = {\n",
        "                \"clip\": torch.randn(32, clip_dim, device=DEVICE),\n",
        "                \"dino\": torch.randn(32, dino_dim, device=DEVICE),\n",
        "            }\n",
        "            labels = torch.randint(0, num_classes, (32,), device=DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits, info = model(batch)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: loss={epoch_loss/10:.4f}\")\n",
        "\n",
        "    # =================================================================\n",
        "    # INFERENCE\n",
        "    # =================================================================\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_batch = {\n",
        "            \"clip\": torch.randn(8, clip_dim, device=DEVICE),\n",
        "            \"dino\": torch.randn(8, dino_dim, device=DEVICE),\n",
        "        }\n",
        "        logits, info = model(test_batch)\n",
        "        preds = logits.argmax(dim=-1)\n",
        "        print(f\"Predictions: {preds.tolist()}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_lightweight_classifier():\n",
        "    \"\"\"Even simpler with LightweightPrototype.\"\"\"\n",
        "\n",
        "    from geofractal.router.factory.prototype import LightweightPrototype\n",
        "\n",
        "    # =================================================================\n",
        "    # BUILD - Just dimensions and classes\n",
        "    # =================================================================\n",
        "\n",
        "    model = LightweightPrototype(\n",
        "        stream_dims={\"clip\": 512, \"dino\": 768},\n",
        "        num_classes=10,\n",
        "        hidden_dim=256,\n",
        "    ).to(DEVICE).compile()\n",
        "\n",
        "    print(f\"\\nLightweight params: {model.num_parameters:,}\")\n",
        "\n",
        "    # =================================================================\n",
        "    # TRAIN\n",
        "    # =================================================================\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    model.train()\n",
        "    for step in range(20):\n",
        "        batch = {\n",
        "            \"clip\": torch.randn(32, 512, device=DEVICE),\n",
        "            \"dino\": torch.randn(32, 768, device=DEVICE),\n",
        "        }\n",
        "        labels = torch.randint(0, 10, (32,), device=DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits, _ = model(batch)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Final loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*60)\n",
        "    print(\"  AssembledPrototype Classifier\")\n",
        "    print(\"=\"*60)\n",
        "    test_simple_classifier()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"  LightweightPrototype Classifier\")\n",
        "    print(\"=\"*60)\n",
        "    test_lightweight_classifier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4LvNdwjIYYR",
        "outputId": "3eecab3c-6801-424b-9373-6833716abdc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "  AssembledPrototype Classifier\n",
            "============================================================\n",
            "Model parameters: 2,615,042\n",
            "Streams: ['clip', 'dino']\n",
            "Epoch 1: loss=12.3842\n",
            "Epoch 2: loss=11.8204\n",
            "Epoch 3: loss=11.7913\n",
            "Predictions: [2, 2, 2, 2, 2, 7, 2, 6]\n",
            "\n",
            "============================================================\n",
            "  LightweightPrototype Classifier\n",
            "============================================================\n",
            "\n",
            "Lightweight params: 331,788\n",
            "Final loss: 2.2896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# cifar100 classifier test"
      ],
      "metadata": {
        "id": "iYiQNrq4IWYB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yOH06jkIZwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imagenet classifier test"
      ],
      "metadata": {
        "id": "RVHdF6mDIYu9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h9YZZNpEIbrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# verify t5 determinism"
      ],
      "metadata": {
        "id": "OvNNSDuoP6Zv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264,
          "referenced_widgets": [
            "93623e56458041f8a5f3ea1184525b10",
            "d996bfbf08134faca1ac87d5f0707cab",
            "3970bfb7034f4a308c9cbc1f0edc9919",
            "e62b34cde51e424c8673337d85cff1f4",
            "276b330ece8f4e2497214890d31a6216",
            "687cc8ab87fc4021bbf565a9b2f9c968",
            "e00b52c7c93348588211881521383738",
            "b6e0de8e72a24bb78afc35163c620b26",
            "cff9d83cb8064af2b970374ccfe247db",
            "82199acdbf2b41a09f792981496e17a2",
            "2a83271d941240edbf87c566e1483b36",
            "ea1c21ddc007443cb5032c1a0f1e1767",
            "949a0eb93ac94e44b5c93f54e49077d7",
            "b3ca7540c13d435d8fa18c251a2dedd8",
            "077ae272e6a84974be005ab79ab73854",
            "8afe58320d514057a307a772ce824dd3",
            "ea52fc950d864673b5d5e13499849f4c",
            "87d1a943c54547fa8c0a92a3d920a88b",
            "d12a934d319642d4ba434058f980dacd",
            "56e64e6200d64bb59c2725846e28f5a4",
            "2c9326cc60dd44cc90ae43608b523464",
            "761f096f066c453081450c5b1ab84344",
            "b7ac4d04de5e453ba3bc5fe9a06f3ffe",
            "db3f8a3caa6a4f6fa21b80d31fc0f878",
            "91f5f3d150034d0092c73b4cc73cb9bf",
            "e90df34f8918438d8b5e529c965e1aad",
            "3e94ec3248fc4509a34a05fd3934710f",
            "20a9082106544500a2c4f387fe0ed05c",
            "11dbbce0789746a7a0b0fdd724033ead",
            "3b6dc87c0dd040c19e216d4febb165a4",
            "624a6d97423c4df6aed87c603a5b58ab",
            "0ccc4325f6224db78097770f7125b3de",
            "cebab80c7cdd4d39a8a61d9d51f2c09a",
            "7858ec8a37bc4b83972334f52965ece3",
            "107b2054347440b380b602c03ba59e65",
            "f592738cf9cc4139a977d60a373d9022",
            "5b68e12ed6e94fff9d80102d5427b13a",
            "eb73c8bb34264f31944b593174946647",
            "36d759220e474f088ea0e92522c77c6b",
            "3673bee51ef34e20b89acfbd992833a5",
            "3bcc1254fcc14992bf3d44feb29a9b1d",
            "5620d9e5484f42e9be10ef400f53c623",
            "f03ab363c3704b90844811aa1b773c41",
            "39f0326be81a405692f8f50391a2bc70",
            "50945a476fd149fc8665b8bfb7424b88",
            "e09deb1b562e4ea6b0df75b1604ea7a9",
            "fc03de57e1d74a57825793e630881aa6",
            "ed681c4b8a9241a2a5e7399e1034adaf",
            "dcc3664e6c334315920f16704d7afd31",
            "028efddb88ff42c3aa18e494a39f3823",
            "663fa8918b4e4c70b2ad32f54306339a",
            "081016f140694f3abe9651d6e071f1c3",
            "c3aad3cc1f7b408e8cb20b1d92561786",
            "84b31d8d40ee4cfc9afdc526fb83b496",
            "a65368991ca94f5e83a8e85809e7f3df",
            "b305a3692e77402f81262cd1961e764e",
            "43651cfb4dfa4577abf22009e12665d4",
            "332ee6707bb04d16a64dc807c41063e2",
            "a1b4e30f308941698a5fb1d0bfe53314",
            "6c3ae5a0aad241c0a6c063f5a5da548b",
            "331ee2ebdc56489c8c828da5412b2510",
            "21cbf771b6d84787b085a5c6d6680747",
            "26f31e2951fd466591736429abbb7cda",
            "fd96306970ee4a0894267ac0fce91c71",
            "739cb7090cad45418228941b968232a1",
            "f8cad35261854c7abd8cdc90b9eb0901"
          ]
        },
        "id": "hsZksiBOOpAs",
        "outputId": "1a6d703c-146d-4f7c-b4bc-2f947d4cafd1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93623e56458041f8a5f3ea1184525b10"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea1c21ddc007443cb5032c1a0f1e1767"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7ac4d04de5e453ba3bc5fe9a06f3ffe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7858ec8a37bc4b83972334f52965ece3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50945a476fd149fc8665b8bfb7424b88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b305a3692e77402f81262cd1961e764e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flan-t5-base: 109,628,544 params (frozen, deterministic)\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: LatticePrism Setup - Deterministic T5\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import T5Tokenizer, T5EncoderModel\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Deterministic everything\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Load flan-t5-base FROZEN + EVAL\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "t5 = T5EncoderModel.from_pretrained(\"google/flan-t5-base\").to(device)\n",
        "t5.eval()\n",
        "for p in t5.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(f\"flan-t5-base: {sum(p.numel() for p in t5.parameters()):,} params (frozen, deterministic)\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hq = load_dataset(\"agentlans/high-quality-english-sentences\", split=\"train\")\n",
        "print(hq[0])\n",
        "sentences = [ex['text'] for ex in hq][:5000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "e5aa5506bc3c4efaa194901b4ed7b750",
            "b6b08ca1268d4396bd0a337c7f5621cd",
            "08a8d150e2c14477b4617021172078b0",
            "86118495fec54945bb926647716d657c",
            "1f73298db35a4ee9990556a4d95bdc44",
            "db93187a4d8a4c4d9fa6ba372387be98",
            "9404cd41b3be4564bb50310a1d546493",
            "f321be7795fa49e9b231428063a93a83",
            "bcad0da8e2ce40898f1dd2135f3c4f47",
            "b04a114bd63243dc9220279d5f84a555",
            "746bb8b9c4f940019ab9e47416822f51",
            "9153378fdf65439f872a1e42fd317dc3",
            "eafbf6b026a0469b8c08a11e38f6ffb0",
            "60fffb20d20a4fa9b5c2e4bf4d7f5591",
            "f786a560b59a46d0a5c100f7b4fa2c41",
            "ed7eb04b31644424bd423810bad973a9",
            "b4e82a14425a443691712226c639d5ab",
            "5c342dbd65bb40f699bc6b2abd82c6f1",
            "7b1b68c4828a43759a608fd4305ba824",
            "04e711bdda434a6a970dd5346b8d8394",
            "b531928d66cb4911a36b5fdd6f473ca2",
            "1ffafa765b9c4a66bac16a545a6a0d03",
            "259ef01709d24d27aed3bc1247dc57bb",
            "4634bc8f69804cfa8262b1cf72604de8",
            "3907b8cb9b6445ac9a4f09284628166a",
            "f933996470394b6eb9a06bef9c4aabfc",
            "eaba2e3dce324b1ebc08de60595cba16",
            "ab78cb7232d741b786e9879e8f6168f4",
            "e0e2836bde1241a498f7bae238768696",
            "62e33c3f25594356b52517c56a03bf86",
            "dd892461a7914ce2a895375478d9232a",
            "b72e0fa60e874ee5bc0b983cf32362d2",
            "abe022163ed6453097af606abce5b022",
            "b6c143271f6a48e1b65af307643b5bcc",
            "b8fbc6164c264f9c8ca96dc897e69019",
            "ded397062cdf444787b4a0a598a97d79",
            "c63f14456b374df3a5717c934c9249f0",
            "9bb18995435640a98481c260778097d6",
            "f7e377dd0bb843dbb2f16df9a5d78b40",
            "03888f66d621436288b083dab1364cd8",
            "30fb70f7d0544623a9c784c464b9c50d",
            "d4c4ca734b2c47f694610a6972ecefe2",
            "42ad895f932547ebb631382ed0fb139f",
            "15a9fee3ef91466ba882ea866b0dc225",
            "34cfdbd674e74678a041baf744df3918",
            "2a1e33204fb74258ad99ddd14b866bfd",
            "4a479e0f4e74465fab7b5b4b379039ff",
            "40130299e30643b3acec03f11f798b8a",
            "3e38088f31d646fcaa6938b0535cae76",
            "922794c4956b45f6b417b4dfb1870a17",
            "f9a28cedd351432398345f8f06583c21",
            "3ceb0f27f7664a8aa71ad3224ebae359",
            "6f081e0342234dfb9726411dbb869d60",
            "404d09baea2949a1ae2d10a4aacf6410",
            "c218deb37e564b79821f44df1e3d72c3"
          ]
        },
        "id": "DarZQxilQr3h",
        "outputId": "0674c140-0fc3-436b-ef34-79b64962a196"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5aa5506bc3c4efaa194901b4ed7b750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train.txt.gz:   0%|          | 0.00/85.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9153378fdf65439f872a1e42fd317dc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "test.txt.gz:   0%|          | 0.00/9.49M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "259ef01709d24d27aed3bc1247dc57bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/1534699 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6c143271f6a48e1b65af307643b5bcc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/170522 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34cfdbd674e74678a041baf744df3918"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Soon we dropped into a living forest, where cold-tolerant evergreens and boreal animals still evoke the Canadian heritage of an ecosystem pushed south by glaciers 20,000 years ago.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Verify determinism\n",
        "@torch.no_grad()\n",
        "def encode_t5(texts, max_length=64):\n",
        "    \"\"\"Deterministic T5 encoding.\"\"\"\n",
        "    enc = tokenizer(texts, return_tensors=\"pt\", padding=True,\n",
        "                    truncation=True, max_length=max_length)\n",
        "    hidden = t5(\n",
        "        input_ids=enc.input_ids.to(device),\n",
        "        attention_mask=enc.attention_mask.to(device)\n",
        "    ).last_hidden_state\n",
        "    return hidden, enc.attention_mask.to(device)\n",
        "\n",
        "h1, _ = encode_t5([\"A cat sits on a mat.\"])\n",
        "h2, _ = encode_t5([\"A cat sits on a mat.\"])\n",
        "print(f\"Deterministic: {torch.allclose(h1, h2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wi1A7LcFQtzf",
        "outputId": "5cd66c8d-655d-491c-908a-19e42d622df7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deterministic: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# building the t5 lattice prism"
      ],
      "metadata": {
        "id": "B9XO-3qCTWlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 4: LatticePrism using geofractal components\n",
        "# ================================================\n",
        "\n",
        "from geofractal.router.config import CollectiveConfig\n",
        "from geofractal.router.streams.trainable import TrainableStream\n",
        "from geofractal.router.fusion.builder import (\n",
        "    FusionBuilder, FusionStrategy, build_fusion\n",
        ")\n",
        "from geofractal.router.registry import RouterMailbox\n",
        "\n",
        "HIDDEN_DIM = 768  # flan-t5-base\n",
        "\n",
        "# Collective config\n",
        "collective_config = CollectiveConfig(\n",
        "    feature_dim=HIDDEN_DIM,\n",
        "    fingerprint_dim=64,\n",
        "    num_anchors=32,\n",
        "    num_routes=8,\n",
        "    num_slots=4,\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STREAM BACKBONES (interpret T5 output differently)\n",
        "# ============================================================================\n",
        "\n",
        "class ConvBackbone(nn.Module):\n",
        "    \"\"\"Local pattern extraction via convolutions.\"\"\"\n",
        "    def __init__(self, input_dim=768):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, input_dim, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv1d(input_dim, input_dim, kernel_size=5, padding=2)\n",
        "        self.gate = nn.Linear(input_dim * 2, input_dim)\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, S, D] -> pool to [B, D]\n",
        "        x_t = x.transpose(1, 2)  # [B, D, S]\n",
        "        c1 = torch.relu(self.conv1(x_t)).mean(dim=-1)  # [B, D]\n",
        "        c2 = torch.relu(self.conv2(x_t)).mean(dim=-1)  # [B, D]\n",
        "        concat = torch.cat([c1, c2], dim=-1)  # [B, D*2]\n",
        "        return self.norm(self.gate(concat))  # [B, D]\n",
        "\n",
        "\n",
        "class TransformerBackbone(nn.Module):\n",
        "    \"\"\"Global attention patterns.\"\"\"\n",
        "    def __init__(self, input_dim=768, num_heads=8, num_layers=2):\n",
        "        super().__init__()\n",
        "        layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=num_heads,\n",
        "            dim_feedforward=input_dim * 4, dropout=0.1,\n",
        "            activation='gelu', batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(layer, num_layers=num_layers)\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, S, D] -> first token pooling [B, D]\n",
        "        out = self.transformer(x)\n",
        "        return self.norm(out[:, 0])\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# BUILD STREAMS\n",
        "# ============================================================================\n",
        "\n",
        "conv_stream = TrainableStream(\n",
        "    config=collective_config,\n",
        "    name=\"conv_local\",\n",
        "    backbone=ConvBackbone(HIDDEN_DIM),\n",
        "    input_dim=HIDDEN_DIM,\n",
        "    cooperation_group=\"lattice_prism\",\n",
        ")\n",
        "\n",
        "transformer_stream = TrainableStream(\n",
        "    config=collective_config,\n",
        "    name=\"transformer_global\",\n",
        "    backbone=TransformerBackbone(HIDDEN_DIM),\n",
        "    input_dim=HIDDEN_DIM,\n",
        "    cooperation_group=\"lattice_prism\",\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# BUILD FUSION (test different strategies)\n",
        "# ============================================================================\n",
        "\n",
        "stream_dims = {\n",
        "    'conv_local': HIDDEN_DIM,\n",
        "    'transformer_global': HIDDEN_DIM,\n",
        "}\n",
        "\n",
        "# Pick fusion strategy to test\n",
        "fusion = build_fusion(\n",
        "    stream_dims=stream_dims,\n",
        "    output_dim=HIDDEN_DIM,\n",
        "    strategy=FusionStrategy.GATED,  # Adaptive per-sample\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# LATTICE PRISM COLLECTIVE\n",
        "# ============================================================================\n",
        "\n",
        "class LatticePrism(nn.Module):\n",
        "    \"\"\"Dual-stream collective with proper geofractal routing.\"\"\"\n",
        "\n",
        "    def __init__(self, streams, fusion, num_classes=None):\n",
        "        super().__init__()\n",
        "        self.streams = nn.ModuleDict({s.name: s for s in streams})\n",
        "        self.fusion = fusion\n",
        "        self.mailbox = RouterMailbox()\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        if num_classes:\n",
        "            self.classifier = nn.Linear(HIDDEN_DIM, num_classes)\n",
        "\n",
        "    def forward(self, t5_hidden):\n",
        "        \"\"\"\n",
        "        t5_hidden: [B, S, 768] from frozen flan-t5-base\n",
        "        \"\"\"\n",
        "        self.mailbox.clear()\n",
        "\n",
        "        stream_outputs = {}\n",
        "        stream_fingerprints = {}\n",
        "\n",
        "        for name, stream in self.streams.items():\n",
        "            routed, info = stream(t5_hidden, self.mailbox)\n",
        "            pooled = stream.pool(routed)  # [B, D]\n",
        "            stream_outputs[name] = pooled\n",
        "            stream_fingerprints[name] = stream.fingerprint\n",
        "\n",
        "        # Fuse streams\n",
        "        fused, fusion_info = self.fusion(\n",
        "            stream_outputs,\n",
        "            stream_fingerprints,\n",
        "            return_weights=True\n",
        "        )\n",
        "\n",
        "        outputs = {\n",
        "            'fused': fused,\n",
        "            'streams': stream_outputs,\n",
        "            'fusion_info': fusion_info,\n",
        "        }\n",
        "\n",
        "        if self.num_classes:\n",
        "            outputs['logits'] = self.classifier(fused)\n",
        "            # Individual stream logits for emergence calculation\n",
        "            for name, feat in stream_outputs.items():\n",
        "                outputs[f'logits_{name}'] = self.classifier(feat)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# Instantiate\n",
        "prism = LatticePrism(\n",
        "    streams=[conv_stream, transformer_stream],\n",
        "    fusion=fusion,\n",
        "    num_classes=10,\n",
        ").to(device)\n",
        "\n",
        "# Count params\n",
        "trainable = sum(p.numel() for p in prism.parameters() if p.requires_grad)\n",
        "print(f\"=== LatticePrism ===\")\n",
        "print(f\"Frozen T5:    {sum(p.numel() for p in t5.parameters()):,}\")\n",
        "print(f\"Trainable:    {trainable:,}\")\n",
        "print(f\"Streams:      {list(prism.streams.keys())}\")\n",
        "print(f\"Fusion:       {type(fusion).__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prwkb8mbTbCh",
        "outputId": "f3387617-2846-4572-b0ea-7cd1bdb368a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LatticePrism ===\n",
            "Frozen T5:    109,628,544\n",
            "Trainable:    51,088,996\n",
            "Streams:      ['conv_local', 'transformer_global']\n",
            "Fusion:       GatedFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Test forward pass\n",
        "test_sentences = [\"A cat sits on a mat.\", \"The quick brown fox jumps.\"]\n",
        "h, mask = encode_t5(test_sentences)\n",
        "\n",
        "outputs = prism(h)\n",
        "\n",
        "print(f\"Input: {h.shape}\")\n",
        "print(f\"Fused: {outputs['fused'].shape}\")\n",
        "print(f\"Conv stream: {outputs['streams']['conv_local'].shape}\")\n",
        "print(f\"Transformer stream: {outputs['streams']['transformer_global'].shape}\")\n",
        "print(f\"Logits: {outputs['logits'].shape}\")\n",
        "if outputs['fusion_info']:\n",
        "    print(f\"Fusion weights: {outputs['fusion_info'].weights}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZLs8QA0Vg2W",
        "outputId": "f4141a8d-30d5-4285-acc7-4d3af5682cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: torch.Size([2, 10, 768])\n",
            "Fused: torch.Size([2, 768])\n",
            "Conv stream: torch.Size([2, 768])\n",
            "Transformer stream: torch.Size([2, 768])\n",
            "Logits: torch.Size([2, 10])\n",
            "Fusion weights: tensor([[0.5058, 0.4942],\n",
            "        [0.4890, 0.5110]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Build 2.5M training pairs via multi-seed clustering\n",
        "# ===========================================================\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "\n",
        "NUM_SENTENCES = 5000\n",
        "NUM_SEEDS = 500\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Step 1: Extract T5 embeddings once\n",
        "print(\"Extracting T5 embeddings...\")\n",
        "all_embeddings = []\n",
        "batch_size = 64\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(sentences), batch_size)):\n",
        "        batch = sentences[i:i+batch_size]\n",
        "        h, _ = encode_t5(batch)\n",
        "        pooled = h[:, 0].cpu().numpy()  # First token\n",
        "        all_embeddings.append(pooled)\n",
        "\n",
        "embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "print(f\"Embeddings: {embeddings.shape}\")\n",
        "\n",
        "# Step 2: Cluster with 500 different seeds\n",
        "print(f\"\\nClustering with {NUM_SEEDS} seeds...\")\n",
        "all_labels = []\n",
        "\n",
        "for seed in tqdm(range(NUM_SEEDS)):\n",
        "    kmeans = KMeans(n_clusters=NUM_CLASSES, random_state=seed, n_init=3)\n",
        "    labels = kmeans.fit_predict(embeddings)\n",
        "    all_labels.append(labels)\n",
        "\n",
        "all_labels = np.stack(all_labels, axis=1)  # [5000, 500]\n",
        "print(f\"Labels shape: {all_labels.shape}\")\n",
        "print(f\"Total training pairs: {NUM_SENTENCES * NUM_SEEDS:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185,
          "referenced_widgets": [
            "fd2d70ef21ab4787b9bf9d709ef29dfc",
            "7593c4589f6f416184b8753d23ae25fb",
            "0633ee4a7d6f4bb6a32fea745cbbb6a8",
            "11c420f1a3e54ddfb4c6229d68d9ea76",
            "887f7979aa15415486b70db951de4683",
            "eafec4d93dd54b2aaac17218f516c16b",
            "82862d8d5aad4215bf1d6837dafd8643",
            "9a5a7d8530ef455abdbc425791584835",
            "1ad38ededdf245168b6036a302659dc3",
            "4d12c4d4b48646a09369adcefca4cf54",
            "e269da2174f2462baa03bfc178935dba",
            "558bad2f5b004575b86acbba908ad7bb",
            "2905ba759f944d1e89f947c7b38ac907",
            "ad945a5144be429cbee5463e47a2ac91",
            "4a51890b558c402fa7d67f797ce1312c",
            "8f82d1b2b9ff49aea699736336495648",
            "ae48b85703dc458586e3f1192d2c930a",
            "75d5a8a77b4a4b679a7e687288d82f44",
            "e678f5d8750448b6b260b48907d31c7f",
            "a969e8ad1e604e51af2734c9331c5420",
            "7836ec6a3d084a4f9d606c954a437389",
            "180bb9264de64ef78c9cd8a09b098074"
          ]
        },
        "id": "jwzTTfRzV153",
        "outputId": "b0585197-979b-46c6-cd8a-052c98ac0c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting T5 embeddings...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/79 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd2d70ef21ab4787b9bf9d709ef29dfc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings: (5000, 768)\n",
            "\n",
            "Clustering with 500 seeds...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "558bad2f5b004575b86acbba908ad7bb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels shape: (5000, 500)\n",
            "Total training pairs: 2,500,000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create HuggingFace Dataset\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Option A: Compact format (5000 rows, 500 labels each)\n",
        "ds_compact = Dataset.from_dict({\n",
        "    'sentence': sentences,\n",
        "    'embedding': [emb.tolist() for emb in embeddings],\n",
        "    'labels': [labels.tolist() for labels in all_labels],  # [500] per row\n",
        "})\n",
        "\n",
        "print(f\"Compact dataset: {ds_compact}\")\n",
        "print(f\"Labels per sentence: {len(ds_compact[0]['labels'])}\")\n",
        "\n",
        "# Push compact version\n",
        "ds_compact.push_to_hub(\"AbstractPhil/lattice_prism_t5_500seeds\")\n",
        "print(\"✓ Pushed: AbstractPhil/lattice_prism_t5_500seeds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281,
          "referenced_widgets": [
            "89c108a2fc224b5582413b7ae90614c5",
            "3da9f058956142a9826488c92117b67c",
            "ba0d732f8f334ddfb817185d2b2d41dd",
            "0000018122944adbb520db629a37a1a7",
            "34d08e26f5e14284916c2fb1637a92a3",
            "707d0d9ece794e96b0035b176d13e635",
            "f67cd9a7bd6b4420946aa1e7bfb533ed",
            "6dc607cddd144647b4892fcda14a447c",
            "db6eb841e1064864a6d7639e7b686773",
            "7e10747a80f441d8addee142235f86e3",
            "e533ab05f6324f66897b729f11666abc",
            "27b2d7f3746749588fcddee8cd2ec35a",
            "2659d507c135495d9ae0fca0b141bedc",
            "41ba0fde77f046cc93d351fc27ed07f1",
            "098eead2e888463ba451d3df23c9955c",
            "da53343938464ac19321388801cc0968",
            "e96bd3caf2f4410eb1952fa8601020bd",
            "3103c9fd9a254baeb81f8426c323989c",
            "25098c0e52f64f3dbcdc14868dd88d23",
            "a4105ef2d2e541cba6531514084326c8",
            "039e2bed8341469cb3ebda053db46aef",
            "e35c31d35f6b45ba907011a25e3071ad",
            "b324b5333e4c4abfb21edadb1cc7fbc7",
            "344390f41fa048b8a919249e09b471f3",
            "64c8b35e57c14e6caca76a5ed3bcea6a",
            "1615ca16f62b4381bdf473f7618ebc8f",
            "b17a8dd8f423452f8dbe20e34b3a71b4",
            "63a490ba37f34e17823c7f4dd89cae4b",
            "3f5e142645f14e33bca9afe53c0f724a",
            "71be5798d84d41c3a31e08d5d70bcd84",
            "00224f174f19478ea0ce74e24219c5cb",
            "55533224b07c475c827f7fd53d890ae6",
            "886984036a9d45fe92b4248dceb0ee5b",
            "76990916b7d14e25b2a70a5dab467587",
            "9e8e331cc19a4a5b8877eaed3172553b",
            "d7005628cfb1401e8ccd9822c5de9194",
            "fb1ad6fd777c482aa98508cd814b6948",
            "a723daf59c6e4076aab910fd10052a80",
            "5e8ef30370864ef9a3062a42a48b7f58",
            "2c40a326e2ec49cdae5b5b2376795d50",
            "0dc4cf2f5a63474a94128c7141f37583",
            "ec4e511544a840e5b35d4a23e22e4ad7",
            "f3b8c202967e4039903228cf1b568b37",
            "1be16488676a41fd8f0425d269b0a8e1",
            "2e1e40299acf47d48d7e9857578da3d2",
            "80fb6bccfeae4845b5d32eb6ea14bdfc",
            "2974a38eedda43d78e76415a12d1564b",
            "0e5e55c66417465d8f490b8918e7e6d8",
            "c5232fcc6f6448d98d52c2a6f5f2b9c0",
            "109ef0ac7a4b498d9aae6052fa095c3c",
            "bf1b3b84ffaf44aebe67676c0ce67d0f",
            "c2b32f5916734dbe8271c0230feb0f84",
            "b43e0666011c4443bccb9fb30546b7be",
            "3b70983302b648459abebe22dee6fe9c",
            "118817d4d6ad486989fdcb3d310f9623"
          ]
        },
        "id": "YSzoG2DJV3eZ",
        "outputId": "828846bd-a79c-4968-ee9e-5d52c6605458"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compact dataset: Dataset({\n",
            "    features: ['sentence', 'embedding', 'labels'],\n",
            "    num_rows: 5000\n",
            "})\n",
            "Labels per sentence: 500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ? shards/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89c108a2fc224b5582413b7ae90614c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Creating parquet from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27b2d7f3746749588fcddee8cd2ec35a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b324b5333e4c4abfb21edadb1cc7fbc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "New Data Upload               : |          |  0.00B /  0.00B            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76990916b7d14e25b2a70a5dab467587"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                              :   0%|          |  125kB / 26.3MB            "
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e1e40299acf47d48d7e9857578da3d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pushed: AbstractPhil/lattice_prism_t5_500seeds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: DataLoader that samples (sentence, seed) pairs\n",
        "class MultiSeedDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Each __getitem__ returns (sentence, embedding, label) for random seed.\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, embeddings, all_labels, seeds_per_epoch=1):\n",
        "        self.sentences = sentences\n",
        "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "        self.all_labels = torch.tensor(all_labels, dtype=torch.long)  # [N, 500]\n",
        "        self.seeds_per_epoch = seeds_per_epoch\n",
        "        self.num_seeds = all_labels.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences) * self.seeds_per_epoch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sent_idx = idx % len(self.sentences)\n",
        "        seed_idx = torch.randint(0, self.num_seeds, (1,)).item()\n",
        "\n",
        "        return {\n",
        "            'sentence': self.sentences[sent_idx],\n",
        "            'embedding': self.embeddings[sent_idx],\n",
        "            'label': self.all_labels[sent_idx, seed_idx],\n",
        "            'seed': seed_idx,\n",
        "        }\n",
        "\n",
        "# Test\n",
        "dataset = MultiSeedDataset(sentences, embeddings, all_labels, seeds_per_epoch=10)\n",
        "print(f\"Effective epoch size: {len(dataset):,}\")\n",
        "sample = dataset[0]\n",
        "print(f\"Sample: label={sample['label'].item()}, seed={sample['seed']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7S9l818Wr1x",
        "outputId": "66dcb87d-2571-40e0-ce80-2c1ea6b42a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Effective epoch size: 50,000\n",
            "Sample: label=3, seed=96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: DataLoader with proper collation\n",
        "def collate_multi_seed(batch):\n",
        "    return {\n",
        "        'sentences': [b['sentence'] for b in batch],\n",
        "        'embeddings': torch.stack([b['embedding'] for b in batch]),\n",
        "        'labels': torch.stack([b['label'] for b in batch]),\n",
        "        'seeds': torch.tensor([b['seed'] for b in batch]),\n",
        "    }\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_multi_seed,\n",
        "    num_workers=0,\n",
        ")\n",
        "\n",
        "# Verify\n",
        "batch = next(iter(train_loader))\n",
        "print(f\"Batch sentences: {len(batch['sentences'])}\")\n",
        "print(f\"Batch embeddings: {batch['embeddings'].shape}\")\n",
        "print(f\"Batch labels: {batch['labels'].shape}\")\n",
        "print(f\"Unique seeds in batch: {batch['seeds'].unique().tolist()[:10]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1EUTj3uDWtkd",
        "outputId": "ee388564-4e77-4a36-aa60-40d37d874c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch sentences: 32\n",
            "Batch embeddings: torch.Size([32, 768])\n",
            "Batch labels: torch.Size([32])\n",
            "Unique seeds in batch: [26, 49, 50, 104, 123, 124, 153, 165, 169, 171]...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gated fusion"
      ],
      "metadata": {
        "id": "IVm-fBrUa-gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Train LatticePrism on multi-seed clusters\n",
        "# ===================================================\n",
        "\n",
        "optimizer = torch.optim.AdamW(prism.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "EVAL_EVERY = 500  # steps\n",
        "\n",
        "def evaluate_batch(model, embeddings, labels):\n",
        "    \"\"\"Quick eval on a batch.\"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Use pre-computed embeddings directly (skip T5)\n",
        "        # But we need sequence format for prism\n",
        "        h = embeddings.unsqueeze(1).expand(-1, 10, -1).to(device)  # [B, S, D]\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(h)\n",
        "\n",
        "        pred_fused = outputs['logits'].argmax(dim=-1)\n",
        "        pred_conv = outputs['logits_conv_local'].argmax(dim=-1)\n",
        "        pred_trans = outputs['logits_transformer_global'].argmax(dim=-1)\n",
        "\n",
        "        acc_fused = (pred_fused == labels).float().mean().item()\n",
        "        acc_conv = (pred_conv == labels).float().mean().item()\n",
        "        acc_trans = (pred_trans == labels).float().mean().item()\n",
        "\n",
        "        best_individual = max(acc_conv, acc_trans)\n",
        "        rho = acc_fused / best_individual if best_individual > 0 else 0\n",
        "\n",
        "    return acc_fused, acc_conv, acc_trans, rho\n",
        "\n",
        "# Training\n",
        "print(f\"{'Step':>6} | {'Loss':>6} | {'Fused':>6} | {'Conv':>6} | {'Trans':>6} | {'ρ':>5}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "history = []\n",
        "step = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    prism.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        # Use pre-computed embeddings (faster than re-encoding)\n",
        "        embeddings = batch['embeddings'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        # Expand to sequence format for streams\n",
        "        h = embeddings.unsqueeze(1).expand(-1, 10, -1)  # [B, 10, 768]\n",
        "\n",
        "        outputs = prism(h)\n",
        "        loss = criterion(outputs['logits'], labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(prism.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        # Periodic eval\n",
        "        if step % EVAL_EVERY == 0:\n",
        "            acc_f, acc_c, acc_t, rho = evaluate_batch(\n",
        "                prism, batch['embeddings'], batch['labels']\n",
        "            )\n",
        "            history.append({\n",
        "                'step': step, 'loss': loss.item(),\n",
        "                'fused': acc_f, 'conv': acc_c, 'trans': acc_t, 'rho': rho\n",
        "            })\n",
        "            print(f\"{step:>6} | {loss.item():>6.3f} | {acc_f*100:>5.1f}% | \"\n",
        "                  f\"{acc_c*100:>5.1f}% | {acc_t*100:>5.1f}% | {rho:>5.3f}\")\n",
        "\n",
        "    # End of epoch summary\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    acc_f, acc_c, acc_t, rho = evaluate_batch(prism, batch['embeddings'], batch['labels'])\n",
        "\n",
        "    print(f\"[Epoch {epoch+1:>2}] Loss: {avg_loss:.3f} | \"\n",
        "          f\"Fused: {acc_f*100:.1f}% | Conv: {acc_c*100:.1f}% | Trans: {acc_t*100:.1f}% | ρ: {rho:.3f}\")\n",
        "\n",
        "print(\"-\" * 55)\n",
        "best_rho = max(h['rho'] for h in history) if history else 0\n",
        "print(f\"Best ρ: {best_rho:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y79txcpFYw1C",
        "outputId": "2c187b79-97f6-4697-f3ed-752f343cd32b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step |   Loss |  Fused |   Conv |  Trans |     ρ\n",
            "-------------------------------------------------------\n",
            "   500 |  2.231 |  18.8% |   6.2% |   6.2% | 3.000\n",
            "  1000 |  2.285 |   6.2% |   6.2% |   6.2% | 1.000\n",
            "  1500 |  2.363 |   3.1% |  15.6% |  12.5% | 0.200\n",
            "[Epoch  1] Loss: 2.329 | Fused: 18.8% | Conv: 6.2% | Trans: 6.2% | ρ: 3.000\n",
            "  2000 |  2.249 |  21.9% |   3.1% |  12.5% | 1.750\n",
            "  2500 |  2.299 |   9.4% |   9.4% |   3.1% | 1.000\n",
            "  3000 |  2.317 |   9.4% |  18.8% |   6.2% | 0.500\n",
            "[Epoch  2] Loss: 2.310 | Fused: 0.0% | Conv: 18.8% | Trans: 18.8% | ρ: 0.000\n",
            "  3500 |  2.310 |   9.4% |  12.5% |  12.5% | 0.750\n",
            "  4000 |  2.303 |   9.4% |  12.5% |   9.4% | 0.750\n",
            "  4500 |  2.302 |  15.6% |   6.2% |  15.6% | 1.000\n",
            "[Epoch  3] Loss: 2.307 | Fused: 0.0% | Conv: 0.0% | Trans: 0.0% | ρ: 0.000\n",
            "  5000 |  2.330 |  18.8% |  15.6% |   9.4% | 1.200\n",
            "  5500 |  2.316 |   6.2% |  12.5% |  21.9% | 0.286\n",
            "  6000 |  2.257 |  18.8% |  12.5% |  12.5% | 1.500\n",
            "[Epoch  4] Loss: 2.304 | Fused: 0.0% | Conv: 12.5% | Trans: 12.5% | ρ: 0.000\n",
            "  6500 |  2.290 |  18.8% |   9.4% |  12.5% | 1.500\n",
            "  7000 |  2.388 |   6.2% |   6.2% |   6.2% | 1.000\n",
            "  7500 |  2.238 |  31.2% |  12.5% |  12.5% | 2.500\n",
            "[Epoch  5] Loss: 2.302 | Fused: 12.5% | Conv: 0.0% | Trans: 6.2% | ρ: 2.000\n",
            "  8000 |  2.241 |  18.8% |   9.4% |   9.4% | 2.000\n",
            "  8500 |  2.267 |   9.4% |   9.4% |   9.4% | 1.000\n",
            "  9000 |  2.334 |   3.1% |   0.0% |   3.1% | 1.000\n",
            "[Epoch  6] Loss: 2.299 | Fused: 12.5% | Conv: 18.8% | Trans: 0.0% | ρ: 0.667\n",
            "  9500 |  2.284 |  12.5% |   9.4% |  15.6% | 0.800\n",
            " 10000 |  2.302 |  15.6% |   6.2% |  12.5% | 1.250\n",
            " 10500 |  2.288 |  12.5% |  12.5% |   9.4% | 1.000\n",
            "[Epoch  7] Loss: 2.299 | Fused: 0.0% | Conv: 12.5% | Trans: 12.5% | ρ: 0.000\n",
            " 11000 |  2.238 |  25.0% |   9.4% |   6.2% | 2.667\n",
            " 11500 |  2.285 |   9.4% |   6.2% |   6.2% | 1.500\n",
            " 12000 |  2.343 |   6.2% |  12.5% |   6.2% | 0.500\n",
            " 12500 |  2.244 |  12.5% |   6.2% |  12.5% | 1.000\n",
            "[Epoch  8] Loss: 2.298 | Fused: 12.5% | Conv: 25.0% | Trans: 0.0% | ρ: 0.500\n",
            " 13000 |  2.290 |  12.5% |  18.8% |   9.4% | 0.667\n",
            " 13500 |  2.227 |  15.6% |   6.2% |  12.5% | 1.250\n",
            " 14000 |  2.351 |   6.2% |  15.6% |   9.4% | 0.400\n",
            "[Epoch  9] Loss: 2.298 | Fused: 6.2% | Conv: 6.2% | Trans: 18.8% | ρ: 0.333\n",
            " 14500 |  2.359 |   6.2% |  12.5% |   6.2% | 0.500\n",
            " 15000 |  2.300 |  15.6% |   9.4% |   9.4% | 1.667\n",
            " 15500 |  2.243 |  15.6% |   3.1% |   3.1% | 5.000\n",
            "[Epoch 10] Loss: 2.296 | Fused: 6.2% | Conv: 12.5% | Trans: 25.0% | ρ: 0.250\n",
            "-------------------------------------------------------\n",
            "Best ρ: 5.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## gated fingerprint fusion training"
      ],
      "metadata": {
        "id": "y7Us1WIUa601"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Rebuild with FingerprintGuidedFusion\n",
        "# =============================================\n",
        "\n",
        "from geofractal.router.fusion.builder import FusionStrategy, build_fusion\n",
        "\n",
        "# Rebuild fusion with fingerprint guidance\n",
        "fusion_fp = build_fusion(\n",
        "    stream_dims=stream_dims,\n",
        "    output_dim=HIDDEN_DIM,\n",
        "    strategy=FusionStrategy.FINGERPRINT,\n",
        "    fingerprint_dim=64,  # Match our config\n",
        ")\n",
        "\n",
        "# New prism instance\n",
        "prism_fp = LatticePrism(\n",
        "    streams=[\n",
        "        TrainableStream(\n",
        "            config=collective_config,\n",
        "            name=\"conv_local\",\n",
        "            backbone=ConvBackbone(HIDDEN_DIM),\n",
        "            input_dim=HIDDEN_DIM,\n",
        "            cooperation_group=\"lattice_prism_fp\",\n",
        "        ),\n",
        "        TrainableStream(\n",
        "            config=collective_config,\n",
        "            name=\"transformer_global\",\n",
        "            backbone=TransformerBackbone(HIDDEN_DIM),\n",
        "            input_dim=HIDDEN_DIM,\n",
        "            cooperation_group=\"lattice_prism_fp\",\n",
        "        ),\n",
        "    ],\n",
        "    fusion=fusion_fp,\n",
        "    num_classes=10,\n",
        ").to(device)\n",
        "\n",
        "trainable = sum(p.numel() for p in prism_fp.parameters() if p.requires_grad)\n",
        "print(f\"=== LatticePrism (FingerprintGuided) ===\")\n",
        "print(f\"Trainable: {trainable:,}\")\n",
        "print(f\"Fusion: {type(fusion_fp).__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_o8zeAD-bA9m",
        "outputId": "56723554-98f2-4bf4-daf3-b4ea4447adb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== LatticePrism (FingerprintGuided) ===\n",
            "Trainable: 50,598,242\n",
            "Fusion: FingerprintGuidedFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Attention fusion - cross-stream communication\n",
        "fusion_attn = build_fusion(\n",
        "    stream_dims=stream_dims,\n",
        "    output_dim=HIDDEN_DIM,\n",
        "    strategy=FusionStrategy.ATTENTION,\n",
        ")\n",
        "\n",
        "prism_attn = LatticePrism(\n",
        "    streams=[\n",
        "        TrainableStream(\n",
        "            config=collective_config,\n",
        "            name=\"conv_local\",\n",
        "            backbone=ConvBackbone(HIDDEN_DIM),\n",
        "            input_dim=HIDDEN_DIM,\n",
        "            cooperation_group=\"lattice_prism_attn\",\n",
        "        ),\n",
        "        TrainableStream(\n",
        "            config=collective_config,\n",
        "            name=\"transformer_global\",\n",
        "            backbone=TransformerBackbone(HIDDEN_DIM),\n",
        "            input_dim=HIDDEN_DIM,\n",
        "            cooperation_group=\"lattice_prism_attn\",\n",
        "        ),\n",
        "    ],\n",
        "    fusion=fusion_attn,\n",
        "    num_classes=10,\n",
        ").to(device)\n",
        "\n",
        "print(f\"Fusion: {type(fusion_attn).__name__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtarBxeveEsB",
        "outputId": "41870b9a-2986-45c0-9b7b-24d7fb3c2a75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fusion: AttentionFusion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Train with fingerprint-guided fusion\n",
        "optimizer = torch.optim.AdamW(prism_fp.parameters(), lr=1e-4, weight_decay=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"{'Step':>6} | {'Loss':>6} | {'Fused':>6} | {'Conv':>6} | {'Trans':>6} | {'ρ':>5}\")\n",
        "print(\"-\" * 55)\n",
        "\n",
        "history_fp = []\n",
        "step = 0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    prism_fp.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        embeddings = batch['embeddings'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        h = embeddings.unsqueeze(1).expand(-1, 10, -1)\n",
        "\n",
        "        outputs = prism_fp(h)\n",
        "        loss = criterion(outputs['logits'], labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(prism_fp.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        step += 1\n",
        "\n",
        "        if step % EVAL_EVERY == 0:\n",
        "            prism_fp.eval()\n",
        "            with torch.no_grad():\n",
        "                h_eval = batch['embeddings'].unsqueeze(1).expand(-1, 10, -1).to(device)\n",
        "                out = prism_fp(h_eval)\n",
        "\n",
        "                acc_f = (out['logits'].argmax(-1) == labels).float().mean().item()\n",
        "                acc_c = (out['logits_conv_local'].argmax(-1) == labels).float().mean().item()\n",
        "                acc_t = (out['logits_transformer_global'].argmax(-1) == labels).float().mean().item()\n",
        "\n",
        "                best = max(acc_c, acc_t)\n",
        "                rho = acc_f / best if best > 0 else 0\n",
        "\n",
        "            history_fp.append({'step': step, 'rho': rho, 'fused': acc_f})\n",
        "            print(f\"{step:>6} | {loss.item():>6.3f} | {acc_f*100:>5.1f}% | \"\n",
        "                  f\"{acc_c*100:>5.1f}% | {acc_t*100:>5.1f}% | {rho:>5.3f}\")\n",
        "            prism_fp.train()\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"[Epoch {epoch+1:>2}] Loss: {avg_loss:.3f}\")\n",
        "\n",
        "print(\"-\" * 55)\n",
        "best_rho_fp = max(h['rho'] for h in history_fp) if history_fp else 0\n",
        "print(f\"Best ρ (FingerprintGuided): {best_rho_fp:.3f}\")\n",
        "print(f\"Best ρ (Gated):             {max(h['rho'] for h in history):.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "R7Zw8fzKbD2n",
        "outputId": "ceb79a24-e743-465f-e5f3-846ab6f3b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Step |   Loss |  Fused |   Conv |  Trans |     ρ\n",
            "-------------------------------------------------------\n",
            "   500 |  2.268 |  15.6% |  18.8% |   0.0% | 0.833\n",
            "  1000 |  2.330 |  12.5% |   6.2% |   9.4% | 1.333\n",
            "  1500 |  2.275 |  12.5% |   6.2% |   6.2% | 2.000\n",
            "[Epoch  1] Loss: 2.298\n",
            "  2000 |  2.235 |  15.6% |   6.2% |  18.8% | 0.833\n",
            "  2500 |  2.300 |   6.2% |  12.5% |  12.5% | 0.500\n",
            "  3000 |  2.337 |   9.4% |   3.1% |  25.0% | 0.375\n",
            "[Epoch  2] Loss: 2.297\n",
            "  3500 |  2.245 |  21.9% |  12.5% |  12.5% | 1.750\n",
            "  4000 |  2.363 |   6.2% |  18.8% |   3.1% | 0.333\n",
            "  4500 |  2.283 |  15.6% |  21.9% |   9.4% | 0.714\n",
            "[Epoch  3] Loss: 2.295\n",
            "  5000 |  2.340 |   3.1% |   6.2% |   6.2% | 0.500\n",
            "  5500 |  2.294 |   9.4% |   9.4% |  12.5% | 0.750\n",
            "  6000 |  2.321 |   6.2% |   9.4% |  12.5% | 0.500\n",
            "[Epoch  4] Loss: 2.296\n",
            "  6500 |  2.315 |   6.2% |   9.4% |  15.6% | 0.400\n",
            "  7000 |  2.257 |  18.8% |   3.1% |   9.4% | 2.000\n",
            "  7500 |  2.302 |  15.6% |   3.1% |  15.6% | 1.000\n",
            "[Epoch  5] Loss: 2.296\n",
            "  8000 |  2.297 |  18.8% |  12.5% |   0.0% | 1.500\n",
            "  8500 |  2.233 |  15.6% |   3.1% |   3.1% | 5.000\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1331395211.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# collective tower tests"
      ],
      "metadata": {
        "id": "f8qt5-tnh405"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 tower cifar 10"
      ],
      "metadata": {
        "id": "CWbgylj4oqE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-10 Tower Collective Classifier\n",
        "=====================================\n",
        "\n",
        "Each geometric tower interprets the same image differently.\n",
        "Fused interpretations → classification.\n",
        "\n",
        "Run in Colab with geofractal pre-installed.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Geofractal imports\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.base_tower import BaseTower\n",
        "from geofractal.router.components.transformer_component import (\n",
        "    TransformerConfig,\n",
        "    TransformerVariant,\n",
        "    ActivationType,\n",
        "    PreNormBlock,\n",
        ")\n",
        "from geofractal.router.components.rope_component import (\n",
        "    RoPE,\n",
        "    DualRoPE,\n",
        "    TriRoPE,\n",
        "    QuadRoPE,\n",
        "    BeatrixRoPE,\n",
        "    CantorRoPE,\n",
        ")\n",
        "from geofractal.router.components.address_component import (\n",
        "    AddressComponent,\n",
        "    SimplexAddressComponent,\n",
        "    SphericalAddressComponent,\n",
        "    FractalAddressComponent,\n",
        "    CantorAddressComponent,\n",
        ")\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# =============================================================================\n",
        "# GEOMETRY TYPES\n",
        "# =============================================================================\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "class GeometryType(str, Enum):\n",
        "    CANTOR = \"cantor\"\n",
        "    BEATRIX = \"beatrix\"\n",
        "    HELIX = \"helix\"\n",
        "    SIMPLEX = \"simplex\"\n",
        "    FRACTAL = \"fractal\"\n",
        "    STANDARD = \"standard\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GEOMETRIC TOWER (simplified for CIFAR)\n",
        "# =============================================================================\n",
        "\n",
        "def build_rope(name: str, geometry: GeometryType, head_dim: int) -> nn.Module:\n",
        "    \"\"\"Build geometry-specific RoPE.\"\"\"\n",
        "    if geometry == GeometryType.CANTOR:\n",
        "        return CantorRoPE(f'{name}_rope', head_dim=head_dim, levels=5, mode='hybrid')\n",
        "    elif geometry == GeometryType.BEATRIX:\n",
        "        return BeatrixRoPE(f'{name}_rope', head_dim=head_dim, levels=5)\n",
        "    elif geometry == GeometryType.HELIX:\n",
        "        return TriRoPE(f'{name}_rope', head_dim=head_dim,\n",
        "                       theta_alpha=10000.0, theta_beta=5000.0, theta_gamma=2500.0)\n",
        "    elif geometry == GeometryType.SIMPLEX:\n",
        "        return QuadRoPE(f'{name}_rope', head_dim=head_dim,\n",
        "                        theta_w=10000.0, theta_x=7500.0, theta_y=5000.0, theta_z=2500.0)\n",
        "    elif geometry == GeometryType.FRACTAL:\n",
        "        return DualRoPE(f'{name}_rope', head_dim=head_dim,\n",
        "                        theta_primary=10000.0, theta_secondary=3000.0)\n",
        "    else:\n",
        "        return RoPE(f'{name}_rope', head_dim=head_dim, theta=10000.0)\n",
        "\n",
        "\n",
        "class GeometricTower(BaseTower):\n",
        "    \"\"\"Lightweight tower for CIFAR-10.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int = 256,\n",
        "        depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        geometry: GeometryType = GeometryType.STANDARD,\n",
        "        head_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.geometry = geometry\n",
        "\n",
        "        config = TransformerConfig(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            variant=TransformerVariant.PRENORM,\n",
        "            activation=ActivationType.GELU,\n",
        "            dropout=0.1,\n",
        "            depth=depth,\n",
        "        )\n",
        "\n",
        "        # Geometry-specific RoPE\n",
        "        self.attach('rope', build_rope(name, geometry, head_dim))\n",
        "\n",
        "        # Transformer blocks\n",
        "        for i in range(depth):\n",
        "            self.append(PreNormBlock(f'{name}_block_{i}', config=config, block_idx=i))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns pooled opinion [B, D].\"\"\"\n",
        "        for block in self.stages:\n",
        "            x, _ = block(x)\n",
        "        x = self['norm'](x)\n",
        "        return x.mean(dim=1)  # Global average pool\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARTowerClassifier(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-10 classifier using geometric tower collective.\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → Towers (same input) → Fuse → Classify\n",
        "    \"\"\"\n",
        "\n",
        "    TOWER_CONFIGS = [\n",
        "        ('cantor', GeometryType.CANTOR),\n",
        "        ('beatrix', GeometryType.BEATRIX),\n",
        "        ('helix', GeometryType.HELIX),\n",
        "        ('simplex', GeometryType.SIMPLEX),\n",
        "        ('fractal', GeometryType.FRACTAL),\n",
        "        ('standard', GeometryType.STANDARD),\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 10,\n",
        "        tower_subset: Optional[list] = None,\n",
        "    ):\n",
        "        super().__init__('cifar_classifier', strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Select towers\n",
        "        if tower_subset:\n",
        "            configs = [(n, g) for n, g in self.TOWER_CONFIGS if n in tower_subset]\n",
        "        else:\n",
        "            configs = self.TOWER_CONFIGS\n",
        "\n",
        "        self._tower_names = [c[0] for c in configs]\n",
        "        num_towers = len(configs)\n",
        "\n",
        "        # Patch embedding: [B, 3, 32, 32] → [B, num_patches, dim]\n",
        "        num_patches = (32 // patch_size) ** 2  # 64 patches for patch_size=4\n",
        "        self.attach('patch_embed', nn.Conv2d(3, dim, patch_size, patch_size))\n",
        "\n",
        "        # Position embedding as a proper module\n",
        "        class PosEmbed(nn.Module):\n",
        "            def __init__(self, num_patches, dim):\n",
        "                super().__init__()\n",
        "                self.pos = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "            def forward(self, x):\n",
        "                return x + self.pos\n",
        "\n",
        "        self.attach('pos_embed', PosEmbed(num_patches, dim))\n",
        "\n",
        "        # Build towers\n",
        "        for name, geometry in configs:\n",
        "            tower = GeometricTower(\n",
        "                name=name,\n",
        "                dim=dim,\n",
        "                depth=tower_depth,\n",
        "                num_heads=num_heads,\n",
        "                geometry=geometry,\n",
        "            )\n",
        "            self.attach(name, tower)\n",
        "\n",
        "        # Fusion\n",
        "        self.attach('fusion', AdaptiveFusion(\n",
        "            'tower_fusion', num_inputs=num_towers, in_features=dim\n",
        "        ))\n",
        "\n",
        "        # Classification head\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Store config\n",
        "        self.objects['tower_names'] = self._tower_names\n",
        "        self.objects['num_patches'] = num_patches\n",
        "\n",
        "    @property\n",
        "    def tower_names(self):\n",
        "        return self.objects['tower_names']\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, 3, 32, 32] CIFAR images\n",
        "\n",
        "        Returns:\n",
        "            logits: [B, num_classes]\n",
        "        \"\"\"\n",
        "        B = x.shape[0]\n",
        "\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](x)  # [B, dim, H/p, W/p]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, dim]\n",
        "        x = self['pos_embed'](x)  # Add position embedding\n",
        "\n",
        "        # Each tower processes same input\n",
        "        opinions = []\n",
        "        for name in self._tower_names:\n",
        "            opinion = self[name](x)  # [B, dim]\n",
        "            opinions.append(opinion)\n",
        "\n",
        "        # Fuse\n",
        "        fused = self['fusion'](*opinions)  # [B, dim]\n",
        "\n",
        "        # Classify\n",
        "        logits = self['classifier'](fused)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_tower_opinions(self, x: Tensor) -> dict:\n",
        "        \"\"\"Debug: get individual tower outputs.\"\"\"\n",
        "        B = x.shape[0]\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        return {name: self[name](x) for name in self._tower_names}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update progress bar\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/total:.3f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits = model(images)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"CIFAR-10 Tower Collective Classifier\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 20\n",
        "    LR = 3e-4\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 2\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 4\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Model\n",
        "    model = CIFARTowerClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=10,\n",
        "        tower_subset=None,  # Use all 6 towers\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.tower_names}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.tower_names:\n",
        "        tower_params = sum(p.numel() for p in model[name].parameters())\n",
        "        print(f\"  {name}: {tower_params:,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"-\" * 60)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate(model, test_loader, device)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            marker = \" *\"\n",
        "        else:\n",
        "            marker = \"\"\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Analyze tower contributions\n",
        "    print(\"\\nTower Analysis (test batch):\")\n",
        "    model.eval()\n",
        "    test_images, test_labels = next(iter(test_loader))\n",
        "    test_images = test_images[:16].to(device)\n",
        "\n",
        "    opinions = model.get_tower_opinions(test_images)\n",
        "    print(\"Opinion norms:\")\n",
        "    for name, op in opinions.items():\n",
        "        print(f\"  {name}: {op.norm(dim=-1).mean():.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "p6za2OZTh8XA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f446777c-1c90-454d-a6a2-762aa2a7c3a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CIFAR-10 Tower Collective Classifier\n",
            "============================================================\n",
            "Device: cuda\n",
            "Train: 50000, Test: 10000\n",
            "\n",
            "Towers: ['cantor', 'beatrix', 'helix', 'simplex', 'fractal', 'standard']\n",
            "Total parameters: 9,519,577\n",
            "  cantor: 1,578,497\n",
            "  beatrix: 1,578,497\n",
            "  helix: 1,578,564\n",
            "  simplex: 1,578,503\n",
            "  fractal: 1,578,497\n",
            "  standard: 1,578,496\n",
            "\n",
            "------------------------------------------------------------\n",
            "Training\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/20 | Loss: 1.7204 | Train: 36.8% | Test: 46.3% * | Time: 27.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/20 | Loss: 1.3928 | Train: 49.7% | Test: 53.3% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/20 | Loss: 1.2549 | Train: 54.6% | Test: 56.4% * | Time: 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/20 | Loss: 1.1694 | Train: 58.0% | Test: 60.1% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/20 | Loss: 1.1052 | Train: 60.2% | Test: 61.8% * | Time: 27.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/20 | Loss: 1.0456 | Train: 62.4% | Test: 61.5% | Time: 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/20 | Loss: 1.0059 | Train: 64.0% | Test: 64.2% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/20 | Loss: 0.9652 | Train: 65.3% | Test: 66.1% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/20 | Loss: 0.9213 | Train: 66.9% | Test: 67.5% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Loss: 0.8840 | Train: 68.4% | Test: 68.4% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Loss: 0.8449 | Train: 69.9% | Test: 69.6% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Loss: 0.8091 | Train: 71.2% | Test: 71.1% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Loss: 0.7774 | Train: 72.5% | Test: 71.4% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Loss: 0.7445 | Train: 73.6% | Test: 72.0% * | Time: 27.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Loss: 0.7134 | Train: 74.5% | Test: 73.5% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Loss: 0.6945 | Train: 75.5% | Test: 73.6% * | Time: 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Loss: 0.6697 | Train: 76.3% | Test: 74.2% * | Time: 27.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Loss: 0.6533 | Train: 76.9% | Test: 74.6% * | Time: 27.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Loss: 0.6389 | Train: 77.5% | Test: 75.0% * | Time: 27.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Loss: 0.6359 | Train: 77.6% | Test: 74.9% | Time: 27.5s\n",
            "\n",
            "============================================================\n",
            "Best Test Accuracy: 75.00%\n",
            "============================================================\n",
            "\n",
            "Tower Analysis (test batch):\n",
            "Opinion norms:\n",
            "  cantor: 8.636\n",
            "  beatrix: 9.319\n",
            "  helix: 10.891\n",
            "  simplex: 10.442\n",
            "  fractal: 9.512\n",
            "  standard: 10.343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cantor pair + simplex pair, 4 towers"
      ],
      "metadata": {
        "id": "MLqW2rrjotql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-10 Dual-Pair Tower Classifier\n",
        "====================================\n",
        "\n",
        "4 towers with Cantor/Simplex RoPE × Normal/Inverted fingerprints:\n",
        "    - Cantor + Cantor FP\n",
        "    - Cantor + Inverted Cantor FP\n",
        "    - Simplex + Cantor FP\n",
        "    - Simplex + Inverted Cantor FP\n",
        "\n",
        "Fusion hierarchy:\n",
        "    Cantor pair → fuse → ┐\n",
        "                         ├─ concat → classify\n",
        "    Simplex pair → fuse → ┘\n",
        "\n",
        "Per-class analysis tracks which towers excel at which classes.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "# Geofractal imports\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.base_tower import BaseTower\n",
        "from geofractal.router.components.transformer_component import (\n",
        "    TransformerConfig,\n",
        "    TransformerVariant,\n",
        "    ActivationType,\n",
        "    PreNormBlock,\n",
        ")\n",
        "from geofractal.router.components.rope_component import (\n",
        "    QuadRoPE,\n",
        "    CantorRoPE,\n",
        ")\n",
        "from geofractal.router.components.address_component import CantorAddressComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# =============================================================================\n",
        "# INVERTED CANTOR ADDRESS\n",
        "# =============================================================================\n",
        "\n",
        "class InvertedCantorAddressComponent(CantorAddressComponent):\n",
        "    \"\"\"Cantor address with inverted fingerprint (1 - cantor_value).\"\"\"\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        \"\"\"Inverted fingerprint.\"\"\"\n",
        "        base_fp = super().fingerprint\n",
        "        return 1.0 - base_fp\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GEOMETRIC TOWER\n",
        "# =============================================================================\n",
        "\n",
        "class DualTower(BaseTower):\n",
        "    \"\"\"\n",
        "    Tower with configurable RoPE and Address.\n",
        "\n",
        "    Args:\n",
        "        rope_type: 'cantor' or 'simplex'\n",
        "        inverted: If True, use inverted Cantor fingerprint\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int = 256,\n",
        "        depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        rope_type: str = 'cantor',\n",
        "        inverted: bool = False,\n",
        "        head_dim: int = 64,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.rope_type = rope_type\n",
        "        self.inverted = inverted\n",
        "\n",
        "        config = TransformerConfig(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            variant=TransformerVariant.PRENORM,\n",
        "            activation=ActivationType.GELU,\n",
        "            dropout=0.1,\n",
        "            depth=depth,\n",
        "        )\n",
        "\n",
        "        # RoPE based on type\n",
        "        if rope_type == 'cantor':\n",
        "            self.attach('rope', CantorRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim, levels=5, mode='hybrid'\n",
        "            ))\n",
        "        elif rope_type == 'simplex':\n",
        "            self.attach('rope', QuadRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim,\n",
        "                theta_w=10000.0, theta_x=7500.0, theta_y=5000.0, theta_z=2500.0,\n",
        "                augmentation='simplex'\n",
        "            ))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown rope_type: {rope_type}\")\n",
        "\n",
        "        # Address - Cantor or Inverted Cantor\n",
        "        if inverted:\n",
        "            self.attach('address', InvertedCantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "        else:\n",
        "            self.attach('address', CantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "\n",
        "        # Transformer blocks\n",
        "        for i in range(depth):\n",
        "            self.append(PreNormBlock(f'{name}_block_{i}', config=config, block_idx=i))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        return F.normalize(self['address'].fingerprint, dim=-1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns pooled opinion [B, D].\"\"\"\n",
        "        for block in self.stages:\n",
        "            x, _ = block(x)\n",
        "        x = self['norm'](x)\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DUAL-PAIR CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "class DualPairClassifier(BaseRouter):\n",
        "    \"\"\"\n",
        "    4-tower classifier with dual-pair fusion.\n",
        "\n",
        "    Towers:\n",
        "        cantor_pos:   Cantor RoPE + Cantor FP\n",
        "        cantor_neg:   Cantor RoPE + Inverted Cantor FP\n",
        "        simplex_pos:  Simplex RoPE + Cantor FP\n",
        "        simplex_neg:  Simplex RoPE + Inverted Cantor FP\n",
        "\n",
        "    Fusion:\n",
        "        cantor_pair = fuse(cantor_pos, cantor_neg)\n",
        "        simplex_pair = fuse(simplex_pos, simplex_neg)\n",
        "        final = concat(cantor_pair, simplex_pair) → classify\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 10,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__('dual_pair_classifier', strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.attach('patch_embed', nn.Conv2d(3, dim, patch_size, patch_size))\n",
        "\n",
        "        class PosEmbed(nn.Module):\n",
        "            def __init__(self, num_patches, dim):\n",
        "                super().__init__()\n",
        "                self.pos = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "            def forward(self, x):\n",
        "                return x + self.pos\n",
        "\n",
        "        self.attach('pos_embed', PosEmbed(num_patches, dim))\n",
        "\n",
        "        # 4 Towers\n",
        "        self.attach('cantor_pos', DualTower(\n",
        "            'cantor_pos', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "            rope_type='cantor', inverted=False, fingerprint_dim=fingerprint_dim\n",
        "        ))\n",
        "        self.attach('cantor_neg', DualTower(\n",
        "            'cantor_neg', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "            rope_type='cantor', inverted=True, fingerprint_dim=fingerprint_dim\n",
        "        ))\n",
        "        self.attach('simplex_pos', DualTower(\n",
        "            'simplex_pos', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "            rope_type='simplex', inverted=False, fingerprint_dim=fingerprint_dim\n",
        "        ))\n",
        "        self.attach('simplex_neg', DualTower(\n",
        "            'simplex_neg', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "            rope_type='simplex', inverted=True, fingerprint_dim=fingerprint_dim\n",
        "        ))\n",
        "\n",
        "        # Pair fusions\n",
        "        self.attach('cantor_fusion', AdaptiveFusion(\n",
        "            'cantor_fusion', num_inputs=2, in_features=dim\n",
        "        ))\n",
        "        self.attach('simplex_fusion', AdaptiveFusion(\n",
        "            'simplex_fusion', num_inputs=2, in_features=dim\n",
        "        ))\n",
        "\n",
        "        # Final classifier on concatenated pairs\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim * 2),\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Tower names for iteration\n",
        "        self.objects['tower_names'] = ['cantor_pos', 'cantor_neg', 'simplex_pos', 'simplex_neg']\n",
        "        self.objects['num_patches'] = num_patches\n",
        "\n",
        "        # Class-specific tracking\n",
        "        self.objects['class_stats'] = None\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self.objects['tower_names']\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, 3, 32, 32] CIFAR images\n",
        "\n",
        "        Returns:\n",
        "            logits: [B, num_classes]\n",
        "        \"\"\"\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        # Tower opinions\n",
        "        cantor_pos = self['cantor_pos'](x)\n",
        "        cantor_neg = self['cantor_neg'](x)\n",
        "        simplex_pos = self['simplex_pos'](x)\n",
        "        simplex_neg = self['simplex_neg'](x)\n",
        "\n",
        "        # Pair fusion\n",
        "        cantor_fused = self['cantor_fusion'](cantor_pos, cantor_neg)\n",
        "        simplex_fused = self['simplex_fusion'](simplex_pos, simplex_neg)\n",
        "\n",
        "        # Concatenate pairs\n",
        "        combined = torch.cat([cantor_fused, simplex_fused], dim=-1)\n",
        "\n",
        "        # Classify\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(self, x: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward pass returning logits and individual tower opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        # Tower opinions\n",
        "        opinions = {\n",
        "            'cantor_pos': self['cantor_pos'](x),\n",
        "            'cantor_neg': self['cantor_neg'](x),\n",
        "            'simplex_pos': self['simplex_pos'](x),\n",
        "            'simplex_neg': self['simplex_neg'](x),\n",
        "        }\n",
        "\n",
        "        # Pair fusion\n",
        "        cantor_fused = self['cantor_fusion'](opinions['cantor_pos'], opinions['cantor_neg'])\n",
        "        simplex_fused = self['simplex_fusion'](opinions['simplex_pos'], opinions['simplex_neg'])\n",
        "\n",
        "        # Add fused opinions\n",
        "        opinions['cantor_fused'] = cantor_fused\n",
        "        opinions['simplex_fused'] = simplex_fused\n",
        "\n",
        "        # Concatenate and classify\n",
        "        combined = torch.cat([cantor_fused, simplex_fused], dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "    def get_fingerprint_info(self) -> Dict[str, Tensor]:\n",
        "        \"\"\"Get fingerprints from all towers.\"\"\"\n",
        "        return {name: self[name].fingerprint for name in self.tower_names}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS-SPECIFIC ANALYSIS\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance for each tower.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = DualPairClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset all tracking.\"\"\"\n",
        "        # Per-class correct/total for each tower's opinion\n",
        "        self.tower_class_correct = {\n",
        "            name: defaultdict(int) for name in self.tower_names\n",
        "        }\n",
        "        self.tower_class_total = {\n",
        "            name: defaultdict(int) for name in self.tower_names\n",
        "        }\n",
        "\n",
        "        # Per-class opinion norms\n",
        "        self.tower_class_norms = {\n",
        "            name: defaultdict(list) for name in self.tower_names\n",
        "        }\n",
        "\n",
        "        # Overall per-class stats\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(\n",
        "        self,\n",
        "        opinions: Dict[str, Tensor],\n",
        "        logits: Tensor,\n",
        "        labels: Tensor,\n",
        "        classifier: nn.Module,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Update stats with a batch.\n",
        "\n",
        "        Args:\n",
        "            opinions: Dict of tower opinions [B, D]\n",
        "            logits: Model output logits [B, C]\n",
        "            labels: Ground truth [B]\n",
        "            classifier: The classifier head to probe tower opinions\n",
        "        \"\"\"\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "\n",
        "            # Overall accuracy\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            # Per-tower analysis\n",
        "            for name in self.tower_names:\n",
        "                opinion = opinions[name][i:i+1]  # [1, D]\n",
        "\n",
        "                # Track opinion norm per class\n",
        "                norm = opinion.norm().item()\n",
        "                self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "                self.tower_class_total[name][label_idx] += 1\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        \"\"\"Get per-class accuracy.\"\"\"\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(10)\n",
        "        }\n",
        "\n",
        "    def get_tower_class_norms(self) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"Get average opinion norm per tower per class.\"\"\"\n",
        "        result = {}\n",
        "        for name in self.tower_names:\n",
        "            result[name] = {}\n",
        "            for c in range(10):\n",
        "                norms = self.tower_class_norms[name][c]\n",
        "                if norms:\n",
        "                    result[name][self.CIFAR_CLASSES[c]] = sum(norms) / len(norms)\n",
        "                else:\n",
        "                    result[name][self.CIFAR_CLASSES[c]] = 0.0\n",
        "        return result\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print analysis report.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 70)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Per-class accuracy\n",
        "        print(\"\\nPer-Class Accuracy:\")\n",
        "        print(\"-\" * 40)\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        for cls, acc in sorted(class_acc.items(), key=lambda x: -x[1]):\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:12s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        # Tower opinion norms per class\n",
        "        print(\"\\nTower Opinion Norms by Class:\")\n",
        "        print(\"-\" * 70)\n",
        "        tower_norms = self.get_tower_class_norms()\n",
        "\n",
        "        # Header\n",
        "        header = f\"{'Class':12s}\"\n",
        "        for name in self.tower_names:\n",
        "            short_name = name.replace('_', '-')[:10]\n",
        "            header += f\" {short_name:>10s}\"\n",
        "        print(header)\n",
        "        print(\"-\" * 70)\n",
        "\n",
        "        # Rows\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            row = f\"{cls:12s}\"\n",
        "            for name in self.tower_names:\n",
        "                norm = tower_norms[name][cls]\n",
        "                row += f\" {norm:10.2f}\"\n",
        "            print(row)\n",
        "\n",
        "        # Tower averages\n",
        "        print(\"-\" * 70)\n",
        "        row = f\"{'Average':12s}\"\n",
        "        for name in self.tower_names:\n",
        "            avg = sum(tower_norms[name].values()) / 10\n",
        "            row += f\" {avg:10.2f}\"\n",
        "        print(row)\n",
        "\n",
        "        # Find which tower is strongest for each class\n",
        "        print(\"\\nStrongest Tower per Class:\")\n",
        "        print(\"-\" * 40)\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            best_tower = max(self.tower_names, key=lambda n: tower_norms[n][cls])\n",
        "            best_norm = tower_norms[best_tower][cls]\n",
        "            print(f\"  {cls:12s}: {best_tower:15s} ({best_norm:.2f})\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/total:.3f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    \"\"\"Evaluate and collect per-class stats.\"\"\"\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Update analyzer (only tower opinions, not fused)\n",
        "        tower_opinions = {k: v for k, v in opinions.items() if k in model.tower_names}\n",
        "        analyzer.update(tower_opinions, logits, labels, model['classifier'])\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-10 Dual-Pair Tower Classifier\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 25\n",
        "    LR = 3e-4\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 2\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 4\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Model\n",
        "    model = DualPairClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=10,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nArchitecture:\")\n",
        "    print(f\"  Towers: {model.tower_names}\")\n",
        "    print(f\"  Fusion: cantor_pair → fuse, simplex_pair → fuse, concat → classify\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "    for name in model.tower_names:\n",
        "        tower_params = sum(p.numel() for p in model[name].parameters())\n",
        "        print(f\"  {name}: {tower_params:,}\")\n",
        "\n",
        "    # Fingerprint info\n",
        "    print(\"\\nFingerprint similarities:\")\n",
        "    fps = model.get_fingerprint_info()\n",
        "    fp_list = list(fps.values())\n",
        "    names = list(fps.keys())\n",
        "    for i, n1 in enumerate(names):\n",
        "        for j, n2 in enumerate(names):\n",
        "            if i < j:\n",
        "                sim = F.cosine_similarity(fp_list[i].unsqueeze(0), fp_list[j].unsqueeze(0)).item()\n",
        "                print(f\"  {n1} <-> {n2}: {sim:.3f}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            marker = \" *\"\n",
        "        else:\n",
        "            marker = \"\"\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIlVW8iooxU5",
        "outputId": "f2f58ae7-fa42-449c-d426-e0639d5d0bb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-10 Dual-Pair Tower Classifier\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Train: 50000, Test: 10000\n",
            "\n",
            "Architecture:\n",
            "  Towers: ['cantor_pos', 'cantor_neg', 'simplex_pos', 'simplex_neg']\n",
            "  Fusion: cantor_pair → fuse, simplex_pair → fuse, concat → classify\n",
            "\n",
            "Total parameters: 6,510,884\n",
            "  cantor_pos: 1,578,499\n",
            "  cantor_neg: 1,578,499\n",
            "  simplex_pos: 1,578,505\n",
            "  simplex_neg: 1,578,505\n",
            "\n",
            "Fingerprint similarities:\n",
            "  cantor_pos <-> cantor_neg: 0.600\n",
            "  cantor_pos <-> simplex_pos: 1.000\n",
            "  cantor_pos <-> simplex_neg: 0.600\n",
            "  cantor_neg <-> simplex_pos: 0.600\n",
            "  cantor_neg <-> simplex_neg: 1.000\n",
            "  simplex_pos <-> simplex_neg: 0.600\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Training\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/25 | Loss: 1.7188 | Train: 36.0% | Test: 43.4% * | Time: 21.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/25 | Loss: 1.3958 | Train: 49.3% | Test: 53.2% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/25 | Loss: 1.2564 | Train: 54.5% | Test: 55.3% * | Time: 21.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/25 | Loss: 1.1643 | Train: 58.1% | Test: 60.4% * | Time: 21.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/25 | Loss: 1.0978 | Train: 60.4% | Test: 61.2% * | Time: 21.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/25 | Loss: 1.0434 | Train: 62.6% | Test: 63.6% * | Time: 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/25 | Loss: 0.9904 | Train: 64.5% | Test: 65.9% * | Time: 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/25 | Loss: 0.9461 | Train: 66.2% | Test: 67.7% * | Time: 21.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/25 | Loss: 0.9049 | Train: 67.8% | Test: 68.8% * | Time: 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25 | Loss: 0.8710 | Train: 68.9% | Test: 70.4% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25 | Loss: 0.8319 | Train: 70.4% | Test: 71.3% * | Time: 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/25 | Loss: 0.8051 | Train: 71.3% | Test: 71.5% * | Time: 21.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25 | Loss: 0.7700 | Train: 72.3% | Test: 72.4% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25 | Loss: 0.7417 | Train: 73.6% | Test: 72.5% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25 | Loss: 0.7166 | Train: 74.6% | Test: 74.2% * | Time: 21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25 | Loss: 0.6893 | Train: 75.7% | Test: 75.2% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25 | Loss: 0.6601 | Train: 76.5% | Test: 75.8% * | Time: 21.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25 | Loss: 0.6393 | Train: 77.3% | Test: 76.4% * | Time: 21.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25 | Loss: 0.6154 | Train: 78.1% | Test: 75.8% | Time: 21.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/25 | Loss: 0.6001 | Train: 78.7% | Test: 76.8% * | Time: 21.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/25 | Loss: 0.5857 | Train: 79.0% | Test: 77.2% * | Time: 21.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/25 | Loss: 0.5677 | Train: 79.8% | Test: 77.3% * | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/25 | Loss: 0.5644 | Train: 79.9% | Test: 77.2% | Time: 21.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/25 | Loss: 0.5576 | Train: 80.2% | Test: 77.4% * | Time: 21.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/25 | Loss: 0.5497 | Train: 80.5% | Test: 77.6% * | Time: 21.3s\n",
            "\n",
            "======================================================================\n",
            "Best Test Accuracy: 77.60%\n",
            "======================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "Per-Class Accuracy:\n",
            "----------------------------------------\n",
            "  automobile  :  88.0% █████████████████\n",
            "  ship        :  85.4% █████████████████\n",
            "  truck       :  84.7% ████████████████\n",
            "  frog        :  84.6% ████████████████\n",
            "  horse       :  82.0% ████████████████\n",
            "  airplane    :  79.8% ███████████████\n",
            "  dog         :  75.8% ███████████████\n",
            "  bird        :  69.9% █████████████\n",
            "  deer        :  69.9% █████████████\n",
            "  cat         :  55.9% ███████████\n",
            "\n",
            "Tower Opinion Norms by Class:\n",
            "----------------------------------------------------------------------\n",
            "Class        cantor-pos cantor-neg simplex-po simplex-ne\n",
            "----------------------------------------------------------------------\n",
            "airplane           9.29      10.18       8.99       9.37\n",
            "automobile         7.39       8.42       8.71       8.13\n",
            "bird               9.75      10.20       9.19       9.37\n",
            "cat                8.46       9.24       8.88       8.42\n",
            "deer               9.99      10.19       9.31       9.22\n",
            "dog                8.42       9.11       8.85       8.67\n",
            "frog               9.67      10.04       9.31       9.14\n",
            "horse              8.17       8.52       8.37       8.55\n",
            "ship               8.77       9.68       8.64       9.26\n",
            "truck              7.17       8.39       7.98       8.16\n",
            "----------------------------------------------------------------------\n",
            "Average            8.71       9.40       8.82       8.83\n",
            "\n",
            "Strongest Tower per Class:\n",
            "----------------------------------------\n",
            "  airplane    : cantor_neg      (10.18)\n",
            "  automobile  : simplex_pos     (8.71)\n",
            "  bird        : cantor_neg      (10.20)\n",
            "  cat         : cantor_neg      (9.24)\n",
            "  deer        : cantor_neg      (10.19)\n",
            "  dog         : cantor_neg      (9.11)\n",
            "  frog        : cantor_neg      (10.04)\n",
            "  horse       : simplex_neg     (8.55)\n",
            "  ship        : cantor_neg      (9.68)\n",
            "  truck       : cantor_neg      (8.39)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qPGHDxOMh8KS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cantor pair + helix pair + beatrix +  simplex"
      ],
      "metadata": {
        "id": "qNGyPmKhsLDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-10 Shape Tower Classifier\n",
        "================================\n",
        "\n",
        "8 towers with 4 geometries × pos/neg fingerprints:\n",
        "    - Cantor  + pos/neg Cantor FP (fractal wormholes)\n",
        "    - Simplex + pos/neg Cantor FP (barycentric 4D)\n",
        "    - Helix   + pos/neg Cantor FP (3-scale spiral)\n",
        "    - Beatrix + pos/neg Cantor FP (devil's staircase)\n",
        "\n",
        "Fusion hierarchy:\n",
        "    cantor_pair  → fuse ─┐\n",
        "    simplex_pair → fuse ─┼─► concat [4×dim] → classify\n",
        "    helix_pair   → fuse ─┤\n",
        "    beatrix_pair → fuse ─┘\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "import time\n",
        "\n",
        "# Geofractal imports\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.base_tower import BaseTower\n",
        "from geofractal.router.components.transformer_component import (\n",
        "    TransformerConfig,\n",
        "    TransformerVariant,\n",
        "    ActivationType,\n",
        "    PreNormBlock,\n",
        ")\n",
        "from geofractal.router.components.rope_component import (\n",
        "    QuadRoPE,\n",
        "    TriRoPE,\n",
        "    CantorRoPE,\n",
        "    BeatrixRoPE,\n",
        ")\n",
        "from geofractal.router.components.address_component import CantorAddressComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# =============================================================================\n",
        "# INVERTED CANTOR ADDRESS\n",
        "# =============================================================================\n",
        "\n",
        "class InvertedCantorAddressComponent(CantorAddressComponent):\n",
        "    \"\"\"Cantor address with inverted fingerprint (1 - cantor_value).\"\"\"\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        base_fp = super().fingerprint\n",
        "        return 1.0 - base_fp\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SHAPE TOWER\n",
        "# =============================================================================\n",
        "\n",
        "class ShapeTower(BaseTower):\n",
        "    \"\"\"\n",
        "    Tower with configurable geometry RoPE and pos/neg fingerprint.\n",
        "\n",
        "    Args:\n",
        "        rope_type: 'cantor', 'simplex', 'helix', 'beatrix'\n",
        "        inverted: If True, use inverted Cantor fingerprint\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int = 256,\n",
        "        depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        rope_type: str = 'cantor',\n",
        "        inverted: bool = False,\n",
        "        head_dim: int = 64,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.rope_type = rope_type\n",
        "        self.inverted = inverted\n",
        "\n",
        "        config = TransformerConfig(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            variant=TransformerVariant.PRENORM,\n",
        "            activation=ActivationType.GELU,\n",
        "            dropout=0.1,\n",
        "            depth=depth,\n",
        "        )\n",
        "\n",
        "        # Geometry-specific RoPE\n",
        "        if rope_type == 'cantor':\n",
        "            self.attach('rope', CantorRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim, levels=5, mode='hybrid'\n",
        "            ))\n",
        "        elif rope_type == 'simplex':\n",
        "            self.attach('rope', QuadRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim,\n",
        "                theta_w=10000.0, theta_x=7500.0, theta_y=5000.0, theta_z=2500.0,\n",
        "                augmentation='simplex'\n",
        "            ))\n",
        "        elif rope_type == 'helix':\n",
        "            self.attach('rope', TriRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim,\n",
        "                theta_alpha=10000.0, theta_beta=5000.0, theta_gamma=2500.0,\n",
        "                augmentation='barycentric'\n",
        "            ))\n",
        "        elif rope_type == 'beatrix':\n",
        "            self.attach('rope', BeatrixRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim, levels=5\n",
        "            ))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown rope_type: {rope_type}\")\n",
        "\n",
        "        # Address - Cantor or Inverted Cantor\n",
        "        if inverted:\n",
        "            self.attach('address', InvertedCantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "        else:\n",
        "            self.attach('address', CantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "\n",
        "        # Transformer blocks\n",
        "        for i in range(depth):\n",
        "            self.append(PreNormBlock(f'{name}_block_{i}', config=config, block_idx=i))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        return F.normalize(self['address'].fingerprint, dim=-1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns pooled opinion [B, D].\"\"\"\n",
        "        for block in self.stages:\n",
        "            x, _ = block(x)\n",
        "        x = self['norm'](x)\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SHAPE CLASSIFIER\n",
        "# =============================================================================\n",
        "\n",
        "class ShapeClassifier(BaseRouter):\n",
        "    \"\"\"\n",
        "    8-tower classifier with shape-based geometry pairs.\n",
        "\n",
        "    Towers (geometry × polarity):\n",
        "        cantor_pos, cantor_neg\n",
        "        simplex_pos, simplex_neg\n",
        "        helix_pos, helix_neg\n",
        "        beatrix_pos, beatrix_neg\n",
        "\n",
        "    Fusion:\n",
        "        Each pair → AdaptiveFusion\n",
        "        All 4 fused pairs → concat → classify\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "    GEOMETRIES = ['cantor', 'simplex', 'helix', 'beatrix']\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 10,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__('shape_classifier', strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.attach('patch_embed', nn.Conv2d(3, dim, patch_size, patch_size))\n",
        "\n",
        "        class PosEmbed(nn.Module):\n",
        "            def __init__(self, num_patches, dim):\n",
        "                super().__init__()\n",
        "                self.pos = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "            def forward(self, x):\n",
        "                return x + self.pos\n",
        "\n",
        "        self.attach('pos_embed', PosEmbed(num_patches, dim))\n",
        "\n",
        "        # Build tower pairs for each geometry\n",
        "        tower_names = []\n",
        "        for geom in self.GEOMETRIES:\n",
        "            # Positive fingerprint tower\n",
        "            self.attach(f'{geom}_pos', ShapeTower(\n",
        "                f'{geom}_pos', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "                rope_type=geom, inverted=False, fingerprint_dim=fingerprint_dim\n",
        "            ))\n",
        "            tower_names.append(f'{geom}_pos')\n",
        "\n",
        "            # Negative fingerprint tower\n",
        "            self.attach(f'{geom}_neg', ShapeTower(\n",
        "                f'{geom}_neg', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "                rope_type=geom, inverted=True, fingerprint_dim=fingerprint_dim\n",
        "            ))\n",
        "            tower_names.append(f'{geom}_neg')\n",
        "\n",
        "            # Pair fusion\n",
        "            self.attach(f'{geom}_fusion', AdaptiveFusion(\n",
        "                f'{geom}_fusion', num_inputs=2, in_features=dim\n",
        "            ))\n",
        "\n",
        "        # Final classifier on concatenated pairs (4 geometries × dim)\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim * len(self.GEOMETRIES)),\n",
        "            nn.Linear(dim * len(self.GEOMETRIES), dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        self.objects['tower_names'] = tower_names\n",
        "        self.objects['num_patches'] = num_patches\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self.objects['tower_names']\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        # Get fused opinion from each geometry pair\n",
        "        fused_pairs = []\n",
        "        for geom in self.GEOMETRIES:\n",
        "            pos_opinion = self[f'{geom}_pos'](x)\n",
        "            neg_opinion = self[f'{geom}_neg'](x)\n",
        "            fused = self[f'{geom}_fusion'](pos_opinion, neg_opinion)\n",
        "            fused_pairs.append(fused)\n",
        "\n",
        "        # Concatenate all pairs\n",
        "        combined = torch.cat(fused_pairs, dim=-1)  # [B, 4×dim]\n",
        "\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def forward_with_opinions(self, x: Tensor) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        opinions = {}\n",
        "        fused_pairs = []\n",
        "\n",
        "        for geom in self.GEOMETRIES:\n",
        "            pos_opinion = self[f'{geom}_pos'](x)\n",
        "            neg_opinion = self[f'{geom}_neg'](x)\n",
        "\n",
        "            opinions[f'{geom}_pos'] = pos_opinion\n",
        "            opinions[f'{geom}_neg'] = neg_opinion\n",
        "\n",
        "            fused = self[f'{geom}_fusion'](pos_opinion, neg_opinion)\n",
        "            opinions[f'{geom}_fused'] = fused\n",
        "            fused_pairs.append(fused)\n",
        "\n",
        "        combined = torch.cat(fused_pairs, dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "    def get_fingerprint_info(self) -> Dict[str, Tensor]:\n",
        "        return {name: self[name].fingerprint for name in self.tower_names}\n",
        "\n",
        "    def get_fusion_weights(self, x: Tensor) -> Dict[str, Tensor]:\n",
        "        \"\"\"Get fusion weights for each geometry pair.\"\"\"\n",
        "        x = self['patch_embed'](x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        weights = {}\n",
        "        for geom in self.GEOMETRIES:\n",
        "            pos_op = self[f'{geom}_pos'](x)\n",
        "            neg_op = self[f'{geom}_neg'](x)\n",
        "\n",
        "            fusion = self[f'{geom}_fusion']\n",
        "            stacked = torch.stack([pos_op, neg_op], dim=0)\n",
        "            w = fusion.weight_net(stacked)\n",
        "            w = F.softmax(w / fusion.temperature, dim=0).squeeze(-1)\n",
        "            weights[geom] = {'pos': w[0].mean().item(), 'neg': w[1].mean().item()}\n",
        "\n",
        "        return weights\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance for each tower.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = ShapeClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str], geometries: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.geometries = geometries\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {\n",
        "            name: defaultdict(list) for name in self.tower_names\n",
        "        }\n",
        "        self.fused_class_norms = {\n",
        "            geom: defaultdict(list) for geom in self.geometries\n",
        "        }\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            # Tower norms\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "            # Fused norms\n",
        "            for geom in self.geometries:\n",
        "                key = f'{geom}_fused'\n",
        "                if key in opinions:\n",
        "                    norm = opinions[key][i].norm().item()\n",
        "                    self.fused_class_norms[geom][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(10)\n",
        "        }\n",
        "\n",
        "    def get_tower_class_norms(self) -> Dict[str, Dict[str, float]]:\n",
        "        result = {}\n",
        "        for name in self.tower_names:\n",
        "            result[name] = {}\n",
        "            for c in range(10):\n",
        "                norms = self.tower_class_norms[name][c]\n",
        "                result[name][self.CIFAR_CLASSES[c]] = sum(norms) / len(norms) if norms else 0.0\n",
        "        return result\n",
        "\n",
        "    def get_fused_class_norms(self) -> Dict[str, Dict[str, float]]:\n",
        "        result = {}\n",
        "        for geom in self.geometries:\n",
        "            result[geom] = {}\n",
        "            for c in range(10):\n",
        "                norms = self.fused_class_norms[geom][c]\n",
        "                result[geom][self.CIFAR_CLASSES[c]] = sum(norms) / len(norms) if norms else 0.0\n",
        "        return result\n",
        "\n",
        "    def print_report(self):\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        # Per-class accuracy\n",
        "        print(\"\\nPer-Class Accuracy:\")\n",
        "        print(\"-\" * 40)\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        for cls, acc in sorted(class_acc.items(), key=lambda x: -x[1]):\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:12s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        # Tower norms (abbreviated - show pos vs neg comparison)\n",
        "        print(\"\\nTower Opinion Norms (pos vs neg by geometry):\")\n",
        "        print(\"-\" * 80)\n",
        "        tower_norms = self.get_tower_class_norms()\n",
        "\n",
        "        # Header\n",
        "        print(f\"{'Class':12s}\", end=\"\")\n",
        "        for geom in self.geometries:\n",
        "            print(f\" {geom[:6]+'_p':>8s} {geom[:6]+'_n':>8s}\", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            print(f\"{cls:12s}\", end=\"\")\n",
        "            for geom in self.geometries:\n",
        "                pos_norm = tower_norms[f'{geom}_pos'][cls]\n",
        "                neg_norm = tower_norms[f'{geom}_neg'][cls]\n",
        "                print(f\" {pos_norm:8.2f} {neg_norm:8.2f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        # Fused norms per geometry\n",
        "        print(\"\\nFused Geometry Norms by Class:\")\n",
        "        print(\"-\" * 60)\n",
        "        fused_norms = self.get_fused_class_norms()\n",
        "\n",
        "        print(f\"{'Class':12s}\", end=\"\")\n",
        "        for geom in self.geometries:\n",
        "            print(f\" {geom:>10s}\", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            print(f\"{cls:12s}\", end=\"\")\n",
        "            for geom in self.geometries:\n",
        "                norm = fused_norms[geom][cls]\n",
        "                print(f\" {norm:10.2f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        # Strongest geometry per class\n",
        "        print(\"\\nStrongest Geometry per Class:\")\n",
        "        print(\"-\" * 40)\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            best_geom = max(self.geometries, key=lambda g: fused_norms[g][cls])\n",
        "            best_norm = fused_norms[best_geom][cls]\n",
        "            print(f\"  {cls:12s}: {best_geom:10s} ({best_norm:.2f})\")\n",
        "\n",
        "        # Pos vs Neg dominance per geometry\n",
        "        print(\"\\nPos vs Neg Dominance (avg across classes):\")\n",
        "        print(\"-\" * 40)\n",
        "        for geom in self.geometries:\n",
        "            pos_avg = sum(tower_norms[f'{geom}_pos'].values()) / 10\n",
        "            neg_avg = sum(tower_norms[f'{geom}_neg'].values()) / 10\n",
        "            winner = \"POS\" if pos_avg > neg_avg else \"NEG\"\n",
        "            diff = abs(pos_avg - neg_avg)\n",
        "            print(f\"  {geom:10s}: pos={pos_avg:.2f}, neg={neg_avg:.2f} → {winner} (+{diff:.2f})\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, labels in pbar:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/total:.3f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        logits, opinions = model.forward_with_opinions(images)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-10 Shape Tower Classifier\")\n",
        "    print(\"8 Towers: 4 Geometries × Pos/Neg Fingerprints\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 25\n",
        "    LR = 3e-4\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 2\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 4\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Data\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset)}, Test: {len(test_dataset)}\")\n",
        "\n",
        "    # Model\n",
        "    model = ShapeClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=10,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nGeometries: {model.GEOMETRIES}\")\n",
        "    print(f\"Towers: {model.tower_names}\")\n",
        "    print(f\"Fusion: each pair → fuse → concat [4×{DIM}] → classify\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"\\nTotal parameters: {total_params:,}\")\n",
        "\n",
        "    for geom in model.GEOMETRIES:\n",
        "        pos_params = sum(p.numel() for p in model[f'{geom}_pos'].parameters())\n",
        "        neg_params = sum(p.numel() for p in model[f'{geom}_neg'].parameters())\n",
        "        print(f\"  {geom}: pos={pos_params:,}, neg={neg_params:,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names, model.GEOMETRIES)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "    # Fusion weights analysis\n",
        "    print(\"\\nFusion Weights (sample batch):\")\n",
        "    print(\"-\" * 40)\n",
        "    model.eval()\n",
        "    sample_images, _ = next(iter(test_loader))\n",
        "    sample_images = sample_images[:32].to(device)\n",
        "    weights = model.get_fusion_weights(sample_images)\n",
        "    for geom, w in weights.items():\n",
        "        print(f\"  {geom:10s}: pos={w['pos']:.3f}, neg={w['neg']:.3f}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtSFswr3sNF_",
        "outputId": "db15129d-0271-4c08-ff06-04593a8ba5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-10 Shape Tower Classifier\n",
            "8 Towers: 4 Geometries × Pos/Neg Fingerprints\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Train: 50000, Test: 10000\n",
            "\n",
            "Geometries: ['cantor', 'simplex', 'helix', 'beatrix']\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'simplex_pos', 'simplex_neg', 'helix_pos', 'helix_neg', 'beatrix_pos', 'beatrix_neg']\n",
            "Fusion: each pair → fuse → concat [4×256] → classify\n",
            "\n",
            "Total parameters: 12,990,136\n",
            "  cantor: pos=1,578,499, neg=1,578,499\n",
            "  simplex: pos=1,578,505, neg=1,578,505\n",
            "  helix: pos=1,578,566, neg=1,578,566\n",
            "  beatrix: pos=1,578,499, neg=1,578,499\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/25 | Loss: 1.7213 | Train: 35.5% | Test: 46.2% * | Time: 41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/25 | Loss: 1.4042 | Train: 49.1% | Test: 50.5% * | Time: 41.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/25 | Loss: 1.2611 | Train: 54.3% | Test: 55.2% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/25 | Loss: 1.1892 | Train: 56.9% | Test: 58.6% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/25 | Loss: 1.1215 | Train: 59.7% | Test: 60.9% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/25 | Loss: 1.0833 | Train: 61.2% | Test: 62.7% * | Time: 41.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/25 | Loss: 1.0270 | Train: 63.3% | Test: 64.2% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/25 | Loss: 0.9893 | Train: 64.3% | Test: 65.2% * | Time: 41.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/25 | Loss: 0.9524 | Train: 65.8% | Test: 66.6% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25 | Loss: 0.9136 | Train: 67.6% | Test: 65.9% | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25 | Loss: 0.8857 | Train: 68.5% | Test: 69.9% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/25 | Loss: 0.8472 | Train: 69.6% | Test: 70.5% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25 | Loss: 0.8169 | Train: 70.9% | Test: 71.2% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25 | Loss: 0.7848 | Train: 72.1% | Test: 71.5% * | Time: 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25 | Loss: 0.7601 | Train: 73.0% | Test: 72.6% * | Time: 41.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25 | Loss: 0.7308 | Train: 74.0% | Test: 73.7% * | Time: 41.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25 | Loss: 0.7016 | Train: 74.9% | Test: 74.5% * | Time: 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25 | Loss: 0.6806 | Train: 75.9% | Test: 74.5% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25 | Loss: 0.6582 | Train: 76.6% | Test: 75.4% * | Time: 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/25 | Loss: 0.6356 | Train: 77.3% | Test: 75.8% * | Time: 41.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/25 | Loss: 0.6202 | Train: 78.1% | Test: 76.3% * | Time: 41.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/25 | Loss: 0.6031 | Train: 78.8% | Test: 76.3% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/25 | Loss: 0.5946 | Train: 79.2% | Test: 76.7% * | Time: 41.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/25 | Loss: 0.5926 | Train: 78.9% | Test: 76.7% | Time: 41.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/25 | Loss: 0.5835 | Train: 79.4% | Test: 76.8% * | Time: 41.0s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 76.77%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy:\n",
            "----------------------------------------\n",
            "  frog        :  85.5% █████████████████\n",
            "  ship        :  85.5% █████████████████\n",
            "  automobile  :  85.3% █████████████████\n",
            "  truck       :  84.7% ████████████████\n",
            "  horse       :  80.1% ████████████████\n",
            "  airplane    :  79.3% ███████████████\n",
            "  dog         :  75.4% ███████████████\n",
            "  deer        :  69.2% █████████████\n",
            "  bird        :  67.5% █████████████\n",
            "  cat         :  55.2% ███████████\n",
            "\n",
            "Tower Opinion Norms (pos vs neg by geometry):\n",
            "--------------------------------------------------------------------------------\n",
            "Class        cantor_p cantor_n simple_p simple_n  helix_p  helix_n beatri_p beatri_n\n",
            "--------------------------------------------------------------------------------\n",
            "airplane        11.27     9.84    10.81     9.69    11.84    10.50     9.10    11.72\n",
            "automobile      10.33     8.65     9.51     7.86    10.20     9.71     7.87    10.16\n",
            "bird            11.61     9.77    11.29     9.70    11.61    10.52     9.42    11.45\n",
            "cat             11.00     9.28    10.22     8.76    10.90     9.88     8.75    10.43\n",
            "deer            11.91     9.93    11.61     9.54    11.65    10.36     9.32    11.53\n",
            "dog             10.87     9.34    10.22     8.78    10.89    10.06     8.80    10.44\n",
            "frog            11.93    10.06    11.40     9.21    11.40    10.20     9.48    11.02\n",
            "horse           10.43     8.92     9.94     7.95    10.65     9.85     8.11    10.42\n",
            "ship            10.68     9.46    10.37     9.13    11.50    10.18     8.80    11.35\n",
            "truck           10.01     8.31     9.20     7.44    10.25     9.36     7.84    10.26\n",
            "\n",
            "Fused Geometry Norms by Class:\n",
            "------------------------------------------------------------\n",
            "Class            cantor    simplex      helix    beatrix\n",
            "------------------------------------------------------------\n",
            "airplane           7.82       7.36       8.36       7.35\n",
            "automobile         6.67       6.13       7.85       6.35\n",
            "bird               7.63       7.48       8.31       7.49\n",
            "cat                7.09       6.75       7.79       6.92\n",
            "deer               7.80       7.59       8.17       7.58\n",
            "dog                7.08       6.76       7.92       6.98\n",
            "frog               7.99       7.33       8.00       7.66\n",
            "horse              6.71       6.39       7.88       6.82\n",
            "ship               7.37       7.16       8.25       7.11\n",
            "truck              6.39       5.94       7.72       6.49\n",
            "\n",
            "Strongest Geometry per Class:\n",
            "----------------------------------------\n",
            "  airplane    : helix      (8.36)\n",
            "  automobile  : helix      (7.85)\n",
            "  bird        : helix      (8.31)\n",
            "  cat         : helix      (7.79)\n",
            "  deer        : helix      (8.17)\n",
            "  dog         : helix      (7.92)\n",
            "  frog        : helix      (8.00)\n",
            "  horse       : helix      (7.88)\n",
            "  ship        : helix      (8.25)\n",
            "  truck       : helix      (7.72)\n",
            "\n",
            "Pos vs Neg Dominance (avg across classes):\n",
            "----------------------------------------\n",
            "  cantor    : pos=11.00, neg=9.36 → POS (+1.65)\n",
            "  simplex   : pos=10.46, neg=8.81 → POS (+1.65)\n",
            "  helix     : pos=11.09, neg=10.06 → POS (+1.03)\n",
            "  beatrix   : pos=8.75, neg=10.88 → NEG (+2.13)\n",
            "\n",
            "Fusion Weights (sample batch):\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  cantor    : pos=0.327, neg=0.673\n",
            "  simplex   : pos=0.506, neg=0.494\n",
            "  helix     : pos=0.278, neg=0.722\n",
            "  beatrix   : pos=0.564, neg=0.436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 tower + dino v3"
      ],
      "metadata": {
        "id": "LHYpUlaCsNqd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-10 Shape Tower Classifier + DINOv3 (CACHED)\n",
        "==================================================\n",
        "\n",
        "Precaches DINOv3 outputs to disk, then trains with cached latents.\n",
        "Massive speedup since DINO only runs once.\n",
        "\n",
        "Usage:\n",
        "    1. First run: caches DINO outputs (~2-3 min)\n",
        "    2. Subsequent epochs: loads from cache (fast)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Geofractal imports\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.base_tower import BaseTower\n",
        "from geofractal.router.components.transformer_component import (\n",
        "    TransformerConfig,\n",
        "    TransformerVariant,\n",
        "    ActivationType,\n",
        "    PreNormBlock,\n",
        ")\n",
        "from geofractal.router.components.rope_component import (\n",
        "    QuadRoPE,\n",
        "    TriRoPE,\n",
        "    CantorRoPE,\n",
        "    BeatrixRoPE,\n",
        ")\n",
        "from geofractal.router.components.address_component import CantorAddressComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 outputs for entire dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = 768\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        \"\"\"Load cached latents.\"\"\"\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINO latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        \"\"\"Build cache for dataset.\"\"\"\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINO cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        # Load DINOv3\n",
        "        model_name = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "        processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "        backbone = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        # Simple loader without augmentation for caching\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "            images = images.to(self.device)\n",
        "\n",
        "            # Resize for DINO\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            latents = outputs.pooler_output.cpu()\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "\n",
        "        # Save cache\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        # Cleanup\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINO latents.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base_dataset: Dataset,\n",
        "        dino_latents: Tensor,\n",
        "        transform: Optional[transforms.Compose] = None,\n",
        "    ):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.dino_latents = dino_latents\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, int]:\n",
        "        image, label = self.base_dataset[idx]\n",
        "\n",
        "        # Apply augmentation transform if provided\n",
        "        if self.transform is not None:\n",
        "            # base_dataset already has ToTensor+Normalize, so we need raw image\n",
        "            # Actually we need to be careful here - let's apply transform to the image\n",
        "            pass  # Transform already applied in base_dataset\n",
        "\n",
        "        dino_latent = self.dino_latents[idx]\n",
        "\n",
        "        return image, dino_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# INVERTED CANTOR ADDRESS\n",
        "# =============================================================================\n",
        "\n",
        "class InvertedCantorAddressComponent(CantorAddressComponent):\n",
        "    \"\"\"Cantor address with inverted fingerprint (1 - cantor_value).\"\"\"\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        base_fp = super().fingerprint\n",
        "        return 1.0 - base_fp\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SHAPE TOWER\n",
        "# =============================================================================\n",
        "\n",
        "class ShapeTower(BaseTower):\n",
        "    \"\"\"Tower with configurable geometry RoPE and pos/neg fingerprint.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int = 256,\n",
        "        depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        rope_type: str = 'cantor',\n",
        "        inverted: bool = False,\n",
        "        head_dim: int = 64,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.rope_type = rope_type\n",
        "        self.inverted = inverted\n",
        "\n",
        "        config = TransformerConfig(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            variant=TransformerVariant.PRENORM,\n",
        "            activation=ActivationType.GELU,\n",
        "            dropout=0.1,\n",
        "            depth=depth,\n",
        "        )\n",
        "\n",
        "        # Geometry-specific RoPE\n",
        "        if rope_type == 'cantor':\n",
        "            self.attach('rope', CantorRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim, levels=5, mode='hybrid'\n",
        "            ))\n",
        "        elif rope_type == 'simplex':\n",
        "            self.attach('rope', QuadRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim,\n",
        "                theta_w=10000.0, theta_x=7500.0, theta_y=5000.0, theta_z=2500.0,\n",
        "                augmentation='simplex'\n",
        "            ))\n",
        "        elif rope_type == 'helix':\n",
        "            self.attach('rope', TriRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim,\n",
        "                theta_alpha=10000.0, theta_beta=5000.0, theta_gamma=2500.0,\n",
        "                augmentation='barycentric'\n",
        "            ))\n",
        "        elif rope_type == 'beatrix':\n",
        "            self.attach('rope', BeatrixRoPE(\n",
        "                f'{name}_rope', head_dim=head_dim, levels=5\n",
        "            ))\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown rope_type: {rope_type}\")\n",
        "\n",
        "        # Address - Cantor or Inverted Cantor\n",
        "        if inverted:\n",
        "            self.attach('address', InvertedCantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "        else:\n",
        "            self.attach('address', CantorAddressComponent(\n",
        "                f'{name}_address', k_simplex=4,\n",
        "                fingerprint_dim=fingerprint_dim, mode='staircase'\n",
        "            ))\n",
        "\n",
        "        # Transformer blocks\n",
        "        for i in range(depth):\n",
        "            self.append(PreNormBlock(f'{name}_block_{i}', config=config, block_idx=i))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    @property\n",
        "    def fingerprint(self) -> Tensor:\n",
        "        return F.normalize(self['address'].fingerprint, dim=-1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"Returns pooled opinion [B, D].\"\"\"\n",
        "        for block in self.stages:\n",
        "            x, _ = block(x)\n",
        "        x = self['norm'](x)\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION (no backbone, just projection head)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents to tower dim.\"\"\"\n",
        "\n",
        "    def __init__(self, dino_dim: int = 768, out_dim: int = 256):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(dino_dim),\n",
        "            nn.Linear(dino_dim, out_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SHAPE CLASSIFIER (CACHED VERSION)\n",
        "# =============================================================================\n",
        "\n",
        "class ShapeClassifierCached(BaseRouter):\n",
        "    \"\"\"\n",
        "    8-tower classifier with shape-based geometry pairs + cached DINO.\n",
        "\n",
        "    Takes precached DINO latents instead of raw images for DINO path.\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "    GEOMETRIES = ['cantor', 'simplex', 'helix', 'beatrix']\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 10,\n",
        "        fingerprint_dim: int = 64,\n",
        "        dino_dim: int = 768,\n",
        "    ):\n",
        "        super().__init__('shape_classifier_cached', strict=False)\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding for geometric towers\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.attach('patch_embed', nn.Conv2d(3, dim, patch_size, patch_size))\n",
        "\n",
        "        class PosEmbed(nn.Module):\n",
        "            def __init__(self, num_patches, dim):\n",
        "                super().__init__()\n",
        "                self.pos = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "            def forward(self, x):\n",
        "                return x + self.pos\n",
        "\n",
        "        self.attach('pos_embed', PosEmbed(num_patches, dim))\n",
        "\n",
        "        # Build tower pairs for each geometry\n",
        "        tower_names = []\n",
        "        for geom in self.GEOMETRIES:\n",
        "            self.attach(f'{geom}_pos', ShapeTower(\n",
        "                f'{geom}_pos', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "                rope_type=geom, inverted=False, fingerprint_dim=fingerprint_dim\n",
        "            ))\n",
        "            tower_names.append(f'{geom}_pos')\n",
        "\n",
        "            self.attach(f'{geom}_neg', ShapeTower(\n",
        "                f'{geom}_neg', dim=dim, depth=tower_depth, num_heads=num_heads,\n",
        "                rope_type=geom, inverted=True, fingerprint_dim=fingerprint_dim\n",
        "            ))\n",
        "            tower_names.append(f'{geom}_neg')\n",
        "\n",
        "            self.attach(f'{geom}_fusion', AdaptiveFusion(\n",
        "                f'{geom}_fusion', num_inputs=2, in_features=dim\n",
        "            ))\n",
        "\n",
        "        # DINO projection (no backbone!)\n",
        "        self.attach('dino_proj', DinoProjection(dino_dim=dino_dim, out_dim=dim))\n",
        "        tower_names.append('dino')\n",
        "\n",
        "        # Final classifier\n",
        "        classifier_in_dim = dim * (len(self.GEOMETRIES) + 1)\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(classifier_in_dim),\n",
        "            nn.Linear(classifier_in_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        self.objects['tower_names'] = tower_names\n",
        "        self.objects['num_patches'] = num_patches\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self.objects['tower_names']\n",
        "\n",
        "    def forward(self, images: Tensor, dino_latents: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: [B, 3, 32, 32] CIFAR images\n",
        "            dino_latents: [B, 768] precached DINO outputs\n",
        "        \"\"\"\n",
        "        # Patch embed for geometric towers\n",
        "        x = self['patch_embed'](images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        # Geometric tower pairs\n",
        "        fused_pairs = []\n",
        "        for geom in self.GEOMETRIES:\n",
        "            pos_opinion = self[f'{geom}_pos'](x)\n",
        "            neg_opinion = self[f'{geom}_neg'](x)\n",
        "            fused = self[f'{geom}_fusion'](pos_opinion, neg_opinion)\n",
        "            fused_pairs.append(fused)\n",
        "\n",
        "        # DINO projection from cached latents\n",
        "        dino_opinion = self['dino_proj'](dino_latents)\n",
        "        fused_pairs.append(dino_opinion)\n",
        "\n",
        "        combined = torch.cat(fused_pairs, dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, dino_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        x = self['patch_embed'](images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self['pos_embed'](x)\n",
        "\n",
        "        opinions = {}\n",
        "        fused_pairs = []\n",
        "\n",
        "        for geom in self.GEOMETRIES:\n",
        "            pos_opinion = self[f'{geom}_pos'](x)\n",
        "            neg_opinion = self[f'{geom}_neg'](x)\n",
        "\n",
        "            opinions[f'{geom}_pos'] = pos_opinion\n",
        "            opinions[f'{geom}_neg'] = neg_opinion\n",
        "\n",
        "            fused = self[f'{geom}_fusion'](pos_opinion, neg_opinion)\n",
        "            opinions[f'{geom}_fused'] = fused\n",
        "            fused_pairs.append(fused)\n",
        "\n",
        "        dino_opinion = self['dino_proj'](dino_latents)\n",
        "        opinions['dino'] = dino_opinion\n",
        "        fused_pairs.append(dino_opinion)\n",
        "\n",
        "        combined = torch.cat(fused_pairs, dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "    def get_fingerprint_info(self) -> Dict[str, Tensor]:\n",
        "        return {name: self[name].fingerprint for name in self.tower_names if name != 'dino'}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CLASS ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance for each tower.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = ShapeClassifierCached.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str], geometries: List[str], has_dino: bool = True):\n",
        "        self.tower_names = tower_names\n",
        "        self.geometries = geometries\n",
        "        self.has_dino = has_dino\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {\n",
        "            name: defaultdict(list) for name in self.tower_names if name != 'dino'\n",
        "        }\n",
        "        self.fused_class_norms = {\n",
        "            geom: defaultdict(list) for geom in self.geometries\n",
        "        }\n",
        "        if self.has_dino:\n",
        "            self.dino_class_norms = defaultdict(list)\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name != 'dino':\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "            if self.has_dino and 'dino' in opinions:\n",
        "                norm = opinions['dino'][i].norm().item()\n",
        "                self.dino_class_norms[label_idx].append(norm)\n",
        "\n",
        "            for geom in self.geometries:\n",
        "                key = f'{geom}_fused'\n",
        "                if key in opinions:\n",
        "                    norm = opinions[key][i].norm().item()\n",
        "                    self.fused_class_norms[geom][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(10)\n",
        "        }\n",
        "\n",
        "    def get_tower_class_norms(self) -> Dict[str, Dict[str, float]]:\n",
        "        result = {}\n",
        "        for name in self.tower_names:\n",
        "            if name == 'dino':\n",
        "                continue\n",
        "            result[name] = {}\n",
        "            for c in range(10):\n",
        "                norms = self.tower_class_norms[name][c]\n",
        "                result[name][self.CIFAR_CLASSES[c]] = sum(norms) / len(norms) if norms else 0.0\n",
        "        return result\n",
        "\n",
        "    def get_fused_class_norms(self) -> Dict[str, Dict[str, float]]:\n",
        "        result = {}\n",
        "        for geom in self.geometries:\n",
        "            result[geom] = {}\n",
        "            for c in range(10):\n",
        "                norms = self.fused_class_norms[geom][c]\n",
        "                result[geom][self.CIFAR_CLASSES[c]] = sum(norms) / len(norms) if norms else 0.0\n",
        "        return result\n",
        "\n",
        "    def print_report(self):\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy:\")\n",
        "        print(\"-\" * 40)\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        for cls, acc in sorted(class_acc.items(), key=lambda x: -x[1]):\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:12s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nTower Opinion Norms (pos vs neg by geometry):\")\n",
        "        print(\"-\" * 80)\n",
        "        tower_norms = self.get_tower_class_norms()\n",
        "\n",
        "        print(f\"{'Class':12s}\", end=\"\")\n",
        "        for geom in self.geometries:\n",
        "            print(f\" {geom[:6]+'_p':>8s} {geom[:6]+'_n':>8s}\", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            print(f\"{cls:12s}\", end=\"\")\n",
        "            for geom in self.geometries:\n",
        "                pos_norm = tower_norms.get(f'{geom}_pos', {}).get(cls, 0)\n",
        "                neg_norm = tower_norms.get(f'{geom}_neg', {}).get(cls, 0)\n",
        "                print(f\" {pos_norm:8.2f} {neg_norm:8.2f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        print(\"\\nFused Geometry Norms by Class:\")\n",
        "        print(\"-\" * 60)\n",
        "        fused_norms = self.get_fused_class_norms()\n",
        "\n",
        "        print(f\"{'Class':12s}\", end=\"\")\n",
        "        for geom in self.geometries:\n",
        "            print(f\" {geom:>10s}\", end=\"\")\n",
        "        print()\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            print(f\"{cls:12s}\", end=\"\")\n",
        "            for geom in self.geometries:\n",
        "                norm = fused_norms[geom][cls]\n",
        "                print(f\" {norm:10.2f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        print(\"\\nStrongest Geometry per Class:\")\n",
        "        print(\"-\" * 40)\n",
        "        for c in range(10):\n",
        "            cls = self.CIFAR_CLASSES[c]\n",
        "            best_geom = max(self.geometries, key=lambda g: fused_norms[g][cls])\n",
        "            best_norm = fused_norms[best_geom][cls]\n",
        "            print(f\"  {cls:12s}: {best_geom:10s} ({best_norm:.2f})\")\n",
        "\n",
        "        print(\"\\nPos vs Neg Dominance (avg across classes):\")\n",
        "        print(\"-\" * 40)\n",
        "        for geom in self.geometries:\n",
        "            pos_avg = sum(tower_norms.get(f'{geom}_pos', {}).values()) / 10\n",
        "            neg_avg = sum(tower_norms.get(f'{geom}_neg', {}).values()) / 10\n",
        "            winner = \"POS\" if pos_avg > neg_avg else \"NEG\"\n",
        "            diff = abs(pos_avg - neg_avg)\n",
        "            print(f\"  {geom:10s}: pos={pos_avg:.2f}, neg={neg_avg:.2f} → {winner} (+{diff:.2f})\")\n",
        "\n",
        "        if self.has_dino and self.dino_class_norms:\n",
        "            print(\"\\nDINO Opinion Norms by Class:\")\n",
        "            print(\"-\" * 40)\n",
        "            for c in range(10):\n",
        "                cls = self.CIFAR_CLASSES[c]\n",
        "                norms = self.dino_class_norms[c]\n",
        "                avg_norm = sum(norms) / len(norms) if norms else 0.0\n",
        "                bar = \"█\" * int(avg_norm)\n",
        "                print(f\"  {cls:12s}: {avg_norm:6.2f} {bar}\")\n",
        "\n",
        "            dino_avg = sum(sum(self.dino_class_norms[c]) / len(self.dino_class_norms[c])\n",
        "                          for c in range(10) if self.dino_class_norms[c]) / 10\n",
        "            geom_avg = sum(sum(fused_norms[g].values()) / 10 for g in self.geometries) / len(self.geometries)\n",
        "            print(f\"\\n  DINO avg: {dino_avg:.2f}\")\n",
        "            print(f\"  Geom avg: {geom_avg:.2f}\")\n",
        "            print(f\"  Ratio:    {dino_avg/geom_avg:.2f}x\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, dino_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, dino_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/total:.3f}',\n",
        "            'acc': f'{100.*correct/total:.1f}%'\n",
        "        })\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, dino_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, dino_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-10 Shape Tower Classifier + DINOv3 (CACHED)\")\n",
        "    print(\"9 Opinions: 4 Geometries × Pos/Neg + DINOv3 ConvNeXt-Small\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 25\n",
        "    LR = 3e-4\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 2\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 4\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "\n",
        "    # Base transforms (no augmentation for caching)\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Training transform with augmentation\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Load base datasets (no augmentation - for caching)\n",
        "    train_dataset_base = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINO outputs\n",
        "    cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_dino_latents = cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_dino_latents = cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Create augmented training dataset for geometric towers\n",
        "    # Note: DINO latents are from unaugmented images, but geometric towers see augmented\n",
        "    train_dataset_aug = datasets.CIFAR10(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    # Wrap with cached latents\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_dino_latents)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_dino_latents)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model (no DINO backbone!)\n",
        "    print(\"\\nBuilding model (no DINO backbone - using cached latents)...\")\n",
        "    model = ShapeClassifierCached(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=10,\n",
        "        dino_dim=768,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nGeometries: {model.GEOMETRIES}\")\n",
        "    print(f\"Towers: {model.tower_names}\")\n",
        "    print(f\"DINO: Cached latents (768-dim) → projection\")\n",
        "    print(f\"Fusion: each pair → fuse + dino → concat [5×{DIM}] → classify\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal parameters: {total_params:,} (ALL trainable - no frozen backbone)\")\n",
        "\n",
        "    for geom in model.GEOMETRIES:\n",
        "        pos_params = sum(p.numel() for p in model[f'{geom}_pos'].parameters())\n",
        "        neg_params = sum(p.numel() for p in model[f'{geom}_neg'].parameters())\n",
        "        print(f\"  {geom}: pos={pos_params:,}, neg={neg_params:,}\")\n",
        "\n",
        "    dino_proj_params = sum(p.numel() for p in model['dino_proj'].parameters())\n",
        "    print(f\"  dino_proj: {dino_proj_params:,}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names, model.GEOMETRIES, has_dino=True)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training (with cached DINO latents)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8-rmOZIyKSD",
        "outputId": "e11403a5-09ab-4cb5-9baa-c94d59f64d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-10 Shape Tower Classifier + DINOv3 (CACHED)\n",
            "9 Opinions: 4 Geometries × Pos/Neg + DINOv3 ConvNeXt-Small\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Train: 50000, Test: 10000\n",
            "Building DINO cache for train (50000 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching train: 100%|██████████| 782/782 [01:10<00:00, 11.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/dinov3_convnext_small_train.pt - shape: torch.Size([50000, 768])\n",
            "Building DINO cache for test (10000 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching test: 100%|██████████| 157/157 [00:13<00:00, 11.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/dinov3_convnext_small_test.pt - shape: torch.Size([10000, 768])\n",
            "\n",
            "Building model (no DINO backbone - using cached latents)...\n",
            "\n",
            "Geometries: ['cantor', 'simplex', 'helix', 'beatrix']\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'simplex_pos', 'simplex_neg', 'helix_pos', 'helix_neg', 'beatrix_pos', 'beatrix_neg', 'dino']\n",
            "DINO: Cached latents (768-dim) → projection\n",
            "Fusion: each pair → fuse + dino → concat [5×256] → classify\n",
            "\n",
            "Total parameters: 13,320,376 (ALL trainable - no frozen backbone)\n",
            "  cantor: pos=1,578,499, neg=1,578,499\n",
            "  simplex: pos=1,578,505, neg=1,578,505\n",
            "  helix: pos=1,578,566, neg=1,578,566\n",
            "  beatrix: pos=1,578,499, neg=1,578,499\n",
            "  dino_proj: 264,192\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training (with cached DINO latents)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/25 | Loss: 0.2104 | Train: 93.4% | Test: 95.9% * | Time: 42.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/25 | Loss: 0.0952 | Train: 96.9% | Test: 96.4% * | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/25 | Loss: 0.0809 | Train: 97.4% | Test: 96.7% * | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/25 | Loss: 0.0668 | Train: 97.8% | Test: 96.7% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/25 | Loss: 0.0544 | Train: 98.1% | Test: 96.5% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/25 | Loss: 0.0439 | Train: 98.5% | Test: 96.3% | Time: 42.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/25 | Loss: 0.0333 | Train: 98.9% | Test: 96.5% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/25 | Loss: 0.0231 | Train: 99.2% | Test: 96.4% | Time: 42.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/25 | Loss: 0.0159 | Train: 99.5% | Test: 96.3% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/25 | Loss: 0.0111 | Train: 99.6% | Test: 96.4% | Time: 42.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/25 | Loss: 0.0079 | Train: 99.8% | Test: 96.6% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/25 | Loss: 0.0049 | Train: 99.9% | Test: 96.5% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/25 | Loss: 0.0043 | Train: 99.9% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/25 | Loss: 0.0016 | Train: 100.0% | Test: 96.3% | Time: 42.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/25 | Loss: 0.0008 | Train: 100.0% | Test: 96.5% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/25 | Loss: 0.0004 | Train: 100.0% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/25 | Loss: 0.0003 | Train: 100.0% | Test: 96.6% | Time: 42.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/25 | Loss: 0.0002 | Train: 100.0% | Test: 96.6% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/25 | Loss: 0.0002 | Train: 100.0% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/25 | Loss: 0.0001 | Train: 100.0% | Test: 96.5% | Time: 42.5s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 96.70%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy:\n",
            "----------------------------------------\n",
            "  automobile  :  98.8% ███████████████████\n",
            "  ship        :  98.1% ███████████████████\n",
            "  horse       :  97.9% ███████████████████\n",
            "  airplane    :  97.5% ███████████████████\n",
            "  truck       :  97.3% ███████████████████\n",
            "  frog        :  97.2% ███████████████████\n",
            "  bird        :  96.7% ███████████████████\n",
            "  deer        :  96.1% ███████████████████\n",
            "  dog         :  93.1% ██████████████████\n",
            "  cat         :  92.3% ██████████████████\n",
            "\n",
            "Tower Opinion Norms (pos vs neg by geometry):\n",
            "--------------------------------------------------------------------------------\n",
            "Class        cantor_p cantor_n simple_p simple_n  helix_p  helix_n beatri_p beatri_n\n",
            "--------------------------------------------------------------------------------\n",
            "airplane         8.09     7.71     7.58     7.60     8.06     8.03     7.98     8.09\n",
            "automobile       5.49     5.46     5.27     5.16     5.35     5.45     5.35     5.50\n",
            "bird             7.50     7.36     7.12     7.02     7.46     7.42     7.36     7.47\n",
            "cat              5.93     5.90     5.70     5.57     5.83     5.88     5.81     5.94\n",
            "deer             6.96     6.84     6.52     6.33     6.86     6.80     6.89     7.02\n",
            "dog              5.43     5.37     5.22     5.12     5.37     5.45     5.26     5.41\n",
            "frog             7.20     7.04     6.73     6.54     7.06     6.97     7.16     7.36\n",
            "horse            4.92     4.84     4.76     4.60     4.84     4.94     4.84     4.92\n",
            "ship             5.62     5.34     5.34     5.35     5.63     5.60     5.57     5.63\n",
            "truck            4.53     4.46     4.39     4.24     4.44     4.56     4.40     4.50\n",
            "\n",
            "Fused Geometry Norms by Class:\n",
            "------------------------------------------------------------\n",
            "Class            cantor    simplex      helix    beatrix\n",
            "------------------------------------------------------------\n",
            "airplane           1.23       1.29       1.29       1.18\n",
            "automobile         1.02       1.08       1.04       1.01\n",
            "bird               1.12       1.18       1.15       1.08\n",
            "cat                1.01       1.08       1.04       0.99\n",
            "deer               1.10       1.18       1.14       1.07\n",
            "dog                0.99       1.07       1.03       0.97\n",
            "frog               1.07       1.12       1.13       1.06\n",
            "horse              0.98       1.08       1.04       0.98\n",
            "ship               1.11       1.11       1.13       1.04\n",
            "truck              0.98       1.06       1.04       0.98\n",
            "\n",
            "Strongest Geometry per Class:\n",
            "----------------------------------------\n",
            "  airplane    : simplex    (1.29)\n",
            "  automobile  : simplex    (1.08)\n",
            "  bird        : simplex    (1.18)\n",
            "  cat         : simplex    (1.08)\n",
            "  deer        : simplex    (1.18)\n",
            "  dog         : simplex    (1.07)\n",
            "  frog        : helix      (1.13)\n",
            "  horse       : simplex    (1.08)\n",
            "  ship        : helix      (1.13)\n",
            "  truck       : simplex    (1.06)\n",
            "\n",
            "Pos vs Neg Dominance (avg across classes):\n",
            "----------------------------------------\n",
            "  cantor    : pos=6.17, neg=6.03 → POS (+0.13)\n",
            "  simplex   : pos=5.86, neg=5.75 → POS (+0.11)\n",
            "  helix     : pos=6.09, neg=6.11 → NEG (+0.02)\n",
            "  beatrix   : pos=6.06, neg=6.18 → NEG (+0.12)\n",
            "\n",
            "DINO Opinion Norms by Class:\n",
            "----------------------------------------\n",
            "  airplane    :  12.24 ████████████\n",
            "  automobile  :  10.52 ██████████\n",
            "  bird        :  11.94 ███████████\n",
            "  cat         :   5.99 █████\n",
            "  deer        :   8.80 ████████\n",
            "  dog         :  10.91 ██████████\n",
            "  frog        :  12.92 ████████████\n",
            "  horse       :  11.37 ███████████\n",
            "  ship        :  10.41 ██████████\n",
            "  truck       :  11.66 ███████████\n",
            "\n",
            "  DINO avg: 10.68\n",
            "  Geom avg: 1.08\n",
            "  Ratio:    9.87x\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 tower + dino v3 cifar100"
      ],
      "metadata": {
        "id": "lZFLktRL-gjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Import the actual codebase towers\n",
        "from geofractal.router.prefab.geometric_towers import (\n",
        "    GeometricTower,\n",
        "    AgathaTowerCollective,\n",
        "    GeometryType,\n",
        "    TowerOpinion,\n",
        "    CollectiveOpinion,\n",
        "    create_tower_collective,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 outputs for entire dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = 768\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINO latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINO cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        model_name = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "        processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "        backbone = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            latents = outputs.pooler_output.cpu()\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINO latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, dino_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.dino_latents = dino_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, int]:\n",
        "        image, label = self.base_dataset[idx]\n",
        "        dino_latent = self.dino_latents[idx]\n",
        "        return image, dino_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with weakening controls.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "        dropout: float = 0.0,\n",
        "        scale: float = 1.0,\n",
        "        bottleneck: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        if bottleneck:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, bottleneck),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(bottleneck, out_dim),\n",
        "            )\n",
        "        else:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, out_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(out_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj(x)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using actual GeometricTowers from codebase.\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → AgathaTowerCollective → Fused Opinion\n",
        "                              ↓\n",
        "        DINO latent → DinoProjection → DINO Opinion\n",
        "                              ↓\n",
        "        [Collective Fused + DINO] → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        dino_dim: int = 768,\n",
        "        dino_dropout: float = 0.3,\n",
        "        dino_scale: float = 0.5,\n",
        "        dino_bottleneck: Optional[int] = 128,\n",
        "        tower_subset: Optional[List[str]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Tower collective from codebase\n",
        "        self.collective = create_tower_collective(\n",
        "            dim=dim,\n",
        "            tower_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=dim // num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            activation='gelu',\n",
        "            fusion_type=fusion_type,\n",
        "            subset=tower_subset,\n",
        "        )\n",
        "\n",
        "        # DINO projection\n",
        "        self.dino_proj = DinoProjection(\n",
        "            dino_dim=dino_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + dino] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 2),\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['dino']\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, dino_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINO projection\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, dino_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINO\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "        opinions['dino'] = dino_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, dino_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, dino_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, dino_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, dino_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "torch.set_float32_matmul_precision('high')\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 with GeometricTowers from Codebase + DINOv3\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    LR = 3e-4\n",
        "    DIM = 128\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 4\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINO weakening\n",
        "    DINO_DROPOUT = 0.3\n",
        "    DINO_SCALE = 0.5\n",
        "    DINO_BOTTLENECK = 128\n",
        "\n",
        "    # Use all 6 tower geometries or subset\n",
        "    TOWER_SUBSET = None  # None = all 6, or ['cantor', 'beatrix', 'helix', 'simplex']\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"DINO: dropout={DINO_DROPOUT}, scale={DINO_SCALE}, bottleneck={DINO_BOTTLENECK}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"Train: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINO\n",
        "    cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_dino_latents = cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_dino_latents = cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_dino_latents)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_dino_latents)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with GeometricTowers...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        dino_dim=768,\n",
        "        dino_dropout=DINO_DROPOUT,\n",
        "        dino_scale=DINO_SCALE,\n",
        "        dino_bottleneck=DINO_BOTTLENECK,\n",
        "        tower_subset=TOWER_SUBSET,\n",
        "    ).to(device)\n",
        "\n",
        "    model.compile()\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        print(f\"  {name}: {tower_params:,} params, geometry={tower.geometry.value}\")\n",
        "\n",
        "    # Optimizer\n",
        "    optimizer = torch.optim.Adafactor(model.parameters(), lr=LR, weight_decay=0.00)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        scheduler.step()\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM8o41Yj-kJ3",
        "outputId": "2f1473ae-9e29-4e56-bc18-c171a54cd9d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 with GeometricTowers from Codebase + DINOv3\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 4 → 64 patches\n",
            "Tower depth: 1\n",
            "DINO: dropout=0.3, scale=0.5, bottleneck=128\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINO latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINO latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "\n",
            "Building model with GeometricTowers...\n",
            "\n",
            "Towers: ['cantor', 'beatrix', 'helix', 'simplex', 'fractal', 'standard']\n",
            "Patches: 64\n",
            "Total parameters: 1,547,932\n",
            "  cantor: 214,659 params, geometry=cantor\n",
            "  beatrix: 214,659 params, geometry=beatrix\n",
            "  helix: 214,756 params, geometry=helix\n",
            "  simplex: 214,988 params, geometry=simplex\n",
            "  fractal: 214,721 params, geometry=fractal\n",
            "  standard: 214,720 params, geometry=standard\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | Loss: 4.4603 | Train: 5.0% | Test: 12.5% * | Time: 30.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | Loss: 4.1392 | Train: 18.0% | Test: 33.8% * | Time: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | Loss: 3.6891 | Train: 35.3% | Test: 50.3% * | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | Loss: 3.1280 | Train: 47.5% | Test: 59.5% * | Time: 26.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | Loss: 2.5509 | Train: 55.5% | Test: 65.5% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | Loss: 2.0593 | Train: 61.0% | Test: 69.9% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | Loss: 1.6924 | Train: 65.0% | Test: 72.8% * | Time: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | Loss: 1.4524 | Train: 67.8% | Test: 74.6% * | Time: 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | Loss: 1.2981 | Train: 69.5% | Test: 76.0% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 1.1882 | Train: 71.1% | Test: 76.7% * | Time: 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 1.1174 | Train: 72.0% | Test: 77.6% * | Time: 25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 1.0648 | Train: 72.9% | Test: 78.0% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 1.0154 | Train: 73.8% | Test: 78.3% * | Time: 25.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 0.9830 | Train: 74.4% | Test: 78.9% * | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 0.9602 | Train: 74.6% | Test: 79.3% * | Time: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 0.9396 | Train: 75.1% | Test: 79.5% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 0.9153 | Train: 75.6% | Test: 79.5% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 0.9049 | Train: 75.9% | Test: 79.7% * | Time: 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 0.8846 | Train: 76.4% | Test: 80.0% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 0.8797 | Train: 76.6% | Test: 79.9% | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 0.8635 | Train: 76.9% | Test: 80.0% * | Time: 26.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 0.8480 | Train: 77.1% | Test: 80.3% * | Time: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 0.8399 | Train: 77.4% | Test: 80.3% | Time: 25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 0.8382 | Train: 77.3% | Test: 80.4% * | Time: 25.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 0.8282 | Train: 77.8% | Test: 80.5% * | Time: 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 0.8181 | Train: 78.1% | Test: 80.6% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 0.8134 | Train: 77.8% | Test: 80.7% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 0.8070 | Train: 78.1% | Test: 80.8% * | Time: 25.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 0.8047 | Train: 78.1% | Test: 80.8% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.7968 | Train: 78.3% | Test: 80.8% * | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.7966 | Train: 78.5% | Test: 81.0% * | Time: 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.7907 | Train: 78.5% | Test: 81.0% * | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.7861 | Train: 78.5% | Test: 81.0% | Time: 25.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.7813 | Train: 78.6% | Test: 81.1% * | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.7848 | Train: 78.5% | Test: 81.1% | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.7772 | Train: 78.7% | Test: 81.3% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.7775 | Train: 78.7% | Test: 81.2% | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.7716 | Train: 78.7% | Test: 81.3% * | Time: 26.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.7763 | Train: 78.8% | Test: 81.3% | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.7675 | Train: 78.8% | Test: 81.2% | Time: 25.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.7695 | Train: 78.9% | Test: 81.3% * | Time: 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.7698 | Train: 78.8% | Test: 81.3% | Time: 26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.7688 | Train: 78.8% | Test: 81.3% * | Time: 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.7713 | Train: 79.0% | Test: 81.3% | Time: 26.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.7691 | Train: 78.9% | Test: 81.4% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.7719 | Train: 78.8% | Test: 81.4% | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.7654 | Train: 79.0% | Test: 81.4% | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.7687 | Train: 78.8% | Test: 81.4% | Time: 25.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.7682 | Train: 78.9% | Test: 81.4% * | Time: 25.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.7662 | Train: 78.8% | Test: 81.4% | Time: 25.5s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 81.38%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  orange          :  99.0% ███████████████████\n",
            "  motorcycle      :  97.0% ███████████████████\n",
            "  sunflower       :  97.0% ███████████████████\n",
            "  apple           :  96.0% ███████████████████\n",
            "  elephant        :  96.0% ███████████████████\n",
            "  chimpanzee      :  94.0% ██████████████████\n",
            "  keyboard        :  94.0% ██████████████████\n",
            "  aquarium_fish   :  93.0% ██████████████████\n",
            "  mushroom        :  93.0% ██████████████████\n",
            "  palm_tree       :  93.0% ██████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  possum          :  65.0% █████████████\n",
            "  crocodile       :  64.0% ████████████\n",
            "  oak_tree        :  61.0% ████████████\n",
            "  bowl            :  60.0% ████████████\n",
            "  forest          :  60.0% ████████████\n",
            "  boy             :  57.0% ███████████\n",
            "  maple_tree      :  57.0% ███████████\n",
            "  girl            :  56.0% ███████████\n",
            "  otter           :  56.0% ███████████\n",
            "  shrew           :  48.0% █████████\n",
            "\n",
            "  Average across 100 classes: 81.4%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor      : 4.05\n",
            "  beatrix     : 4.14\n",
            "  helix       : 4.34\n",
            "  simplex     : 4.31\n",
            "  fractal     : 4.15\n",
            "  standard    : 4.21\n",
            "  dino        : 1.82\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer tower builder system"
      ],
      "metadata": {
        "id": "640jOqQHA2RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 outputs for entire dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = 768\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINO latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINO cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        model_name = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "        processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "        backbone = AutoModel.from_pretrained(model_name).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            latents = outputs.pooler_output.cpu()\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINO latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, dino_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.dino_latents = dino_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[Tensor, Tensor, int]:\n",
        "        image, label = self.base_dataset[idx]\n",
        "        dino_latent = self.dino_latents[idx]\n",
        "        return image, dino_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with weakening controls.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "        dropout: float = 0.0,\n",
        "        scale: float = 1.0,\n",
        "        bottleneck: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        if bottleneck:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, bottleneck),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(bottleneck, out_dim),\n",
        "            )\n",
        "        else:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, out_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(out_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj(x)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder.\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion\n",
        "                              ↓\n",
        "        DINO latent → DinoProjection → DINO Opinion\n",
        "                              ↓\n",
        "        [Collective Fused + DINO] → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        dino_dim: int = 768,\n",
        "        dino_dropout: float = 0.3,\n",
        "        dino_scale: float = 0.5,\n",
        "        dino_bottleneck: Optional[int] = 128,\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # DINO projection\n",
        "        self.dino_proj = DinoProjection(\n",
        "            dino_dim=dino_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + dino] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 2),\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['dino']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, dino_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINO projection\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, dino_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINO\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "        opinions['dino'] = dino_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, dino_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, dino_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, dino_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, dino_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 with Configurable Tower Builder + DINOv3\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINO weakening\n",
        "    DINO_DROPOUT = 0.3\n",
        "    DINO_SCALE = 0.5\n",
        "    DINO_BOTTLENECK = 128\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"DINO: dropout={DINO_DROPOUT}, scale={DINO_SCALE}, bottleneck={DINO_BOTTLENECK}\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINO\n",
        "    cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_dino_latents = cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_dino_latents = cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_dino_latents)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_dino_latents)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        dino_dim=768,\n",
        "        dino_dropout=DINO_DROPOUT,\n",
        "        dino_scale=DINO_SCALE,\n",
        "        dino_bottleneck=DINO_BOTTLENECK,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # Get current LR from Adafactor (for logging)\n",
        "        current_lr = optimizer.param_groups[0].get('lr', 'adaptive')\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4qfH-U8A1VE",
        "outputId": "39793d9f-536f-430f-ed93-54bd08a58ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 with Configurable Tower Builder + DINOv3\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "DINO: dropout=0.3, scale=0.5, bottleneck=128\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINO latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINO latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "\n",
            "Building model with ConfigurableTowers...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Patches: 256\n",
            "Total parameters: 7,392,081\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | Loss: 4.4952 | Train: 4.0% | Test: 10.7% * | Time: 52.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | Loss: 3.8391 | Train: 25.6% | Test: 52.2% * | Time: 52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | Loss: 2.1653 | Train: 60.2% | Test: 73.3% * | Time: 52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | Loss: 1.1309 | Train: 72.1% | Test: 78.0% * | Time: 52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | Loss: 0.8959 | Train: 75.9% | Test: 80.0% * | Time: 52.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | Loss: 0.7961 | Train: 78.2% | Test: 81.4% * | Time: 51.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | Loss: 0.7313 | Train: 79.6% | Test: 82.1% * | Time: 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | Loss: 0.6794 | Train: 80.9% | Test: 82.5% * | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | Loss: 0.6390 | Train: 81.8% | Test: 82.9% * | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 0.6103 | Train: 82.4% | Test: 83.0% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 0.5818 | Train: 83.0% | Test: 83.5% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 0.5594 | Train: 83.6% | Test: 83.6% * | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 0.5334 | Train: 84.3% | Test: 83.6% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 0.5159 | Train: 84.6% | Test: 83.7% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 0.5027 | Train: 85.0% | Test: 83.8% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 0.4858 | Train: 85.4% | Test: 83.5% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 0.4730 | Train: 85.7% | Test: 84.0% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 0.4611 | Train: 85.8% | Test: 83.7% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 0.4476 | Train: 86.4% | Test: 84.0% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 0.4322 | Train: 86.8% | Test: 83.7% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 0.4233 | Train: 87.0% | Test: 83.4% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 0.4192 | Train: 86.9% | Test: 84.1% * | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 0.4117 | Train: 87.2% | Test: 84.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 0.4039 | Train: 87.3% | Test: 83.1% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 0.3985 | Train: 87.5% | Test: 83.5% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 0.3906 | Train: 87.7% | Test: 83.6% | Time: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 0.3829 | Train: 87.8% | Test: 83.2% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 0.3611 | Train: 88.4% | Test: 84.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 0.3593 | Train: 88.5% | Test: 83.4% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.3486 | Train: 89.0% | Test: 83.7% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.3376 | Train: 89.0% | Test: 83.3% | Time: 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.3285 | Train: 89.4% | Test: 83.7% | Time: 52.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.3215 | Train: 89.5% | Test: 83.9% | Time: 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.3103 | Train: 89.8% | Test: 83.5% | Time: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.2951 | Train: 90.3% | Test: 83.6% | Time: 52.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.2954 | Train: 90.2% | Test: 84.1% * | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.2885 | Train: 90.4% | Test: 83.7% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.2821 | Train: 90.7% | Test: 83.1% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.2768 | Train: 90.8% | Test: 83.7% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.2665 | Train: 91.1% | Test: 84.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.2602 | Train: 91.3% | Test: 83.7% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.2539 | Train: 91.5% | Test: 83.9% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.2538 | Train: 91.5% | Test: 83.6% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.2414 | Train: 91.8% | Test: 84.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.2407 | Train: 91.8% | Test: 83.7% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.2323 | Train: 92.3% | Test: 83.9% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.2328 | Train: 92.1% | Test: 84.1% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.2295 | Train: 92.2% | Test: 83.7% | Time: 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.2256 | Train: 92.3% | Test: 83.8% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.2218 | Train: 92.5% | Test: 83.8% | Time: 52.6s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 84.14%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  orange          : 100.0% ████████████████████\n",
            "  apple           :  98.0% ███████████████████\n",
            "  keyboard        :  98.0% ███████████████████\n",
            "  chimpanzee      :  97.0% ███████████████████\n",
            "  motorcycle      :  96.0% ███████████████████\n",
            "  pickup_truck    :  96.0% ███████████████████\n",
            "  tractor         :  96.0% ███████████████████\n",
            "  aquarium_fish   :  95.0% ███████████████████\n",
            "  clock           :  95.0% ███████████████████\n",
            "  elephant        :  95.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  oak_tree        :  70.0% ██████████████\n",
            "  seal            :  70.0% ██████████████\n",
            "  girl            :  68.0% █████████████\n",
            "  possum          :  66.0% █████████████\n",
            "  shrew           :  65.0% █████████████\n",
            "  forest          :  63.0% ████████████\n",
            "  boy             :  60.0% ████████████\n",
            "  willow_tree     :  60.0% ████████████\n",
            "  otter           :  59.0% ███████████\n",
            "  maple_tree      :  58.0% ███████████\n",
            "\n",
            "  Average across 100 classes: 83.8%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 9.10\n",
            "  cantor_neg  : 10.55\n",
            "  beatrix_pos : 8.73\n",
            "  beatrix_neg : 3.85\n",
            "  helix_pos   : 10.29\n",
            "  helix_neg   : 10.62\n",
            "  simplex_pos : 14.10\n",
            "  simplex_neg : 12.83\n",
            "  dino        : 8.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer mega tower, vit-l dino"
      ],
      "metadata": {
        "id": "vpBbtMymPvK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 ViT-L outputs for entire dataset.\n",
        "\n",
        "    ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge - so we get both DINO dense features\n",
        "    AND CLIP-like semantic alignment in one model.\n",
        "    \"\"\"\n",
        "\n",
        "    # ViT-L: 1024d embedding, distilled from 7B with multi-teacher (includes CLIP-L)\n",
        "    DINO_MODEL = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_vitl16_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3-L latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ViT-L cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching DINOv3-L {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ViT uses pooler_output (CLS token projected)\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 1024]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINOv3 ViT-L latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, dino_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.dino_latents = dino_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        dino_latent = self.dino_latents[idx]\n",
        "        return image, dino_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with weakening controls.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "        dropout: float = 0.0,\n",
        "        scale: float = 1.0,\n",
        "        bottleneck: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        if bottleneck:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, bottleneck),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(bottleneck, out_dim),\n",
        "            )\n",
        "        else:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, out_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(out_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj(x)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder + DINOv3 ViT-L.\n",
        "\n",
        "    DINOv3 ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge - so we get both DINO dense features\n",
        "    AND semantic alignment in one 1024d embedding.\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion\n",
        "                              ↓\n",
        "        DINOv3-L latent (1024) → DinoProjection → DINO Opinion (256)\n",
        "                              ↓\n",
        "        [Collective Fused + DINO] → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        dino_dim: int = 1024,  # ViT-L is 1024d\n",
        "        dino_dropout: float = 0.3,\n",
        "        dino_scale: float = 0.5,\n",
        "        dino_bottleneck: Optional[int] = 128,\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # DINO projection (from 1024d ViT-L to 256d)\n",
        "        self.dino_proj = DinoProjection(\n",
        "            dino_dim=dino_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + dino] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 2),\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['dino']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, dino_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINO projection\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, dino_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINO\n",
        "        dino_opinion = self.dino_proj(dino_latents)\n",
        "        opinions['dino'] = dino_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, dino_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, dino_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, dino_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, dino_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        dino_latents = dino_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, dino_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 with Configurable Tower Builder + DINOv3 ViT-L\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DINOv3 ViT-L: Distilled from 7B using multi-teacher (includes CLIP-L)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINO ViT-L config (1024d embedding)\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "    DINO_DROPOUT = 0.3\n",
        "    DINO_SCALE = 0.5\n",
        "    DINO_BOTTLENECK = 128\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"DINO ViT-L: dim={DINO_DIM}, dropout={DINO_DROPOUT}, scale={DINO_SCALE}, bottleneck={DINO_BOTTLENECK}\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINOv3 ViT-L\n",
        "    dino_cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_dino_latents = dino_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_dino_latents = dino_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_dino_latents)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_dino_latents)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers + DINOv3 ViT-L...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        dino_dim=DINO_DIM,\n",
        "        dino_dropout=DINO_DROPOUT,\n",
        "        dino_scale=DINO_SCALE,\n",
        "        dino_bottleneck=DINO_BOTTLENECK,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Expert head: DINOv3 ViT-L (1024d → {DIM}d)\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # Get current LR from Adafactor (for logging)\n",
        "        current_lr = optimizer.param_groups[0].get('lr', 'adaptive')\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9f1c858c6ccf4d07bb636fd9e53f0da9",
            "ea43eed4f2ee40ec95f105c7d9e42bd6",
            "d71a7c484ff0403d978663a045dd1a08",
            "0e652fe65f004d7986067bbcfc034b8a",
            "4e457315be8e43d6871ef5c24a59fd87",
            "4d577fbfd6d84128ad625d402f363b96",
            "34229ce0aca54ba1b4286db89c05d925",
            "66d52246ced04920903b52e2230f1da3",
            "877cb4899abe45e88114817d83602c6c",
            "712a4975276045509aaa4929e3fc8ce8",
            "978f5f8bb54d41548ba7309b9a5278d4",
            "4fcee14038b044d6830aa905bf7d2269",
            "fa18b5f77bc644f295d0db88bcef39a1",
            "bd958f8ed1e043aa971a141b376ab9c3",
            "bd8730ca482f4de48688a05f57f39d76",
            "e5c920b4961b4a3dafda552c4424140f",
            "a342b7516dd04b36970cdaaffe567288",
            "c269ab6ac0ab40eb844e64555c2c28ec",
            "e426527ab2c44a2692edcf4891c1efb7",
            "161fa9207bb64d1d9fa5841a303385fd",
            "fe18fb891e0543fea41353a310ccc558",
            "726e9caa1e0146efb64d18967803efdd",
            "1cd2e198926e409784f5eced56aed830",
            "0ba09ef63c28456a970982254677b5c9",
            "85e5fef78a74447a90776201e8954db4",
            "ea885bfc6ce3416aa76816dea0fc4c2f",
            "53ded89e42a544c8aaffbf07e663fb17",
            "4f8f1c69950e4d7cbf265dea213bad8d",
            "d8c17eead38c4c5db4a586702bf5baa9",
            "171f017e4bbd4e02a73a43ddba610080",
            "58cb9170fdd74ab7a4e4dcf64a1b728d",
            "5dfa1f4859fb41d6be223af70989b99f",
            "0859b65c24684d2a898e974d047dca0e"
          ]
        },
        "id": "-u9oXgGLPyo5",
        "outputId": "c359eb47-6ac6-42d7-bfe0-95f6d74ca2db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 with Configurable Tower Builder + DINOv3 ViT-L\n",
            "================================================================================\n",
            "DINOv3 ViT-L: Distilled from 7B using multi-teacher (includes CLIP-L)\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "DINO ViT-L: dim=1024, dropout=0.3, scale=0.5, bottleneck=128\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Building DINOv3 ViT-L cache for train (50000 images)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f1c858c6ccf4d07bb636fd9e53f0da9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fcee14038b044d6830aa905bf7d2269"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.21G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cd2e198926e409784f5eced56aed830"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching DINOv3-L train: 100%|██████████| 782/782 [01:45<00:00,  7.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/dinov3_vitl16_cifar100_train.pt - shape: torch.Size([50000, 1024])\n",
            "Building DINOv3 ViT-L cache for test (10000 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching DINOv3-L test: 100%|██████████| 157/157 [00:21<00:00,  7.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/dinov3_vitl16_cifar100_test.pt - shape: torch.Size([10000, 1024])\n",
            "\n",
            "Building model with ConfigurableTowers + DINOv3 ViT-L...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Expert head: DINOv3 ViT-L (1024d → 256d)\n",
            "Patches: 256\n",
            "Total parameters: 7,425,361\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | Loss: 4.5061 | Train: 3.2% | Test: 9.2% * | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | Loss: 3.8775 | Train: 24.5% | Test: 57.0% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | Loss: 2.0305 | Train: 68.4% | Test: 84.0% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | Loss: 0.7533 | Train: 82.9% | Test: 88.0% * | Time: 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | Loss: 0.4946 | Train: 86.7% | Test: 89.5% * | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | Loss: 0.4018 | Train: 88.8% | Test: 90.6% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | Loss: 0.3529 | Train: 90.0% | Test: 91.0% * | Time: 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | Loss: 0.3191 | Train: 90.8% | Test: 91.5% * | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | Loss: 0.2877 | Train: 91.5% | Test: 91.6% * | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 0.2634 | Train: 92.3% | Test: 92.0% * | Time: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 0.2474 | Train: 92.5% | Test: 91.9% | Time: 52.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 0.2265 | Train: 93.1% | Test: 92.0% * | Time: 52.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 0.2087 | Train: 93.5% | Test: 92.2% * | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 0.1970 | Train: 93.8% | Test: 91.9% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 0.1846 | Train: 94.2% | Test: 92.1% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 0.1752 | Train: 94.4% | Test: 92.1% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 0.1644 | Train: 94.7% | Test: 92.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 0.1544 | Train: 95.0% | Test: 92.2% * | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 0.1511 | Train: 95.0% | Test: 92.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 0.1426 | Train: 95.3% | Test: 91.7% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 0.1385 | Train: 95.3% | Test: 91.7% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 0.1361 | Train: 95.6% | Test: 91.8% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 0.1298 | Train: 95.6% | Test: 91.7% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 0.1236 | Train: 95.9% | Test: 92.1% | Time: 53.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 0.1212 | Train: 95.9% | Test: 91.7% | Time: 53.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 0.1200 | Train: 95.9% | Test: 91.8% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 0.1155 | Train: 96.1% | Test: 91.7% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 0.1118 | Train: 96.1% | Test: 91.7% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 0.1035 | Train: 96.5% | Test: 91.8% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.1013 | Train: 96.5% | Test: 91.8% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.0949 | Train: 96.8% | Test: 92.0% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.0921 | Train: 96.8% | Test: 91.7% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.0915 | Train: 96.9% | Test: 91.9% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.0842 | Train: 97.1% | Test: 91.8% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.0827 | Train: 97.2% | Test: 91.8% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.0798 | Train: 97.3% | Test: 91.9% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.0785 | Train: 97.3% | Test: 92.2% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.0752 | Train: 97.5% | Test: 91.9% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.0713 | Train: 97.5% | Test: 91.9% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.0699 | Train: 97.5% | Test: 91.8% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.0677 | Train: 97.7% | Test: 91.9% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.0654 | Train: 97.7% | Test: 91.9% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.0674 | Train: 97.6% | Test: 92.1% | Time: 52.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.0645 | Train: 97.8% | Test: 92.1% | Time: 52.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.0602 | Train: 97.9% | Test: 92.0% | Time: 52.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.0619 | Train: 97.8% | Test: 92.1% | Time: 52.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.0627 | Train: 97.8% | Test: 92.2% | Time: 52.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.0590 | Train: 98.0% | Test: 92.2% | Time: 52.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.0578 | Train: 98.0% | Test: 92.3% * | Time: 53.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.0560 | Train: 98.0% | Test: 92.3% | Time: 52.8s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 92.35%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  lawn_mower      : 100.0% ████████████████████\n",
            "  orange          : 100.0% ████████████████████\n",
            "  pickup_truck    : 100.0% ████████████████████\n",
            "  bicycle         :  99.0% ███████████████████\n",
            "  chimpanzee      :  99.0% ███████████████████\n",
            "  clock           :  99.0% ███████████████████\n",
            "  mushroom        :  99.0% ███████████████████\n",
            "  sunflower       :  99.0% ███████████████████\n",
            "  tractor         :  99.0% ███████████████████\n",
            "  apple           :  98.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  porcupine       :  82.0% ████████████████\n",
            "  willow_tree     :  81.0% ████████████████\n",
            "  mouse           :  80.0% ████████████████\n",
            "  otter           :  80.0% ████████████████\n",
            "  shrew           :  80.0% ████████████████\n",
            "  bowl            :  79.0% ███████████████\n",
            "  boy             :  77.0% ███████████████\n",
            "  girl            :  75.0% ███████████████\n",
            "  forest          :  72.0% ██████████████\n",
            "  oak_tree        :  60.0% ████████████\n",
            "\n",
            "  Average across 100 classes: 92.3%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 5.22\n",
            "  cantor_neg  : 6.50\n",
            "  beatrix_pos : 4.97\n",
            "  beatrix_neg : 6.71\n",
            "  helix_pos   : 7.68\n",
            "  helix_neg   : 2.55\n",
            "  simplex_pos : 5.45\n",
            "  simplex_neg : 8.04\n",
            "  dino        : 8.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# clip-l + convnext dino v3 + mega tower"
      ],
      "metadata": {
        "id": "PKr4pkuhU7jS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 ConvNeXt-Small outputs for entire dataset.\"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 768  # ConvNeXt-Small embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ConvNeXt-S latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ConvNeXt-S cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ConvNeXt-S {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ConvNeXt uses pooler_output\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 768]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class DinoVitLCacher:\n",
        "    \"\"\"Precaches DINOv3 ViT-L outputs for entire dataset.\n",
        "\n",
        "    ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_vitl16_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ViT-L latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ViT-L cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ViT-L {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ViT uses pooler_output (CLS token projected)\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 1024]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINOv3 ConvNeXt-S and ViT-L latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, convnext_latents: Tensor, vitl_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.convnext_latents = convnext_latents\n",
        "        self.vitl_latents = vitl_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        convnext_latent = self.convnext_latents[idx]\n",
        "        vitl_latent = self.vitl_latents[idx]\n",
        "        return image, convnext_latent, vitl_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with weakening controls.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "        dropout: float = 0.0,\n",
        "        scale: float = 1.0,\n",
        "        bottleneck: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        if bottleneck:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, bottleneck),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(bottleneck, out_dim),\n",
        "            )\n",
        "        else:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, out_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(out_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj(x)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder + dual DINOv3 experts.\n",
        "\n",
        "    Two DINOv3 experts provide complementary signals:\n",
        "    - ConvNeXt-Small (768d): Dense spatial features, efficient\n",
        "    - ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion (256)\n",
        "                              ↓\n",
        "        DINOv3 ConvNeXt-S (768) → ConvNextProj → ConvNeXt Opinion (256)\n",
        "        DINOv3 ViT-L (1024) → ViTLProj → ViT-L Opinion (256)\n",
        "                              ↓\n",
        "        [Collective + ConvNeXt + ViT-L] (768) → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        convnext_dim: int = 768,   # ConvNeXt-Small\n",
        "        vitl_dim: int = 1024,      # ViT-L\n",
        "        dino_dropout: float = 0.3,\n",
        "        dino_scale: float = 0.5,\n",
        "        dino_bottleneck: Optional[int] = 128,\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ConvNeXt-Small projection (768d → 256d)\n",
        "        self.convnext_proj = DinoProjection(\n",
        "            dino_dim=convnext_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ViT-L projection (1024d → 256d)\n",
        "        self.vitl_proj = DinoProjection(\n",
        "            dino_dim=vitl_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + convnext + vitl] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),  # 256 * 3 = 768\n",
        "            nn.Linear(dim * 3, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['convnext', 'vitl']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINOv3 projections\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINOv3 experts\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "        opinions['convnext'] = convnext_opinion\n",
        "        opinions['vitl'] = vitl_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, convnext_latents, vitl_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, convnext_latents, vitl_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, convnext_latents, vitl_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, convnext_latents, vitl_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 with Geometric Towers + Dual DINOv3 Experts\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DINOv3 ConvNeXt-S (768d): Dense spatial features\")\n",
        "    print(\"DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINOv3 expert config\n",
        "    CONVNEXT_DIM = 768   # ConvNeXt-Small\n",
        "    VITL_DIM = 1024      # ViT-L\n",
        "    DINO_DROPOUT = 0.3\n",
        "    DINO_SCALE = 0.5\n",
        "    DINO_BOTTLENECK = 128\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"ConvNeXt-S: dim={CONVNEXT_DIM}, dropout={DINO_DROPOUT}, scale={DINO_SCALE}\")\n",
        "    print(f\"ViT-L: dim={VITL_DIM}, dropout={DINO_DROPOUT}, scale={DINO_SCALE}\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINOv3 ConvNeXt-Small\n",
        "    convnext_cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_convnext = convnext_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_convnext = convnext_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Cache DINOv3 ViT-L\n",
        "    vitl_cacher = DinoVitLCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_vitl = vitl_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_vitl = vitl_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_convnext, train_vitl)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_convnext, test_vitl)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers + Dual DINOv3 Experts...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        convnext_dim=CONVNEXT_DIM,\n",
        "        vitl_dim=VITL_DIM,\n",
        "        dino_dropout=DINO_DROPOUT,\n",
        "        dino_scale=DINO_SCALE,\n",
        "        dino_bottleneck=DINO_BOTTLENECK,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Expert heads: ConvNeXt-S (768→{DIM}), ViT-L (1024→{DIM})\")\n",
        "    print(f\"Classifier input: {DIM * 3} (collective + convnext + vitl)\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # Get current LR from Adafactor (for logging)\n",
        "        current_lr = optimizer.param_groups[0].get('lr', 'adaptive')\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzxLuSebVEe0",
        "outputId": "37d23492-484c-4844-e665-071c1345127f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 with Geometric Towers + Dual DINOv3 Experts\n",
            "================================================================================\n",
            "DINOv3 ConvNeXt-S (768d): Dense spatial features\n",
            "DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "ConvNeXt-S: dim=768, dropout=0.3, scale=0.5\n",
            "ViT-L: dim=1024, dropout=0.3, scale=0.5\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "\n",
            "Building model with ConfigurableTowers + Dual DINOv3 Experts...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Expert heads: ConvNeXt-S (768→256), ViT-L (1024→256)\n",
            "Classifier input: 768 (collective + convnext + vitl)\n",
            "Patches: 256\n",
            "Total parameters: 7,624,401\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | Loss: 4.4881 | Train: 4.9% | Test: 15.6% * | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | Loss: 3.6235 | Train: 37.8% | Test: 69.7% * | Time: 54.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | Loss: 1.5552 | Train: 75.7% | Test: 86.1% * | Time: 53.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | Loss: 0.6208 | Train: 85.2% | Test: 88.9% * | Time: 53.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | Loss: 0.4367 | Train: 87.9% | Test: 90.4% * | Time: 53.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | Loss: 0.3632 | Train: 89.7% | Test: 90.9% * | Time: 53.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | Loss: 0.3157 | Train: 90.8% | Test: 91.2% * | Time: 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | Loss: 0.2803 | Train: 91.7% | Test: 91.4% * | Time: 53.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | Loss: 0.2540 | Train: 92.4% | Test: 91.9% * | Time: 53.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 0.2295 | Train: 93.1% | Test: 91.9% * | Time: 53.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 0.2081 | Train: 93.6% | Test: 92.0% * | Time: 53.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 0.1906 | Train: 94.1% | Test: 91.6% | Time: 53.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 0.1712 | Train: 94.6% | Test: 91.9% | Time: 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 0.1617 | Train: 94.8% | Test: 92.2% * | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 0.1496 | Train: 95.2% | Test: 92.1% | Time: 53.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 0.1389 | Train: 95.5% | Test: 91.9% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 0.1293 | Train: 95.7% | Test: 91.6% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 0.1212 | Train: 96.1% | Test: 91.9% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 0.1159 | Train: 96.1% | Test: 92.0% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 0.1090 | Train: 96.2% | Test: 91.8% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 0.1053 | Train: 96.4% | Test: 91.3% | Time: 53.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 0.1046 | Train: 96.4% | Test: 91.9% | Time: 53.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 0.0992 | Train: 96.6% | Test: 91.6% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 0.0996 | Train: 96.7% | Test: 91.8% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 0.0970 | Train: 96.7% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 0.0967 | Train: 96.7% | Test: 92.0% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 0.0894 | Train: 97.0% | Test: 91.7% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 0.0838 | Train: 97.2% | Test: 91.7% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 0.0790 | Train: 97.3% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.0773 | Train: 97.3% | Test: 91.7% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.0728 | Train: 97.6% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.0690 | Train: 97.6% | Test: 91.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.0652 | Train: 97.8% | Test: 91.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.0627 | Train: 97.9% | Test: 92.0% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.0617 | Train: 97.9% | Test: 91.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.0588 | Train: 98.0% | Test: 91.7% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.0577 | Train: 98.0% | Test: 92.0% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.0545 | Train: 98.1% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.0525 | Train: 98.1% | Test: 91.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.0499 | Train: 98.3% | Test: 91.8% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.0509 | Train: 98.3% | Test: 91.5% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.0487 | Train: 98.4% | Test: 91.6% | Time: 55.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.0452 | Train: 98.4% | Test: 91.9% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.0441 | Train: 98.5% | Test: 91.9% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.0456 | Train: 98.5% | Test: 92.0% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.0413 | Train: 98.6% | Test: 91.9% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.0425 | Train: 98.5% | Test: 92.1% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.0455 | Train: 98.4% | Test: 91.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.0392 | Train: 98.6% | Test: 92.0% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.0379 | Train: 98.7% | Test: 92.1% | Time: 53.8s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 92.15%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  lawn_mower      : 100.0% ████████████████████\n",
            "  pickup_truck    : 100.0% ████████████████████\n",
            "  bicycle         :  99.0% ███████████████████\n",
            "  chimpanzee      :  99.0% ███████████████████\n",
            "  motorcycle      :  99.0% ███████████████████\n",
            "  orange          :  99.0% ███████████████████\n",
            "  television      :  99.0% ███████████████████\n",
            "  tractor         :  99.0% ███████████████████\n",
            "  bed             :  98.0% ███████████████████\n",
            "  chair           :  98.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  oak_tree        :  82.0% ████████████████\n",
            "  boy             :  81.0% ████████████████\n",
            "  possum          :  81.0% ████████████████\n",
            "  shrew           :  81.0% ████████████████\n",
            "  otter           :  80.0% ████████████████\n",
            "  forest          :  78.0% ███████████████\n",
            "  bowl            :  77.0% ███████████████\n",
            "  girl            :  76.0% ███████████████\n",
            "  willow_tree     :  72.0% ██████████████\n",
            "  maple_tree      :  69.0% █████████████\n",
            "\n",
            "  Average across 100 classes: 92.1%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 2.37\n",
            "  cantor_neg  : 6.54\n",
            "  beatrix_pos : 2.21\n",
            "  beatrix_neg : 7.33\n",
            "  helix_pos   : 4.17\n",
            "  helix_neg   : 5.81\n",
            "  simplex_pos : 6.84\n",
            "  simplex_neg : 5.69\n",
            "  convnext    : 5.01\n",
            "  vitl        : 8.02\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dino 3 clip_l + dino3 convnext small + mega tower - unchained"
      ],
      "metadata": {
        "id": "buqt9zeYjuoo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 ConvNeXt-Small outputs for entire dataset.\"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 768  # ConvNeXt-Small embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ConvNeXt-S latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ConvNeXt-S cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ConvNeXt-S {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ConvNeXt uses pooler_output\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 768]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class DinoVitLCacher:\n",
        "    \"\"\"Precaches DINOv3 ViT-L outputs for entire dataset.\n",
        "\n",
        "    ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_vitl16_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ViT-L latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ViT-L cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ViT-L {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ViT uses pooler_output (CLS token projected)\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 1024]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINOv3 ConvNeXt-S and ViT-L latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, convnext_latents: Tensor, vitl_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.convnext_latents = convnext_latents\n",
        "        self.vitl_latents = vitl_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        convnext_latent = self.convnext_latents[idx]\n",
        "        vitl_latent = self.vitl_latents[idx]\n",
        "        return image, convnext_latent, vitl_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class DinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with weakening controls.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "        dropout: float = 0.0,\n",
        "        scale: float = 1.0,\n",
        "        bottleneck: Optional[int] = None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "\n",
        "        if bottleneck:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, bottleneck),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(bottleneck, out_dim),\n",
        "            )\n",
        "        else:\n",
        "            self.proj = nn.Sequential(\n",
        "                nn.LayerNorm(dino_dim),\n",
        "                nn.Linear(dino_dim, out_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(out_dim, out_dim),\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.dropout(x)\n",
        "        x = self.proj(x)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder + dual DINOv3 experts.\n",
        "\n",
        "    Two DINOv3 experts provide complementary signals:\n",
        "    - ConvNeXt-Small (768d): Dense spatial features, efficient\n",
        "    - ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion (256)\n",
        "                              ↓\n",
        "        DINOv3 ConvNeXt-S (768) → ConvNextProj → ConvNeXt Opinion (256)\n",
        "        DINOv3 ViT-L (1024) → ViTLProj → ViT-L Opinion (256)\n",
        "                              ↓\n",
        "        [Collective + ConvNeXt + ViT-L] (768) → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        convnext_dim: int = 768,   # ConvNeXt-Small\n",
        "        vitl_dim: int = 1024,      # ViT-L\n",
        "        dino_dropout: float = 0.3,\n",
        "        dino_scale: float = 0.5,\n",
        "        dino_bottleneck: Optional[int] = 128,\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ConvNeXt-Small projection (768d → 256d)\n",
        "        self.convnext_proj = DinoProjection(\n",
        "            dino_dim=convnext_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ViT-L projection (1024d → 256d)\n",
        "        self.vitl_proj = DinoProjection(\n",
        "            dino_dim=vitl_dim,\n",
        "            out_dim=dim,\n",
        "            dropout=dino_dropout,\n",
        "            scale=dino_scale,\n",
        "            bottleneck=dino_bottleneck,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + convnext + vitl] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),  # 256 * 3 = 768\n",
        "            nn.Linear(dim * 3, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['convnext', 'vitl']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINOv3 projections\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINOv3 experts\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "        opinions['convnext'] = convnext_opinion\n",
        "        opinions['vitl'] = vitl_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, convnext_latents, vitl_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, convnext_latents, vitl_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, convnext_latents, vitl_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, convnext_latents, vitl_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100: Geometric Towers + Dual DINOv3 [UNCHAINED]\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DINOv3 ConvNeXt-S (768d): Dense spatial features\")\n",
        "    print(\"DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\")\n",
        "    print(\"NO WEAKENING - FULL POWER\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINOv3 expert config - UNCHAINED\n",
        "    CONVNEXT_DIM = 768   # ConvNeXt-Small\n",
        "    VITL_DIM = 1024      # ViT-L\n",
        "    DINO_DROPOUT = 0.0   # No dropout\n",
        "    DINO_SCALE = 1.0     # Full strength\n",
        "    DINO_BOTTLENECK = None  # No bottleneck - direct projection\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"ConvNeXt-S: dim={CONVNEXT_DIM} [UNCHAINED]\")\n",
        "    print(f\"ViT-L: dim={VITL_DIM} [UNCHAINED]\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINOv3 ConvNeXt-Small\n",
        "    convnext_cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_convnext = convnext_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_convnext = convnext_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Cache DINOv3 ViT-L\n",
        "    vitl_cacher = DinoVitLCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_vitl = vitl_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_vitl = vitl_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_convnext, train_vitl)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_convnext, test_vitl)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers + Dual DINOv3 Experts...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        convnext_dim=CONVNEXT_DIM,\n",
        "        vitl_dim=VITL_DIM,\n",
        "        dino_dropout=DINO_DROPOUT,\n",
        "        dino_scale=DINO_SCALE,\n",
        "        dino_bottleneck=DINO_BOTTLENECK,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Expert heads: ConvNeXt-S (768→{DIM}), ViT-L (1024→{DIM})\")\n",
        "    print(f\"Classifier input: {DIM * 3} (collective + convnext + vitl)\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        # Get current LR from Adafactor (for logging)\n",
        "        current_lr = optimizer.param_groups[0].get('lr', 'adaptive')\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrFSNqK7j0bx",
        "outputId": "26ee6378-0b59-4d57-cab5-4d57917d79b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100: Geometric Towers + Dual DINOv3 [UNCHAINED]\n",
            "================================================================================\n",
            "DINOv3 ConvNeXt-S (768d): Dense spatial features\n",
            "DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
            "NO WEAKENING - FULL POWER\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "ConvNeXt-S: dim=768 [UNCHAINED]\n",
            "ViT-L: dim=1024 [UNCHAINED]\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "\n",
            "Building model with ConfigurableTowers + Dual DINOv3 Experts...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Expert heads: ConvNeXt-S (768→256), ViT-L (1024→256)\n",
            "Classifier input: 768 (collective + convnext + vitl)\n",
            "Patches: 256\n",
            "Total parameters: 7,919,569\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | Loss: 4.4020 | Train: 10.8% | Test: 35.0% * | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | Loss: 2.8827 | Train: 63.4% | Test: 81.5% * | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | Loss: 0.8635 | Train: 85.8% | Test: 88.5% * | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | Loss: 0.4067 | Train: 89.9% | Test: 90.3% * | Time: 53.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | Loss: 0.2959 | Train: 92.0% | Test: 91.0% * | Time: 53.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | Loss: 0.2359 | Train: 93.3% | Test: 91.4% * | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | Loss: 0.1897 | Train: 94.5% | Test: 91.7% * | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | Loss: 0.1490 | Train: 95.7% | Test: 91.9% * | Time: 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | Loss: 0.1141 | Train: 96.7% | Test: 91.6% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | Loss: 0.0839 | Train: 97.5% | Test: 91.6% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | Loss: 0.0593 | Train: 98.3% | Test: 91.7% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | Loss: 0.0458 | Train: 98.7% | Test: 91.4% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | Loss: 0.0356 | Train: 98.9% | Test: 91.3% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | Loss: 0.0301 | Train: 99.0% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | Loss: 0.0243 | Train: 99.3% | Test: 91.4% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | Loss: 0.0234 | Train: 99.3% | Test: 91.5% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | Loss: 0.0233 | Train: 99.3% | Test: 91.2% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | Loss: 0.0236 | Train: 99.2% | Test: 91.0% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | Loss: 0.0276 | Train: 99.2% | Test: 91.1% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | Loss: 0.0234 | Train: 99.2% | Test: 90.7% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | Loss: 0.0250 | Train: 99.2% | Test: 91.0% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | Loss: 0.0254 | Train: 99.2% | Test: 90.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | Loss: 0.0279 | Train: 99.1% | Test: 90.8% | Time: 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | Loss: 0.0302 | Train: 99.1% | Test: 90.3% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | Loss: 0.0289 | Train: 99.1% | Test: 91.0% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | Loss: 0.0283 | Train: 99.1% | Test: 90.8% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | Loss: 0.0274 | Train: 99.2% | Test: 90.9% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | Loss: 0.0210 | Train: 99.4% | Test: 91.3% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | Loss: 0.0197 | Train: 99.4% | Test: 91.1% | Time: 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | Loss: 0.0185 | Train: 99.4% | Test: 91.2% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | Loss: 0.0132 | Train: 99.6% | Test: 91.2% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | Loss: 0.0126 | Train: 99.6% | Test: 90.9% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | Loss: 0.0119 | Train: 99.6% | Test: 91.3% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | Loss: 0.0081 | Train: 99.7% | Test: 91.4% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | Loss: 0.0081 | Train: 99.7% | Test: 91.0% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | Loss: 0.0085 | Train: 99.7% | Test: 91.3% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | Loss: 0.0076 | Train: 99.8% | Test: 91.2% | Time: 53.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | Loss: 0.0091 | Train: 99.7% | Test: 91.1% | Time: 54.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | Loss: 0.0050 | Train: 99.8% | Test: 91.2% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | Loss: 0.0059 | Train: 99.8% | Test: 91.3% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | Loss: 0.0050 | Train: 99.8% | Test: 91.0% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | Loss: 0.0049 | Train: 99.8% | Test: 90.8% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | Loss: 0.0037 | Train: 99.9% | Test: 91.1% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | Loss: 0.0030 | Train: 99.9% | Test: 91.3% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | Loss: 0.0037 | Train: 99.9% | Test: 91.1% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | Loss: 0.0030 | Train: 99.9% | Test: 91.2% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | Loss: 0.0025 | Train: 99.9% | Test: 91.2% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | Loss: 0.0026 | Train: 99.9% | Test: 91.2% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | Loss: 0.0032 | Train: 99.9% | Test: 91.3% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | Loss: 0.0023 | Train: 99.9% | Test: 91.4% | Time: 54.7s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 91.89%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  pickup_truck    : 100.0% ████████████████████\n",
            "  apple           :  99.0% ███████████████████\n",
            "  chimpanzee      :  99.0% ███████████████████\n",
            "  orange          :  99.0% ███████████████████\n",
            "  tractor         :  99.0% ███████████████████\n",
            "  aquarium_fish   :  98.0% ███████████████████\n",
            "  bed             :  98.0% ███████████████████\n",
            "  bicycle         :  98.0% ███████████████████\n",
            "  camel           :  98.0% ███████████████████\n",
            "  cattle          :  98.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  shrew           :  82.0% ████████████████\n",
            "  possum          :  80.0% ████████████████\n",
            "  cloud           :  79.0% ███████████████\n",
            "  willow_tree     :  77.0% ███████████████\n",
            "  bowl            :  76.0% ███████████████\n",
            "  forest          :  76.0% ███████████████\n",
            "  maple_tree      :  74.0% ██████████████\n",
            "  otter           :  72.0% ██████████████\n",
            "  oak_tree        :  68.0% █████████████\n",
            "  boy             :  65.0% █████████████\n",
            "\n",
            "  Average across 100 classes: 91.4%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 1.25\n",
            "  cantor_neg  : 2.13\n",
            "  beatrix_pos : 1.94\n",
            "  beatrix_neg : 2.08\n",
            "  helix_pos   : 2.83\n",
            "  helix_neg   : 2.32\n",
            "  simplex_pos : 2.34\n",
            "  simplex_neg : 2.33\n",
            "  convnext    : 6.09\n",
            "  vitl        : 8.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scheduled dual + mega transformer tower"
      ],
      "metadata": {
        "id": "ag-vfpYox8gq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 ConvNeXt-Small outputs for entire dataset.\"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 768  # ConvNeXt-Small embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ConvNeXt-S latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ConvNeXt-S cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ConvNeXt-S {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ConvNeXt uses pooler_output\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 768]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class DinoVitLCacher:\n",
        "    \"\"\"Precaches DINOv3 ViT-L outputs for entire dataset.\n",
        "\n",
        "    ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_vitl16_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ViT-L latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ViT-L cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ViT-L {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ViT uses pooler_output (CLS token projected)\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 1024]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINOv3 ConvNeXt-S and ViT-L latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, convnext_latents: Tensor, vitl_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.convnext_latents = convnext_latents\n",
        "        self.vitl_latents = vitl_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        convnext_latent = self.convnext_latents[idx]\n",
        "        vitl_latent = self.vitl_latents[idx]\n",
        "        return image, convnext_latent, vitl_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION WITH SCHEDULING\n",
        "# =============================================================================\n",
        "\n",
        "class ScheduledDinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with scheduled scale and dropout.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(dino_dim),\n",
        "            nn.Linear(dino_dim, out_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "        )\n",
        "        # Runtime parameters set by scheduler\n",
        "        self.current_scale = 1.0\n",
        "        self.current_dropout = 0.0\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        \"\"\"Called by trainer each epoch.\"\"\"\n",
        "        self.current_scale = scale\n",
        "        self.current_dropout = dropout\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x)\n",
        "        # Apply scheduled dropout\n",
        "        if self.training and self.current_dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1.0 - self.current_dropout))\n",
        "            x = x * mask / (1.0 - self.current_dropout + 1e-8)\n",
        "        return x * self.current_scale\n",
        "\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Curriculum: Rapidly introduce experts, then fade them out.\n",
        "\n",
        "    Phase 1 (warmup): scale 0→1 over warmup_epochs\n",
        "    Phase 2 (full): scale=1, dropout=0\n",
        "    Phase 3 (fadeout): dropout 0→1 over fadeout_epochs\n",
        "\n",
        "    This forces towers to learn WITH experts, then WITHOUT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        total_epochs: int,\n",
        "        warmup_epochs: int = 3,      # Rapid introduction\n",
        "        plateau_epochs: int = 10,    # Full power plateau\n",
        "        fadeout_epochs: int = 37,    # Gradual fadeout\n",
        "    ):\n",
        "        self.total_epochs = total_epochs\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.plateau_epochs = plateau_epochs\n",
        "        self.fadeout_epochs = fadeout_epochs\n",
        "\n",
        "        # Phases\n",
        "        self.warmup_end = warmup_epochs\n",
        "        self.plateau_end = warmup_epochs + plateau_epochs\n",
        "        # Remaining epochs are fadeout\n",
        "\n",
        "    def get_schedule(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout) for current epoch.\"\"\"\n",
        "        if epoch < self.warmup_end:\n",
        "            # Phase 1: Rapid warmup\n",
        "            progress = (epoch + 1) / self.warmup_epochs\n",
        "            scale = progress  # 0 → 1\n",
        "            dropout = 0.0\n",
        "        elif epoch < self.plateau_end:\n",
        "            # Phase 2: Full power plateau\n",
        "            scale = 1.0\n",
        "            dropout = 0.0\n",
        "        else:\n",
        "            # Phase 3: Fadeout via dropout\n",
        "            fadeout_progress = (epoch - self.plateau_end) / self.fadeout_epochs\n",
        "            fadeout_progress = min(1.0, fadeout_progress)  # Cap at 1.0\n",
        "            scale = 1.0\n",
        "            dropout = fadeout_progress  # 0 → 1\n",
        "\n",
        "        return scale, dropout\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"ExpertScheduler(warmup={self.warmup_epochs}, \"\n",
        "                f\"plateau={self.plateau_epochs}, fadeout={self.fadeout_epochs})\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder + dual DINOv3 experts.\n",
        "\n",
        "    Two DINOv3 experts provide complementary signals:\n",
        "    - ConvNeXt-Small (768d): Dense spatial features, efficient\n",
        "    - ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion (256)\n",
        "                              ↓\n",
        "        DINOv3 ConvNeXt-S (768) → ConvNextProj → ConvNeXt Opinion (256)\n",
        "        DINOv3 ViT-L (1024) → ViTLProj → ViT-L Opinion (256)\n",
        "                              ↓\n",
        "        [Collective + ConvNeXt + ViT-L] (768) → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        convnext_dim: int = 768,   # ConvNeXt-Small\n",
        "        vitl_dim: int = 1024,      # ViT-L\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ConvNeXt-Small projection (768d → 256d)\n",
        "        self.convnext_proj = ScheduledDinoProjection(\n",
        "            dino_dim=convnext_dim,\n",
        "            out_dim=dim,\n",
        "        )\n",
        "\n",
        "        # DINOv3 ViT-L projection (1024d → 256d)\n",
        "        self.vitl_proj = ScheduledDinoProjection(\n",
        "            dino_dim=vitl_dim,\n",
        "            out_dim=dim,\n",
        "        )\n",
        "\n",
        "        # Classifier: [collective_fused + convnext + vitl] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),  # 256 * 3 = 768\n",
        "            nn.Linear(dim * 3, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['convnext', 'vitl']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        \"\"\"Update expert projections with current schedule.\"\"\"\n",
        "        self.convnext_proj.set_schedule(scale, dropout)\n",
        "        self.vitl_proj.set_schedule(scale, dropout)\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # DINOv3 projections\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # DINOv3 experts\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "        opinions['convnext'] = convnext_opinion\n",
        "        opinions['vitl'] = vitl_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0, scheduler: Optional[ExpertScheduler] = None):\n",
        "    model.train()\n",
        "\n",
        "    # Apply schedule for this epoch\n",
        "    if scheduler:\n",
        "        scale, dropout = scheduler.get_schedule(epoch)\n",
        "        model.set_expert_schedule(scale, dropout)\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, convnext_latents, vitl_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, convnext_latents, vitl_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, convnext_latents, vitl_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, convnext_latents, vitl_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100: Geometric Towers + Dual DINOv3 [CURRICULUM]\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DINOv3 ConvNeXt-S (768d): Dense spatial features\")\n",
        "    print(\"DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\")\n",
        "    print(\"CURRICULUM: Introduce → Full Power → Fadeout (towers inherit)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # DINOv3 expert config\n",
        "    CONVNEXT_DIM = 768   # ConvNeXt-Small\n",
        "    VITL_DIM = 1024      # ViT-L\n",
        "\n",
        "    # Expert curriculum schedule\n",
        "    WARMUP_EPOCHS = 3     # Rapid introduction (scale 0→1)\n",
        "    PLATEAU_EPOCHS = 12   # Full power\n",
        "    FADEOUT_EPOCHS = 35   # Gradual dropout (0→1)\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"ConvNeXt-S: dim={CONVNEXT_DIM}\")\n",
        "    print(f\"ViT-L: dim={VITL_DIM}\")\n",
        "    print(f\"Schedule: warmup={WARMUP_EPOCHS}, plateau={PLATEAU_EPOCHS}, fadeout={FADEOUT_EPOCHS}\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINOv3 ConvNeXt-Small\n",
        "    convnext_cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_convnext = convnext_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_convnext = convnext_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Cache DINOv3 ViT-L\n",
        "    vitl_cacher = DinoVitLCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_vitl = vitl_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_vitl = vitl_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_convnext, train_vitl)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_convnext, test_vitl)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers + Dual DINOv3 Experts...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        convnext_dim=CONVNEXT_DIM,\n",
        "        vitl_dim=VITL_DIM,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Expert heads: ConvNeXt-S (768→{DIM}), ViT-L (1024→{DIM})\")\n",
        "    print(f\"Classifier input: {DIM * 3} (collective + convnext + vitl)\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Expert Scheduler - curriculum learning\n",
        "    expert_scheduler = ExpertScheduler(\n",
        "        total_epochs=EPOCHS,\n",
        "        warmup_epochs=WARMUP_EPOCHS,\n",
        "        plateau_epochs=PLATEAU_EPOCHS,\n",
        "        fadeout_epochs=FADEOUT_EPOCHS,\n",
        "    )\n",
        "    print(f\"\\nExpert Scheduler: {expert_scheduler}\")\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training with Expert Curriculum\")\n",
        "    print(\"Phase 1: Warmup (scale 0→1)\")\n",
        "    print(\"Phase 2: Plateau (full power)\")\n",
        "    print(\"Phase 3: Fadeout (dropout 0→1, towers inherit)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        # Get current schedule\n",
        "        scale, dropout = expert_scheduler.get_schedule(epoch)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch, expert_scheduler)\n",
        "\n",
        "        # Set to full scale for eval (no dropout)\n",
        "        model.set_expert_schedule(scale=1.0, dropout=0.0)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        # Phase indicator\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            phase = f\"WARMUP s={scale:.2f}\"\n",
        "        elif epoch < WARMUP_EPOCHS + PLATEAU_EPOCHS:\n",
        "            phase = \"PLATEAU\"\n",
        "        else:\n",
        "            phase = f\"FADE d={dropout:.2f}\"\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"{phase:14s} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUb2dPnsx-cY",
        "outputId": "10657efe-e9e4-4d96-aac9-3a81934ff63a"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "CIFAR-100: Geometric Towers + Dual DINOv3 [CURRICULUM]\n",
            "================================================================================\n",
            "DINOv3 ConvNeXt-S (768d): Dense spatial features\n",
            "DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
            "CURRICULUM: Introduce → Full Power → Fadeout (towers inherit)\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "ConvNeXt-S: dim=768\n",
            "ViT-L: dim=1024\n",
            "Schedule: warmup=3, plateau=12, fadeout=35\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "\n",
            "Building model with ConfigurableTowers + Dual DINOv3 Experts...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Expert heads: ConvNeXt-S (768→256), ViT-L (1024→256)\n",
            "Classifier input: 768 (collective + convnext + vitl)\n",
            "Patches: 256\n",
            "Total parameters: 7,919,569\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "Expert Scheduler: ExpertScheduler(warmup=3, plateau=12, fadeout=35)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training with Expert Curriculum\n",
            "Phase 1: Warmup (scale 0→1)\n",
            "Phase 2: Plateau (full power)\n",
            "Phase 3: Fadeout (dropout 0→1, towers inherit)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            ""
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1/50 | WARMUP s=0.33  | Loss: 4.4050 | Train: 8.2% | Test: 40.1% * | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | WARMUP s=0.67  | Loss: 2.8980 | Train: 63.0% | Test: 83.4% * | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | WARMUP s=1.00  | Loss: 0.8621 | Train: 85.9% | Test: 88.6% * | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | PLATEAU        | Loss: 0.4071 | Train: 89.9% | Test: 90.5% * | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | PLATEAU        | Loss: 0.2946 | Train: 91.9% | Test: 91.1% * | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | PLATEAU        | Loss: 0.2369 | Train: 93.3% | Test: 91.6% * | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | PLATEAU        | Loss: 0.1916 | Train: 94.4% | Test: 91.8% * | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | PLATEAU        | Loss: 0.1508 | Train: 95.6% | Test: 91.9% * | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | PLATEAU        | Loss: 0.1162 | Train: 96.6% | Test: 91.7% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | PLATEAU        | Loss: 0.0866 | Train: 97.4% | Test: 91.6% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | PLATEAU        | Loss: 0.0620 | Train: 98.2% | Test: 91.5% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | PLATEAU        | Loss: 0.0447 | Train: 98.7% | Test: 91.5% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | PLATEAU        | Loss: 0.0359 | Train: 98.9% | Test: 91.2% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | PLATEAU        | Loss: 0.0312 | Train: 99.0% | Test: 91.2% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | PLATEAU        | Loss: 0.0246 | Train: 99.3% | Test: 91.1% | Time: 53.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | FADE d=0.00    | Loss: 0.0230 | Train: 99.3% | Test: 91.2% | Time: 54.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | FADE d=0.03    | Loss: 0.0239 | Train: 99.3% | Test: 91.2% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | FADE d=0.06    | Loss: 0.0255 | Train: 99.2% | Test: 90.9% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | FADE d=0.09    | Loss: 0.0287 | Train: 99.1% | Test: 90.7% | Time: 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | FADE d=0.11    | Loss: 0.0300 | Train: 99.1% | Test: 91.5% | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | FADE d=0.14    | Loss: 0.0302 | Train: 99.0% | Test: 91.3% | Time: 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | FADE d=0.17    | Loss: 0.0332 | Train: 98.9% | Test: 90.9% | Time: 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | FADE d=0.20    | Loss: 0.0352 | Train: 98.9% | Test: 90.9% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | FADE d=0.23    | Loss: 0.0388 | Train: 98.7% | Test: 91.2% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | FADE d=0.26    | Loss: 0.0376 | Train: 98.8% | Test: 91.2% | Time: 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | FADE d=0.29    | Loss: 0.0388 | Train: 98.8% | Test: 90.9% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | FADE d=0.31    | Loss: 0.0373 | Train: 98.8% | Test: 91.4% | Time: 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | FADE d=0.34    | Loss: 0.0343 | Train: 98.9% | Test: 91.5% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | FADE d=0.37    | Loss: 0.0318 | Train: 98.9% | Test: 91.4% | Time: 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | FADE d=0.40    | Loss: 0.0331 | Train: 98.9% | Test: 91.6% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | FADE d=0.43    | Loss: 0.0295 | Train: 99.0% | Test: 91.4% | Time: 55.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | FADE d=0.46    | Loss: 0.0328 | Train: 99.0% | Test: 91.4% | Time: 54.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | FADE d=0.49    | Loss: 0.0288 | Train: 99.1% | Test: 91.4% | Time: 55.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | FADE d=0.51    | Loss: 0.0314 | Train: 99.0% | Test: 91.4% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | FADE d=0.54    | Loss: 0.0295 | Train: 99.1% | Test: 91.3% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | FADE d=0.57    | Loss: 0.0324 | Train: 99.0% | Test: 91.7% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | FADE d=0.60    | Loss: 0.0333 | Train: 99.0% | Test: 92.0% * | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | FADE d=0.63    | Loss: 0.0389 | Train: 98.7% | Test: 91.9% | Time: 54.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | FADE d=0.66    | Loss: 0.0409 | Train: 98.7% | Test: 92.0% * | Time: 54.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | FADE d=0.69    | Loss: 0.0397 | Train: 98.7% | Test: 91.7% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | FADE d=0.71    | Loss: 0.0446 | Train: 98.6% | Test: 92.0% | Time: 54.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | FADE d=0.74    | Loss: 0.0524 | Train: 98.3% | Test: 92.0% | Time: 54.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | FADE d=0.77    | Loss: 0.0563 | Train: 98.3% | Test: 92.3% * | Time: 54.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | FADE d=0.80    | Loss: 0.0700 | Train: 97.9% | Test: 92.2% | Time: 54.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | FADE d=0.83    | Loss: 0.0790 | Train: 97.6% | Test: 92.2% | Time: 54.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | FADE d=0.86    | Loss: 0.1066 | Train: 96.7% | Test: 92.2% | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | FADE d=0.89    | Loss: 0.1417 | Train: 95.7% | Test: 92.3% * | Time: 55.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | FADE d=0.91    | Loss: 0.2187 | Train: 93.6% | Test: 92.3% | Time: 54.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | FADE d=0.94    | Loss: 0.4063 | Train: 87.9% | Test: 92.6% * | Time: 54.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | FADE d=0.97    | Loss: 1.0575 | Train: 69.6% | Test: 92.0% | Time: 54.9s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 92.64%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  orange          : 100.0% ████████████████████\n",
            "  pickup_truck    : 100.0% ████████████████████\n",
            "  apple           :  99.0% ███████████████████\n",
            "  bicycle         :  99.0% ███████████████████\n",
            "  chimpanzee      :  99.0% ███████████████████\n",
            "  lawn_mower      :  99.0% ███████████████████\n",
            "  orchid          :  99.0% ███████████████████\n",
            "  tractor         :  99.0% ███████████████████\n",
            "  aquarium_fish   :  98.0% ███████████████████\n",
            "  bed             :  98.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  girl            :  81.0% ████████████████\n",
            "  forest          :  80.0% ████████████████\n",
            "  bowl            :  78.0% ███████████████\n",
            "  maple_tree      :  78.0% ███████████████\n",
            "  oak_tree        :  78.0% ███████████████\n",
            "  otter           :  78.0% ███████████████\n",
            "  shrew           :  75.0% ███████████████\n",
            "  willow_tree     :  74.0% ██████████████\n",
            "  possum          :  73.0% ██████████████\n",
            "  boy             :  71.0% ██████████████\n",
            "\n",
            "  Average across 100 classes: 92.0%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 67.04\n",
            "  cantor_neg  : 51.49\n",
            "  beatrix_pos : 117.25\n",
            "  beatrix_neg : 53.46\n",
            "  helix_pos   : 17.18\n",
            "  helix_neg   : 15.55\n",
            "  simplex_pos : 29.30\n",
            "  simplex_neg : 63.17\n",
            "  convnext    : 13.59\n",
            "  vitl        : 13.56\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# transformer tower - the big gun, bigG, vit-l dino, convnext s dino, collective"
      ],
      "metadata": {
        "id": "4I6ZJ5WvDmMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/mlfoundations/open_clip.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oMgqaK_D0KL",
        "outputId": "d21f48b9-d954-471c-8c0a-b8fed4b921c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/mlfoundations/open_clip.git\n",
            "  Cloning https://github.com/mlfoundations/open_clip.git to /tmp/pip-req-build-tbaqowdt\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/mlfoundations/open_clip.git /tmp/pip-req-build-tbaqowdt\n",
            "  Resolved https://github.com/mlfoundations/open_clip.git to commit d3cdb734a2710feeb4c6307df037afa5f786a3e1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.24.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (2025.11.3)\n",
            "Collecting ftfy (from open_clip_torch==3.2.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (0.7.0)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch==3.2.0) (1.0.22)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch==3.2.0) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch==3.2.0) (3.5.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch==3.2.0) (0.2.14)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch==3.2.0) (1.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch==3.2.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch==3.2.0) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch==3.2.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch==3.2.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch==3.2.0) (2025.11.12)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: open_clip_torch\n",
            "  Building wheel for open_clip_torch (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for open_clip_torch: filename=open_clip_torch-3.2.0-py3-none-any.whl size=1546957 sha256=2e87d13c7c6c3c41e80e3d45a7f371d8fdfc0cf767118f6307388053c1cb382d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-05r5q2t6/wheels/b4/c2/33/0e591fedc30bccff3cead0e2b38fd27aab49a1e543aa9a4360\n",
            "Successfully built open_clip_torch\n",
            "Installing collected packages: ftfy, open_clip_torch\n",
            "Successfully installed ftfy-6.3.1 open_clip_torch-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Classifier using GeometricTowers from Codebase\n",
        "=========================================================\n",
        "\n",
        "Uses the actual AgathaTowerCollective + DINOv3 cached latents.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Import the tower builder\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    ConfigurableCollective,\n",
        "    ConfigurableTower,\n",
        "    CollectiveOpinion,\n",
        "    TowerOpinion,\n",
        "    RoPEType,\n",
        "    AddressType,\n",
        "    FusionType,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        "    preset_all_six,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING (same as before)\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Precaches DINOv3 ConvNeXt-Small outputs for entire dataset.\"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 768  # ConvNeXt-Small embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_convnext_small_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ConvNeXt-S latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ConvNeXt-S cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ConvNeXt-S {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ConvNeXt uses pooler_output\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 768]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class DinoVitLCacher:\n",
        "    \"\"\"Precaches DINOv3 ViT-L outputs for entire dataset.\n",
        "\n",
        "    ViT-L is distilled from the 7B teacher using multi-distillation\n",
        "    which includes CLIP-L knowledge.\n",
        "    \"\"\"\n",
        "\n",
        "    DINO_MODEL = \"facebook/dinov3-vitl16-pretrain-lvd1689m\"\n",
        "    DINO_DIM = 1024  # ViT-L embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.dino_dim = self.DINO_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dinov3_vitl16_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached DINOv3 ViT-L latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building DINOv3 ViT-L cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "        processor = AutoImageProcessor.from_pretrained(self.DINO_MODEL)\n",
        "        backbone = AutoModel.from_pretrained(self.DINO_MODEL).to(self.device)\n",
        "        backbone.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching ViT-L {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            outputs = backbone(pixel_values=images)\n",
        "            # ViT uses pooler_output (CLS token projected)\n",
        "            latents = outputs.pooler_output.cpu()  # [B, 1024]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del backbone\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class ClipGCacher:\n",
        "    \"\"\"Precaches CLIP ViT-G/14 outputs for entire dataset.\n",
        "\n",
        "    LAION's ViT-G/14 - 1.8B params, 1280d embeddings.\n",
        "    Pure CLIP semantic alignment at massive scale.\n",
        "    \"\"\"\n",
        "\n",
        "    CLIP_MODEL = \"laion/CLIP-ViT-bigG-14-laion2B-s39B-b160K\"\n",
        "    CLIP_DIM = 1280  # ViT-bigG embedding dimension\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "        self.clip_dim = self.CLIP_DIM\n",
        "\n",
        "    def _get_cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"clip_vitg14_cifar100_{split}.pt\"\n",
        "\n",
        "    def cache_exists(self, split: str) -> bool:\n",
        "        return self._get_cache_path(split).exists()\n",
        "\n",
        "    def load_cache(self, split: str) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "        print(f\"Loading cached CLIP ViT-G latents from {path}\")\n",
        "        return torch.load(path, weights_only=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 32) -> Tensor:\n",
        "        path = self._get_cache_path(split)\n",
        "\n",
        "        if path.exists():\n",
        "            print(f\"Cache exists at {path}\")\n",
        "            return self.load_cache(split)\n",
        "\n",
        "        print(f\"Building CLIP ViT-G cache for {split} ({len(dataset)} images)...\")\n",
        "\n",
        "        import open_clip\n",
        "\n",
        "        model, _, preprocess = open_clip.create_model_and_transforms(\n",
        "            'ViT-bigG-14', pretrained='laion2b_s39b_b160k'\n",
        "        )\n",
        "        model = model.to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching CLIP-G {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            # CLIP-G expects 224x224\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, size=(224, 224), mode='bilinear', align_corners=False)\n",
        "            # Get image features (already normalized by open_clip)\n",
        "            latents = model.encode_image(images).cpu()  # [B, 1280]\n",
        "            all_latents.append(latents)\n",
        "\n",
        "        all_latents = torch.cat(all_latents, dim=0)\n",
        "        torch.save(all_latents, path)\n",
        "        print(f\"Saved cache to {path} - shape: {all_latents.shape}\")\n",
        "\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return all_latents\n",
        "\n",
        "\n",
        "class CachedCIFARDataset(Dataset):\n",
        "    \"\"\"CIFAR dataset with precached DINOv3 ConvNeXt-S, ViT-L, and CLIP-G latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base_dataset: Dataset, convnext_latents: Tensor, vitl_latents: Tensor, clipg_latents: Tensor):\n",
        "        self.base_dataset = base_dataset\n",
        "        self.convnext_latents = convnext_latents\n",
        "        self.vitl_latents = vitl_latents\n",
        "        self.clipg_latents = clipg_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image, label = self.base_dataset[idx]\n",
        "        convnext_latent = self.convnext_latents[idx]\n",
        "        vitl_latent = self.vitl_latents[idx]\n",
        "        clipg_latent = self.clipg_latents[idx]\n",
        "        return image, convnext_latent, vitl_latent, clipg_latent, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO PROJECTION WITH SCHEDULING\n",
        "# =============================================================================\n",
        "\n",
        "class ScheduledDinoProjection(nn.Module):\n",
        "    \"\"\"Projects cached DINO latents with scheduled scale and dropout.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dino_dim: int = 768,\n",
        "        out_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(dino_dim),\n",
        "            nn.Linear(dino_dim, out_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim, out_dim),\n",
        "        )\n",
        "        # Runtime parameters set by scheduler\n",
        "        self.current_scale = 1.0\n",
        "        self.current_dropout = 0.0\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        \"\"\"Called by trainer each epoch.\"\"\"\n",
        "        self.current_scale = scale\n",
        "        self.current_dropout = dropout\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x)\n",
        "        # Apply scheduled dropout\n",
        "        if self.training and self.current_dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1.0 - self.current_dropout))\n",
        "            x = x * mask / (1.0 - self.current_dropout + 1e-8)\n",
        "        return x * self.current_scale\n",
        "\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Curriculum: Rapidly introduce experts, then fade them out.\n",
        "\n",
        "    Phase 1 (warmup): scale 0→1 over warmup_epochs\n",
        "    Phase 2 (full): scale=1, dropout=0\n",
        "    Phase 3 (fadeout): dropout 0→1 over fadeout_epochs\n",
        "\n",
        "    This forces towers to learn WITH experts, then WITHOUT.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        total_epochs: int,\n",
        "        warmup_epochs: int = 3,      # Rapid introduction\n",
        "        plateau_epochs: int = 10,    # Full power plateau\n",
        "        fadeout_epochs: int = 37,    # Gradual fadeout\n",
        "    ):\n",
        "        self.total_epochs = total_epochs\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.plateau_epochs = plateau_epochs\n",
        "        self.fadeout_epochs = fadeout_epochs\n",
        "\n",
        "        # Phases\n",
        "        self.warmup_end = warmup_epochs\n",
        "        self.plateau_end = warmup_epochs + plateau_epochs\n",
        "        # Remaining epochs are fadeout\n",
        "\n",
        "    def get_schedule(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout) for current epoch.\"\"\"\n",
        "        if epoch < self.warmup_end:\n",
        "            # Phase 1: Rapid warmup\n",
        "            progress = (epoch + 1) / self.warmup_epochs\n",
        "            scale = progress  # 0 → 1\n",
        "            dropout = 0.0\n",
        "        elif epoch < self.plateau_end:\n",
        "            # Phase 2: Full power plateau\n",
        "            scale = 1.0\n",
        "            dropout = 0.0\n",
        "        else:\n",
        "            # Phase 3: Fadeout via dropout\n",
        "            fadeout_progress = (epoch - self.plateau_end) / self.fadeout_epochs\n",
        "            fadeout_progress = min(1.0, fadeout_progress)  # Cap at 1.0\n",
        "            scale = 1.0\n",
        "            dropout = fadeout_progress  # 0 → 1\n",
        "\n",
        "        return scale, dropout\n",
        "\n",
        "    def __repr__(self):\n",
        "        return (f\"ExpertScheduler(warmup={self.warmup_epochs}, \"\n",
        "                f\"plateau={self.plateau_epochs}, fadeout={self.fadeout_epochs})\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR CLASSIFIER WITH GEOMETRIC TOWERS\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARGeometricClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    CIFAR classifier using ConfigurableTower builder + triple expert ensemble.\n",
        "\n",
        "    Three experts provide complementary signals:\n",
        "    - DINOv3 ConvNeXt-Small (768d): Dense spatial features\n",
        "    - DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
        "    - CLIP ViT-G (1280d): Pure CLIP semantic alignment at massive scale\n",
        "\n",
        "    Architecture:\n",
        "        Image → PatchEmbed → ConfigurableCollective → Fused Opinion (256)\n",
        "                              ↓\n",
        "        DINOv3 ConvNeXt-S (768) → Proj → Opinion (256)\n",
        "        DINOv3 ViT-L (1024) → Proj → Opinion (256)\n",
        "        CLIP ViT-G (1280) → Proj → Opinion (256)\n",
        "                              ↓\n",
        "        [Collective + ConvNeXt + ViT-L + CLIP-G] (1024) → Classifier → Logits\n",
        "    \"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = [\n",
        "        'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "        'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "        'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "        'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "        'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "        'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "        'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "        'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "        'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "        'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose',\n",
        "        'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake',\n",
        "        'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table',\n",
        "        'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout',\n",
        "        'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "    ]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        tower_depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        fingerprint_dim: int = 64,\n",
        "        convnext_dim: int = 768,   # ConvNeXt-Small\n",
        "        vitl_dim: int = 1024,      # ViT-L\n",
        "        clipg_dim: int = 1280,     # CLIP ViT-G\n",
        "        tower_configs: Optional[List[TowerConfig]] = None,\n",
        "        fusion_type: str = 'adaptive',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        num_patches = (32 // patch_size) ** 2\n",
        "        self.patch_embed = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, num_patches, dim) * 0.02)\n",
        "\n",
        "        # Default tower configs: pos/neg pairs for 4 geometries\n",
        "        if tower_configs is None:\n",
        "            tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # Build tower collective\n",
        "        self.collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            ffn_mult=4.0,\n",
        "            dropout=0.1,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type=fusion_type,\n",
        "        )\n",
        "\n",
        "        # Expert projections (all scheduled)\n",
        "        self.convnext_proj = ScheduledDinoProjection(dino_dim=convnext_dim, out_dim=dim)\n",
        "        self.vitl_proj = ScheduledDinoProjection(dino_dim=vitl_dim, out_dim=dim)\n",
        "        self.clipg_proj = ScheduledDinoProjection(dino_dim=clipg_dim, out_dim=dim)\n",
        "\n",
        "        # Classifier: [collective + convnext + vitl + clipg] → logits\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 4),  # 256 * 4 = 1024\n",
        "            nn.Linear(dim * 4, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "        self.num_patches = num_patches\n",
        "        self._tower_names = self.collective.tower_names + ['convnext', 'vitl', 'clipg']\n",
        "        self._tower_configs = tower_configs\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        \"\"\"Update all expert projections with current schedule.\"\"\"\n",
        "        self.convnext_proj.set_schedule(scale, dropout)\n",
        "        self.vitl_proj.set_schedule(scale, dropout)\n",
        "        self.clipg_proj.set_schedule(scale, dropout)\n",
        "\n",
        "    @property\n",
        "    def tower_names(self) -> List[str]:\n",
        "        return self._tower_names\n",
        "\n",
        "    def forward(self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor, clipg_latents: Tensor) -> Tensor:\n",
        "        \"\"\"Forward pass returning logits.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Expert projections\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "        clipg_opinion = self.clipg_proj(clipg_latents)\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion, clipg_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def forward_with_opinions(\n",
        "        self, images: Tensor, convnext_latents: Tensor, vitl_latents: Tensor, clipg_latents: Tensor\n",
        "    ) -> Tuple[Tensor, Dict[str, Tensor]]:\n",
        "        \"\"\"Forward returning logits and all opinions.\"\"\"\n",
        "        # Patch embed\n",
        "        x = self.patch_embed(images)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        # Tower collective\n",
        "        collective_out: CollectiveOpinion = self.collective(x)\n",
        "\n",
        "        # Extract individual tower opinions\n",
        "        opinions = {}\n",
        "        for name, tower_op in collective_out.opinions.items():\n",
        "            opinions[name] = tower_op.opinion\n",
        "        opinions['collective_fused'] = collective_out.fused\n",
        "        opinions['collective_weights'] = collective_out.weights\n",
        "\n",
        "        # Expert opinions\n",
        "        convnext_opinion = self.convnext_proj(convnext_latents)\n",
        "        vitl_opinion = self.vitl_proj(vitl_latents)\n",
        "        clipg_opinion = self.clipg_proj(clipg_latents)\n",
        "        opinions['convnext'] = convnext_opinion\n",
        "        opinions['vitl'] = vitl_opinion\n",
        "        opinions['clipg'] = clipg_opinion\n",
        "\n",
        "        # Combine and classify\n",
        "        combined = torch.cat([collective_out.fused, convnext_opinion, vitl_opinion, clipg_opinion], dim=-1)\n",
        "        logits = self.classifier(combined)\n",
        "\n",
        "        return logits, opinions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ANALYZER\n",
        "# =============================================================================\n",
        "\n",
        "class ClassAnalyzer:\n",
        "    \"\"\"Tracks per-class performance.\"\"\"\n",
        "\n",
        "    CIFAR_CLASSES = CIFARGeometricClassifier.CIFAR_CLASSES\n",
        "\n",
        "    def __init__(self, tower_names: List[str]):\n",
        "        self.tower_names = tower_names\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.tower_class_norms = {name: defaultdict(list) for name in self.tower_names}\n",
        "        self.class_correct = defaultdict(int)\n",
        "        self.class_total = defaultdict(int)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update(self, opinions: Dict[str, Tensor], logits: Tensor, labels: Tensor):\n",
        "        _, predicted = logits.max(1)\n",
        "\n",
        "        for i, (pred, label) in enumerate(zip(predicted, labels)):\n",
        "            label_idx = label.item()\n",
        "            self.class_total[label_idx] += 1\n",
        "            if pred == label:\n",
        "                self.class_correct[label_idx] += 1\n",
        "\n",
        "            for name in self.tower_names:\n",
        "                if name in opinions and name not in ['collective_fused', 'collective_weights']:\n",
        "                    norm = opinions[name][i].norm().item()\n",
        "                    self.tower_class_norms[name][label_idx].append(norm)\n",
        "\n",
        "    def get_class_accuracy(self) -> Dict[str, float]:\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "        return {\n",
        "            self.CIFAR_CLASSES[c]: 100. * self.class_correct[c] / max(1, self.class_total[c])\n",
        "            for c in range(num_classes)\n",
        "        }\n",
        "\n",
        "    def print_report(self):\n",
        "        num_classes = len(self.CIFAR_CLASSES)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"CLASS-SPECIFIC ANALYSIS\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        class_acc = self.get_class_accuracy()\n",
        "        sorted_acc = sorted(class_acc.items(), key=lambda x: -x[1])\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Top 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[:10]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        print(\"\\nPer-Class Accuracy (Bottom 10):\")\n",
        "        print(\"-\" * 50)\n",
        "        for cls, acc in sorted_acc[-10:]:\n",
        "            bar = \"█\" * int(acc / 5)\n",
        "            print(f\"  {cls:16s}: {acc:5.1f}% {bar}\")\n",
        "\n",
        "        avg_acc = sum(class_acc.values()) / num_classes\n",
        "        print(f\"\\n  Average across {num_classes} classes: {avg_acc:.1f}%\")\n",
        "\n",
        "        # Tower norms\n",
        "        print(\"\\nTower Opinion Norms (avg across all classes):\")\n",
        "        print(\"-\" * 50)\n",
        "        for name in self.tower_names:\n",
        "            if name in ['collective_fused', 'collective_weights']:\n",
        "                continue\n",
        "            all_norms = []\n",
        "            for c in range(num_classes):\n",
        "                all_norms.extend(self.tower_class_norms[name][c])\n",
        "            avg_norm = sum(all_norms) / len(all_norms) if all_norms else 0\n",
        "            print(f\"  {name:12s}: {avg_norm:.2f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(model, loader, optimizer, device, epoch=0, scheduler: Optional[ExpertScheduler] = None):\n",
        "    model.train()\n",
        "\n",
        "    # Apply schedule for this epoch\n",
        "    if scheduler:\n",
        "        scale, dropout = scheduler.get_schedule(epoch)\n",
        "        model.set_expert_schedule(scale, dropout)\n",
        "\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "    for images, convnext_latents, vitl_latents, clipg_latents, labels in pbar:\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        clipg_latents = clipg_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(images, convnext_latents, vitl_latents, clipg_latents)\n",
        "        loss = F.cross_entropy(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * images.size(0)\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': f'{total_loss/total:.3f}', 'acc': f'{100.*correct/total:.1f}%'})\n",
        "\n",
        "    return total_loss / total, 100. * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_with_analysis(model, loader, device, analyzer: ClassAnalyzer):\n",
        "    model.eval()\n",
        "    analyzer.reset()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, convnext_latents, vitl_latents, clipg_latents, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images = images.to(device)\n",
        "        convnext_latents = convnext_latents.to(device)\n",
        "        vitl_latents = vitl_latents.to(device)\n",
        "        clipg_latents = clipg_latents.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        logits, opinions = model.forward_with_opinions(images, convnext_latents, vitl_latents, clipg_latents)\n",
        "\n",
        "        _, predicted = logits.max(1)\n",
        "        correct += predicted.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        analyzer.update(opinions, logits, labels)\n",
        "\n",
        "    return 100. * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100: Geometric Towers + Triple Expert Ensemble [CURRICULUM]\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"DINOv3 ConvNeXt-S (768d): Dense spatial features\")\n",
        "    print(\"DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\")\n",
        "    print(\"CLIP ViT-G (1280d): Pure CLIP semantic alignment - THE BIG ONE\")\n",
        "    print(\"CURRICULUM: Introduce → Full Power → Fadeout (towers inherit)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH_SIZE = 128\n",
        "    EPOCHS = 50\n",
        "    DIM = 256\n",
        "    TOWER_DEPTH = 1\n",
        "    NUM_HEADS = 4\n",
        "    PATCH_SIZE = 2\n",
        "    CACHE_DIR = \"./dino_cache\"\n",
        "    NUM_CLASSES = 100\n",
        "\n",
        "    # Expert dimensions\n",
        "    CONVNEXT_DIM = 768   # ConvNeXt-Small\n",
        "    VITL_DIM = 1024      # ViT-L\n",
        "    CLIPG_DIM = 1280     # CLIP ViT-G\n",
        "\n",
        "    # Expert curriculum schedule\n",
        "    WARMUP_EPOCHS = 3     # Rapid introduction (scale 0→1)\n",
        "    PLATEAU_EPOCHS = 12   # Full power\n",
        "    FADEOUT_EPOCHS = 35   # Gradual dropout (0→1)\n",
        "\n",
        "    # Tower configs - pos/neg pairs for each geometry\n",
        "    TOWER_CONFIGS = [\n",
        "        # Cantor pair\n",
        "        TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "        TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "        # Beatrix pair\n",
        "        TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "        TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        # Helix pair\n",
        "        TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "        TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        # Simplex pair\n",
        "        TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "        TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "    ]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Patch size: {PATCH_SIZE} → {(32//PATCH_SIZE)**2} patches\")\n",
        "    print(f\"Tower depth: {TOWER_DEPTH}\")\n",
        "    print(f\"Optimizer: Adafactor (relative_step=True, warmup_init=True)\")\n",
        "    print(f\"ConvNeXt-S: dim={CONVNEXT_DIM}\")\n",
        "    print(f\"ViT-L: dim={VITL_DIM}\")\n",
        "    print(f\"CLIP ViT-G: dim={CLIPG_DIM}\")\n",
        "    print(f\"Schedule: warmup={WARMUP_EPOCHS}, plateau={PLATEAU_EPOCHS}, fadeout={FADEOUT_EPOCHS}\")\n",
        "\n",
        "    print(f\"\\nTower Configs ({len(TOWER_CONFIGS)} towers):\")\n",
        "    for cfg in TOWER_CONFIGS:\n",
        "        print(f\"  {cfg.name}: rope={cfg.rope.value}, addr={cfg.address.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Transforms\n",
        "    transform_base = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    # Datasets\n",
        "    train_dataset_base = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_base)\n",
        "    test_dataset_base = datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_base)\n",
        "\n",
        "    print(f\"\\nTrain: {len(train_dataset_base)}, Test: {len(test_dataset_base)}\")\n",
        "\n",
        "    # Cache DINOv3 ConvNeXt-Small\n",
        "    convnext_cacher = DinoCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_convnext = convnext_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_convnext = convnext_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Cache DINOv3 ViT-L\n",
        "    vitl_cacher = DinoVitLCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_vitl = vitl_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_vitl = vitl_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Cache CLIP ViT-G (THE BIG ONE)\n",
        "    clipg_cacher = ClipGCacher(cache_dir=CACHE_DIR, device=device)\n",
        "    train_clipg = clipg_cacher.build_cache(train_dataset_base, split=\"train\")\n",
        "    test_clipg = clipg_cacher.build_cache(test_dataset_base, split=\"test\")\n",
        "\n",
        "    # Augmented training\n",
        "    train_dataset_aug = datasets.CIFAR100(root='./data', train=True, download=False, transform=transform_train)\n",
        "\n",
        "    train_dataset = CachedCIFARDataset(train_dataset_aug, train_convnext, train_vitl, train_clipg)\n",
        "    test_dataset = CachedCIFARDataset(test_dataset_base, test_convnext, test_vitl, test_clipg)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Model\n",
        "    print(\"\\nBuilding model with ConfigurableTowers + Triple Expert Ensemble...\")\n",
        "    model = CIFARGeometricClassifier(\n",
        "        dim=DIM,\n",
        "        tower_depth=TOWER_DEPTH,\n",
        "        num_heads=NUM_HEADS,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        num_classes=NUM_CLASSES,\n",
        "        convnext_dim=CONVNEXT_DIM,\n",
        "        vitl_dim=VITL_DIM,\n",
        "        clipg_dim=CLIPG_DIM,\n",
        "        tower_configs=TOWER_CONFIGS,\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nTowers: {model.collective.tower_names}\")\n",
        "    print(f\"Expert heads: ConvNeXt-S (768→{DIM}), ViT-L (1024→{DIM}), CLIP-G (1280→{DIM})\")\n",
        "    print(f\"Classifier input: {DIM * 4} (collective + convnext + vitl + clipg)\")\n",
        "    print(f\"Patches: {model.num_patches}\")\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "\n",
        "    for name in model.collective.tower_names:\n",
        "        tower = model.collective[name]\n",
        "        tower_params = sum(p.numel() for p in tower.parameters())\n",
        "        cfg = tower.config\n",
        "        print(f\"  {name}: {tower_params:,} params, rope={cfg.rope.value}, inv={cfg.inverted}\")\n",
        "\n",
        "    # Optimizer - Adafactor with internal scheduling (best for tower convergence)\n",
        "    from transformers import Adafactor\n",
        "\n",
        "    optimizer = Adafactor(\n",
        "        model.parameters(),\n",
        "        lr=None,                # Let Adafactor compute LR\n",
        "        scale_parameter=True,   # Scale by RMS of params\n",
        "        relative_step=True,     # LR = 1/sqrt(step) decay\n",
        "        warmup_init=True,       # Linear warmup from 0\n",
        "    )\n",
        "    # No external scheduler - Adafactor handles it internally\n",
        "\n",
        "    # Analyzer\n",
        "    analyzer = ClassAnalyzer(model.tower_names)\n",
        "\n",
        "    # Expert Scheduler - curriculum learning\n",
        "    expert_scheduler = ExpertScheduler(\n",
        "        total_epochs=EPOCHS,\n",
        "        warmup_epochs=WARMUP_EPOCHS,\n",
        "        plateau_epochs=PLATEAU_EPOCHS,\n",
        "        fadeout_epochs=FADEOUT_EPOCHS,\n",
        "    )\n",
        "    print(f\"\\nExpert Scheduler: {expert_scheduler}\")\n",
        "\n",
        "    # Training\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(\"Training with Expert Curriculum\")\n",
        "    print(\"Phase 1: Warmup (scale 0→1)\")\n",
        "    print(\"Phase 2: Plateau (full power)\")\n",
        "    print(\"Phase 3: Fadeout (dropout 0→1, towers inherit)\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_acc = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        start = time.time()\n",
        "\n",
        "        # Get current schedule\n",
        "        scale, dropout = expert_scheduler.get_schedule(epoch)\n",
        "\n",
        "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, device, epoch, expert_scheduler)\n",
        "\n",
        "        # Set to full scale for eval (no dropout)\n",
        "        model.set_expert_schedule(scale=1.0, dropout=0.0)\n",
        "        test_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "\n",
        "        marker = \" *\" if test_acc > best_acc else \"\"\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "\n",
        "        # Phase indicator\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            phase = f\"WARMUP s={scale:.2f}\"\n",
        "        elif epoch < WARMUP_EPOCHS + PLATEAU_EPOCHS:\n",
        "            phase = \"PLATEAU\"\n",
        "        else:\n",
        "            phase = f\"FADE d={dropout:.2f}\"\n",
        "\n",
        "        print(f\"Epoch {epoch+1:2d}/{EPOCHS} | \"\n",
        "              f\"{phase:14s} | \"\n",
        "              f\"Loss: {train_loss:.4f} | \"\n",
        "              f\"Train: {train_acc:.1f}% | \"\n",
        "              f\"Test: {test_acc:.1f}%{marker} | \"\n",
        "              f\"Time: {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Test Accuracy: {best_acc:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Final analysis\n",
        "    print(\"\\nFinal evaluation with class analysis...\")\n",
        "    final_acc = evaluate_with_analysis(model, test_loader, device, analyzer)\n",
        "    analyzer.print_report()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "51664daba73040b98318416a7e82417d",
            "9cea022eed2548ba9548571abe8a297b",
            "0b55f68bc128433386c93294f0c4e8c3",
            "c6ae3e99dca54409baa39504e9a50f50",
            "fc31644cee7345589989ef04318cbb93",
            "9b4cb8481010488e8ab07b24ccec1da8",
            "0976d89565594f7681576d74c5fb2f4d",
            "0464c71aa4354777a273407a25ec43fd",
            "301591f6f12b4e96831e8ad0eee98746",
            "1bc7d0bb45344c53bbe9877c68ea170e",
            "54f4cd5aa162474b91ad3393b25252ee"
          ]
        },
        "id": "LHXeXoDpDu1U",
        "outputId": "099033f3-8ff6-47aa-e3d0-fa774e8a7c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100: Geometric Towers + Triple Expert Ensemble [CURRICULUM]\n",
            "================================================================================\n",
            "DINOv3 ConvNeXt-S (768d): Dense spatial features\n",
            "DINOv3 ViT-L (1024d): Distilled from 7B with CLIP-L knowledge\n",
            "CLIP ViT-G (1280d): Pure CLIP semantic alignment - THE BIG ONE\n",
            "CURRICULUM: Introduce → Full Power → Fadeout (towers inherit)\n",
            "================================================================================\n",
            "Device: cuda\n",
            "Patch size: 2 → 256 patches\n",
            "Tower depth: 1\n",
            "Optimizer: Adafactor (relative_step=True, warmup_init=True)\n",
            "ConvNeXt-S: dim=768\n",
            "ViT-L: dim=1024\n",
            "CLIP ViT-G: dim=1280\n",
            "Schedule: warmup=3, plateau=12, fadeout=35\n",
            "\n",
            "Tower Configs (8 towers):\n",
            "  cantor_pos: rope=cantor, addr=cantor, inv=False\n",
            "  cantor_neg: rope=cantor, addr=cantor, inv=True\n",
            "  beatrix_pos: rope=beatrix, addr=beatrix, inv=False\n",
            "  beatrix_neg: rope=beatrix, addr=beatrix, inv=True\n",
            "  helix_pos: rope=helix, addr=helix, inv=False\n",
            "  helix_neg: rope=helix, addr=helix, inv=True\n",
            "  simplex_pos: rope=simplex, addr=simplex, inv=False\n",
            "  simplex_neg: rope=simplex, addr=simplex, inv=True\n",
            "\n",
            "Train: 50000, Test: 10000\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Loading cached DINOv3 ConvNeXt-S latents from dino_cache/dinov3_convnext_small_cifar100_test.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_train.pt\n",
            "Cache exists at dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "Loading cached DINOv3 ViT-L latents from dino_cache/dinov3_vitl16_cifar100_test.pt\n",
            "Building CLIP ViT-G cache for train (50000 images)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/10.2G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "51664daba73040b98318416a7e82417d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching CLIP-G train: 100%|██████████| 1563/1563 [09:06<00:00,  2.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/clip_vitg14_cifar100_train.pt - shape: torch.Size([50000, 1280])\n",
            "Building CLIP ViT-G cache for test (10000 images)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching CLIP-G test: 100%|██████████| 313/313 [01:49<00:00,  2.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved cache to dino_cache/clip_vitg14_cifar100_test.pt - shape: torch.Size([10000, 1280])\n",
            "\n",
            "Building model with ConfigurableTowers + Triple Expert Ensemble...\n",
            "\n",
            "Towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Expert heads: ConvNeXt-S (768→256), ViT-L (1024→256), CLIP-G (1280→256)\n",
            "Classifier input: 1024 (collective + convnext + vitl + clipg)\n",
            "Patches: 256\n",
            "Total parameters: 8,381,905\n",
            "  cantor_pos: 855,299 params, rope=cantor, inv=False\n",
            "  cantor_neg: 855,299 params, rope=cantor, inv=True\n",
            "  beatrix_pos: 855,299 params, rope=beatrix, inv=False\n",
            "  beatrix_neg: 855,299 params, rope=beatrix, inv=True\n",
            "  helix_pos: 855,428 params, rope=helix, inv=False\n",
            "  helix_neg: 855,428 params, rope=helix, inv=True\n",
            "  simplex_pos: 855,628 params, rope=simplex, inv=False\n",
            "  simplex_neg: 855,628 params, rope=simplex, inv=True\n",
            "\n",
            "Expert Scheduler: ExpertScheduler(warmup=3, plateau=12, fadeout=35)\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Training with Expert Curriculum\n",
            "Phase 1: Warmup (scale 0→1)\n",
            "Phase 2: Plateau (full power)\n",
            "Phase 3: Fadeout (dropout 0→1, towers inherit)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  1/50 | WARMUP s=0.33  | Loss: 4.3838 | Train: 10.3% | Test: 47.1% * | Time: 55.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  2/50 | WARMUP s=0.67  | Loss: 2.5635 | Train: 69.6% | Test: 86.4% * | Time: 55.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  3/50 | WARMUP s=1.00  | Loss: 0.6632 | Train: 88.6% | Test: 90.8% * | Time: 55.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  4/50 | PLATEAU        | Loss: 0.3232 | Train: 91.8% | Test: 92.3% * | Time: 55.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  5/50 | PLATEAU        | Loss: 0.2328 | Train: 93.4% | Test: 92.8% * | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  6/50 | PLATEAU        | Loss: 0.1827 | Train: 94.7% | Test: 93.0% * | Time: 56.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  7/50 | PLATEAU        | Loss: 0.1430 | Train: 95.7% | Test: 93.3% * | Time: 56.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  8/50 | PLATEAU        | Loss: 0.1089 | Train: 96.7% | Test: 93.3% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch  9/50 | PLATEAU        | Loss: 0.0793 | Train: 97.7% | Test: 93.2% | Time: 56.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/50 | PLATEAU        | Loss: 0.0558 | Train: 98.4% | Test: 93.0% | Time: 56.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/50 | PLATEAU        | Loss: 0.0403 | Train: 98.8% | Test: 93.1% | Time: 56.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/50 | PLATEAU        | Loss: 0.0329 | Train: 99.0% | Test: 92.9% | Time: 56.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/50 | PLATEAU        | Loss: 0.0252 | Train: 99.2% | Test: 92.8% | Time: 56.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/50 | PLATEAU        | Loss: 0.0217 | Train: 99.4% | Test: 92.7% | Time: 56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/50 | PLATEAU        | Loss: 0.0174 | Train: 99.5% | Test: 92.5% | Time: 56.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/50 | FADE d=0.00    | Loss: 0.0198 | Train: 99.4% | Test: 92.8% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/50 | FADE d=0.03    | Loss: 0.0202 | Train: 99.4% | Test: 92.3% | Time: 56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/50 | FADE d=0.06    | Loss: 0.0226 | Train: 99.3% | Test: 92.6% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/50 | FADE d=0.09    | Loss: 0.0249 | Train: 99.2% | Test: 91.8% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/50 | FADE d=0.11    | Loss: 0.0239 | Train: 99.3% | Test: 92.3% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21/50 | FADE d=0.14    | Loss: 0.0239 | Train: 99.2% | Test: 92.7% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22/50 | FADE d=0.17    | Loss: 0.0271 | Train: 99.1% | Test: 92.5% | Time: 56.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23/50 | FADE d=0.20    | Loss: 0.0276 | Train: 99.1% | Test: 92.5% | Time: 55.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24/50 | FADE d=0.23    | Loss: 0.0284 | Train: 99.1% | Test: 92.0% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25/50 | FADE d=0.26    | Loss: 0.0319 | Train: 99.0% | Test: 92.5% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26/50 | FADE d=0.29    | Loss: 0.0335 | Train: 98.9% | Test: 92.4% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27/50 | FADE d=0.31    | Loss: 0.0292 | Train: 99.1% | Test: 92.4% | Time: 56.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28/50 | FADE d=0.34    | Loss: 0.0268 | Train: 99.1% | Test: 92.2% | Time: 56.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29/50 | FADE d=0.37    | Loss: 0.0242 | Train: 99.3% | Test: 92.4% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30/50 | FADE d=0.40    | Loss: 0.0210 | Train: 99.3% | Test: 92.7% | Time: 56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31/50 | FADE d=0.43    | Loss: 0.0209 | Train: 99.3% | Test: 92.6% | Time: 56.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32/50 | FADE d=0.46    | Loss: 0.0213 | Train: 99.3% | Test: 92.9% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33/50 | FADE d=0.49    | Loss: 0.0206 | Train: 99.4% | Test: 93.1% | Time: 56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34/50 | FADE d=0.51    | Loss: 0.0215 | Train: 99.3% | Test: 92.8% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35/50 | FADE d=0.54    | Loss: 0.0199 | Train: 99.3% | Test: 92.8% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36/50 | FADE d=0.57    | Loss: 0.0218 | Train: 99.3% | Test: 92.9% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37/50 | FADE d=0.60    | Loss: 0.0226 | Train: 99.2% | Test: 93.1% | Time: 56.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38/50 | FADE d=0.63    | Loss: 0.0246 | Train: 99.2% | Test: 92.8% | Time: 56.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39/50 | FADE d=0.66    | Loss: 0.0255 | Train: 99.2% | Test: 93.0% | Time: 56.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40/50 | FADE d=0.69    | Loss: 0.0295 | Train: 99.0% | Test: 93.1% | Time: 56.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41/50 | FADE d=0.71    | Loss: 0.0299 | Train: 99.0% | Test: 93.0% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42/50 | FADE d=0.74    | Loss: 0.0318 | Train: 99.0% | Test: 93.2% | Time: 56.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43/50 | FADE d=0.77    | Loss: 0.0380 | Train: 98.8% | Test: 93.3% | Time: 56.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44/50 | FADE d=0.80    | Loss: 0.0473 | Train: 98.5% | Test: 93.5% * | Time: 57.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45/50 | FADE d=0.83    | Loss: 0.0546 | Train: 98.3% | Test: 93.5% | Time: 57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46/50 | FADE d=0.86    | Loss: 0.0776 | Train: 97.6% | Test: 93.4% | Time: 57.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47/50 | FADE d=0.89    | Loss: 0.1012 | Train: 96.9% | Test: 93.5% * | Time: 57.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48/50 | FADE d=0.91    | Loss: 0.1533 | Train: 95.4% | Test: 93.5% * | Time: 57.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49/50 | FADE d=0.94    | Loss: 0.2798 | Train: 91.7% | Test: 93.7% * | Time: 58.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50/50 | FADE d=0.97    | Loss: 0.8125 | Train: 76.9% | Test: 93.3% | Time: 60.8s\n",
            "\n",
            "================================================================================\n",
            "Best Test Accuracy: 93.66%\n",
            "================================================================================\n",
            "\n",
            "Final evaluation with class analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CLASS-SPECIFIC ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "Per-Class Accuracy (Top 10):\n",
            "--------------------------------------------------\n",
            "  orange          : 100.0% ████████████████████\n",
            "  pickup_truck    : 100.0% ████████████████████\n",
            "  tank            : 100.0% ████████████████████\n",
            "  apple           :  99.0% ███████████████████\n",
            "  bee             :  99.0% ███████████████████\n",
            "  chimpanzee      :  99.0% ███████████████████\n",
            "  lawn_mower      :  99.0% ███████████████████\n",
            "  mushroom        :  99.0% ███████████████████\n",
            "  palm_tree       :  99.0% ███████████████████\n",
            "  road            :  99.0% ███████████████████\n",
            "\n",
            "Per-Class Accuracy (Bottom 10):\n",
            "--------------------------------------------------\n",
            "  porcupine       :  85.0% █████████████████\n",
            "  oak_tree        :  82.0% ████████████████\n",
            "  possum          :  82.0% ████████████████\n",
            "  forest          :  81.0% ████████████████\n",
            "  mouse           :  81.0% ████████████████\n",
            "  maple_tree      :  80.0% ████████████████\n",
            "  otter           :  79.0% ███████████████\n",
            "  pine_tree       :  79.0% ███████████████\n",
            "  willow_tree     :  78.0% ███████████████\n",
            "  shrew           :  76.0% ███████████████\n",
            "\n",
            "  Average across 100 classes: 93.3%\n",
            "\n",
            "Tower Opinion Norms (avg across all classes):\n",
            "--------------------------------------------------\n",
            "  cantor_pos  : 18.13\n",
            "  cantor_neg  : 51.33\n",
            "  beatrix_pos : 82.72\n",
            "  beatrix_neg : 36.86\n",
            "  helix_pos   : 39.95\n",
            "  helix_neg   : 31.54\n",
            "  simplex_pos : 36.88\n",
            "  simplex_neg : 18.21\n",
            "  convnext    : 12.39\n",
            "  vitl        : 12.50\n",
            "  clipg       : 12.03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conv 16 tower set + dino convnext small"
      ],
      "metadata": {
        "id": "n8uc7Qe3cpI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router\n",
        "=======================\n",
        "\n",
        "Uses existing geofractal components:\n",
        "- build_tower_collective() for transformer towers\n",
        "- build_conv_tower() for conv towers\n",
        "- AdaptiveFusion from fusion_component\n",
        "- BaseRouter with attach() / network_to()\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  trans_collective (8 towers)      │\n",
        "    │  → CollectiveOpinion.fused        │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  conv towers (8 towers)           │\n",
        "    │  → conv_fusion                    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Tower builders (batched versions)\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig,\n",
        "    build_tower_collective,\n",
        "    preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig,\n",
        "    build_conv_collective,\n",
        "    preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Cache DINOv3 ConvNeXt-Small outputs.\"\"\"\n",
        "\n",
        "    MODEL = \"facebook/dinov3-convnext-small-pretrain-lvd1689m\"\n",
        "    DIM = 768\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "\n",
        "    def _path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dino_convnext_s_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._path(split)\n",
        "        if path.exists():\n",
        "            print(f\"Loading cache: {path}\")\n",
        "            return torch.load(path, weights_only=True)\n",
        "\n",
        "        print(f\"Building DINO cache for {split}...\")\n",
        "        from transformers import AutoModel\n",
        "\n",
        "        model = AutoModel.from_pretrained(self.MODEL).to(self.device).eval()\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "            out = model(pixel_values=images).pooler_output.cpu()\n",
        "            latents.append(out)\n",
        "\n",
        "        latents = torch.cat(latents, 0)\n",
        "        torch.save(latents, path)\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return latents\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS (TorchComponent subclasses)\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding component.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 2, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"Curriculum: warmup → plateau → fadeout.\"\"\"\n",
        "\n",
        "    def __init__(self, warmup: int = 3, plateau: int = 10, fadeout_end: int = 48):\n",
        "        self.warmup = warmup\n",
        "        self.plateau_end = warmup + plateau\n",
        "        self.fadeout_end = fadeout_end\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        if epoch < self.warmup:\n",
        "            return (epoch + 1) / self.warmup, 0.0\n",
        "        elif epoch < self.plateau_end:\n",
        "            return 1.0, 0.0\n",
        "        elif epoch < self.fadeout_end:\n",
        "            progress = (epoch - self.plateau_end) / (self.fadeout_end - self.plateau_end)\n",
        "            return 1.0, min(1.0, progress)\n",
        "        else:\n",
        "            return 0.0, 1.0\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using geofractal tower system.\n",
        "\n",
        "    Uses existing builders:\n",
        "    - build_tower_collective() for transformer section\n",
        "    - build_conv_tower() for conv section\n",
        "    - AdaptiveFusion for conv tower fusion\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 2,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 768,\n",
        "        trans_depth: int = 1,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "\n",
        "        # Config (stored in objects)\n",
        "        self.attach('config', {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "        })\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE ===\n",
        "        # Uses existing build_tower_collective with preset configs\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE ===\n",
        "        # Uses build_conv_collective for batched forward\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        # trans_fused (D) + conv_fused (D) + expert (D) = 3D\n",
        "        self.attach('classifier', ClassifierHead(\n",
        "            f'{name}_head', dim * 3, num_classes, dim\n",
        "        ))\n",
        "\n",
        "    @property\n",
        "    def conv_names(self) -> List[str]:\n",
        "        return self['conv_collective'].tower_names\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tensor:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (returns CollectiveOpinion)\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused\n",
        "\n",
        "        # Conv collective (returns fused, opinions_dict)\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "\n",
        "        # Expert\n",
        "        expert = self['expert'](expert_latents)\n",
        "\n",
        "        # Classify\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def forward_detailed(self, images: Tensor, expert_latents: Tensor) -> Tuple[Tensor, Dict]:\n",
        "        \"\"\"Forward with all opinions for analysis.\"\"\"\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        conv_fused, conv_opinions = self['conv_collective'](x)\n",
        "        expert = self['expert'](expert_latents)\n",
        "\n",
        "        combined = torch.cat([trans_out.fused, conv_fused, expert], dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        details = {\n",
        "            'trans_opinions': {k: v.opinion for k, v in trans_out.opinions.items()},\n",
        "            'trans_fused': trans_out.fused,\n",
        "            'conv_opinions': conv_opinions,\n",
        "            'conv_fused': conv_fused,\n",
        "            'expert': expert,\n",
        "        }\n",
        "        return logits, details\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    router.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = router(img, exp)\n",
        "        loss = F.cross_entropy(logits, lbl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "        pbar.set_postfix(loss=f'{total_loss/total:.3f}', acc=f'{100*correct/total:.1f}%')\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True):\n",
        "    router.eval()\n",
        "    router.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        logits = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router\")\n",
        "    print(\"Using: build_tower_collective, build_conv_tower, AdaptiveFusion\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 60\n",
        "    DIM = 256\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache DINO\n",
        "    cacher = DinoCacher(device=DEVICE)\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM)\n",
        "    router.network_to(device=DEVICE)  # Proper device control\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {len(router['trans_collective'].tower_names)}\")\n",
        "    print(f\"Conv towers: {len(router['conv_collective'].tower_names)}\")\n",
        "    print(f\"Trans groups: {router['trans_collective'].tower_groups}\")\n",
        "    print(f\"Conv groups: {router['conv_collective'].tower_groups}\")\n",
        "\n",
        "    # Optimizer\n",
        "    from transformers import Adafactor\n",
        "    opt = Adafactor(router.parameters(), lr=None, scale_parameter=True,\n",
        "                    relative_step=True, warmup_init=True)\n",
        "\n",
        "    sched = ExpertScheduler(warmup=3, plateau=10, fadeout_end=48)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    best = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        loss, train_acc = train_epoch(router, train_loader, opt, DEVICE, epoch, sched)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        # Phase\n",
        "        if epoch < 3:\n",
        "            phase = f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < 13:\n",
        "            phase = \"PLATEAU\"\n",
        "        elif epoch < 48:\n",
        "            phase = f\"FADE d={drop:.2f}\"\n",
        "        else:\n",
        "            phase = \"OFF\"\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:12s} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    final_with = evaluate(router, test_loader, DEVICE, expert_on=True)\n",
        "    final_without = evaluate(router, test_loader, DEVICE, expert_on=False)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(f\"Final w/ expert: {final_with:.2f}%\")\n",
        "    print(f\"Final standalone: {final_without:.2f}%\")\n",
        "    print(f\"Retained: {final_without/best*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0VR1dfUcotb",
        "outputId": "d2d7a0da-500a-4137-981a-24a541af9cae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router\n",
            "Using: build_tower_collective, build_conv_tower, AdaptiveFusion\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Building DINO cache for train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching train: 100%|██████████| 782/782 [01:14<00:00, 10.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building DINO cache for test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching test: 100%|██████████| 157/157 [00:14<00:00, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Params: 43,059,602\n",
            "Trans towers: 8\n",
            "Conv towers: 8\n",
            "Trans groups: {'cantor_cantor': ['cantor_pos', 'cantor_neg'], 'beatrix_beatrix': ['beatrix_pos', 'beatrix_neg'], 'helix_helix': ['helix_pos', 'helix_neg'], 'simplex_simplex': ['simplex_pos', 'simplex_neg']}\n",
            "Conv groups: {'depth': ['depth_pos', 'depth_neg'], 'frequency': ['frequency_pos', 'frequency_neg'], 'coarse_fine': ['coarse_fine_pos', 'coarse_fine_neg'], 'wide_resnet': ['wide_resnet_pos', 'wide_resnet_neg']}\n",
            "\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33  | loss=4.4821 | train=4.2% | test=17.1% * | 123.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67  | loss=3.3249 | train=44.2% | test=68.5% * | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00  | loss=1.4038 | train=73.9% | test=78.1% * | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PLATEAU      | loss=0.8193 | train=79.1% | test=80.6% * | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PLATEAU      | loss=0.6752 | train=81.5% | test=82.0% * | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PLATEAU      | loss=0.5967 | train=83.3% | test=82.8% * | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | PLATEAU      | loss=0.5372 | train=84.5% | test=82.8% * | 122.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | PLATEAU      | loss=0.4893 | train=85.8% | test=83.3% * | 122.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | PLATEAU      | loss=0.4443 | train=86.9% | test=83.5% * | 122.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | PLATEAU      | loss=0.4027 | train=88.0% | test=83.6% * | 122.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | PLATEAU      | loss=0.3609 | train=89.2% | test=84.0% * | 122.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | PLATEAU      | loss=0.3208 | train=90.2% | test=83.4% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | PLATEAU      | loss=0.2812 | train=91.3% | test=83.5% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | FADE d=0.00  | loss=0.2416 | train=92.4% | test=83.4% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | FADE d=0.03  | loss=0.2185 | train=93.0% | test=83.0% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | FADE d=0.06  | loss=0.1962 | train=93.7% | test=83.2% | 122.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | FADE d=0.09  | loss=0.1815 | train=94.0% | test=83.3% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | FADE d=0.11  | loss=0.1678 | train=94.3% | test=83.1% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | FADE d=0.14  | loss=0.1645 | train=94.5% | test=82.7% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | FADE d=0.17  | loss=0.1593 | train=94.6% | test=82.9% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | FADE d=0.20  | loss=0.1559 | train=94.6% | test=82.7% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | FADE d=0.23  | loss=0.1583 | train=94.7% | test=83.0% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | FADE d=0.26  | loss=0.1609 | train=94.5% | test=83.4% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | FADE d=0.29  | loss=0.1584 | train=94.7% | test=83.0% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E25 | FADE d=0.31  | loss=0.1661 | train=94.5% | test=83.0% | 122.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E26 | FADE d=0.34  | loss=0.1724 | train=94.2% | test=82.9% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E27 | FADE d=0.37  | loss=0.1661 | train=94.4% | test=83.0% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E28 | FADE d=0.40  | loss=0.1556 | train=94.8% | test=82.7% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E29 | FADE d=0.43  | loss=0.1615 | train=94.6% | test=82.9% | 122.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E30 | FADE d=0.46  | loss=0.1587 | train=94.7% | test=83.2% | 122.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E31 | FADE d=0.49  | loss=0.1577 | train=94.8% | test=83.3% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E32 | FADE d=0.51  | loss=0.1576 | train=94.8% | test=83.3% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E33 | FADE d=0.54  | loss=0.1596 | train=94.7% | test=83.6% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E34 | FADE d=0.57  | loss=0.1612 | train=94.5% | test=83.5% | 122.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E35 | FADE d=0.60  | loss=0.1659 | train=94.5% | test=83.3% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E36 | FADE d=0.63  | loss=0.1711 | train=94.4% | test=83.3% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E37 | FADE d=0.66  | loss=0.1792 | train=94.1% | test=83.7% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E38 | FADE d=0.69  | loss=0.1921 | train=93.7% | test=83.7% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E39 | FADE d=0.71  | loss=0.2000 | train=93.4% | test=84.2% * | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E40 | FADE d=0.74  | loss=0.2175 | train=93.0% | test=83.8% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E41 | FADE d=0.77  | loss=0.2353 | train=92.4% | test=84.0% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E42 | FADE d=0.80  | loss=0.2646 | train=91.7% | test=84.1% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E43 | FADE d=0.83  | loss=0.2990 | train=90.5% | test=84.1% | 123.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E44 | FADE d=0.86  | loss=0.3541 | train=89.2% | test=84.4% * | 123.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E45 | FADE d=0.89  | loss=0.4304 | train=86.9% | test=84.3% | 123.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E46 | FADE d=0.91  | loss=0.5682 | train=82.9% | test=84.4% * | 123.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E47 | FADE d=0.94  | loss=0.8615 | train=74.6% | test=83.7% | 123.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E48 | FADE d=0.97  | loss=1.6243 | train=53.2% | test=81.2% | 122.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E49 | OFF          | loss=3.3624 | train=18.2% | test=81.5% | 122.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E50 | OFF          | loss=3.2012 | train=21.2% | test=81.2% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E51 | OFF          | loss=3.0874 | train=23.3% | test=81.3% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E52 | OFF          | loss=2.9982 | train=25.1% | test=81.3% | 123.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E53 | OFF          | loss=2.9192 | train=26.4% | test=81.7% | 122.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E54 | OFF          | loss=2.8522 | train=27.7% | test=81.9% | 123.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E55 | OFF          | loss=2.7914 | train=29.1% | test=81.4% | 123.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E56 | OFF          | loss=2.7372 | train=30.1% | test=81.6% | 123.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E57 | OFF          | loss=2.6794 | train=30.9% | test=81.2% | 123.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E58 | OFF          | loss=2.6392 | train=32.1% | test=80.8% | 122.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E59 | OFF          | loss=2.5837 | train=33.1% | test=80.9% | 122.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E60 | OFF          | loss=2.5457 | train=33.7% | test=80.6% | 122.8s\n",
            "\n",
            "======================================================================\n",
            "Best: 84.38%\n",
            "Final w/ expert: 80.58%\n",
            "Final standalone: 32.46%\n",
            "Retained: 38.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# compiled and optimized wide router"
      ],
      "metadata": {
        "id": "WqeCR2XnQ96d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter for compile-optimized wide model execution.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  trans_collective (WideRouter)    │\n",
        "    │  8 towers → AdaptiveFusion        │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  conv_collective (WideRouter)     │\n",
        "    │  8 towers → AdaptiveFusion        │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "import math\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.base_tower import BaseTower\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "from geofractal.router.components.address_component import SphericalAddressComponent\n",
        "from geofractal.router.prefab.wide_router import WideRouter\n",
        "\n",
        "# Try to import RoPE components, fall back to simple implementation\n",
        "try:\n",
        "    from geofractal.router.components.rope_component import (\n",
        "        CantorRoPE,\n",
        "        BeatrixRoPE,\n",
        "        HelixRoPE,\n",
        "        SimplexRoPE,\n",
        "    )\n",
        "    ROPE_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ROPE_AVAILABLE = False\n",
        "\n",
        "# We'll create components directly instead of using builders\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class TransTowerConfig:\n",
        "    \"\"\"Config for transformer tower.\"\"\"\n",
        "    name: str\n",
        "    rope_type: str = 'cantor'\n",
        "    rope_scale: float = 1.0\n",
        "    depth: int = None\n",
        "    group: str = None\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DINO CACHING\n",
        "# =============================================================================\n",
        "\n",
        "class DinoCacher:\n",
        "    \"\"\"Cache DINOv3 ConvNeXt-Large outputs.\"\"\"\n",
        "\n",
        "    MODEL = \"facebook/dinov3-convnext-large-pretrain-lvd1689m\"\n",
        "    DIM = 1536\n",
        "\n",
        "    def __init__(self, cache_dir: str = \"./dino_cache\", device: str = \"cuda\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.device = device\n",
        "\n",
        "    def _path(self, split: str) -> Path:\n",
        "        return self.cache_dir / f\"dino_convnext_l_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, dataset: Dataset, split: str, batch_size: int = 64) -> Tensor:\n",
        "        path = self._path(split)\n",
        "        if path.exists():\n",
        "            print(f\"Loading cache: {path}\")\n",
        "            return torch.load(path, weights_only=True)\n",
        "\n",
        "        print(f\"Building DINO cache for {split}...\")\n",
        "        from transformers import AutoModel\n",
        "\n",
        "        model = AutoModel.from_pretrained(self.MODEL).to(self.device).eval()\n",
        "        loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "        latents = []\n",
        "        for images, _ in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "            images = images.to(self.device)\n",
        "            if images.shape[-1] != 224:\n",
        "                images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "            out = model(pixel_values=images).pooler_output.cpu()\n",
        "            latents.append(out)\n",
        "\n",
        "        latents = torch.cat(latents, 0)\n",
        "        torch.save(latents, path)\n",
        "        del model\n",
        "        torch.cuda.empty_cache()\n",
        "        return latents\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding component.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# Try to import CantorMultiheadFusionV2\n",
        "try:\n",
        "    from geofractal.model.layers.attention.cantor_multiheaded_fusion_fp64_v2 import (\n",
        "        CantorMultiheadFusionV2,\n",
        "        CantorFusionConfigV2,\n",
        "        create_cantor_fusion_v2,\n",
        "    )\n",
        "    CANTOR_FUSION_AVAILABLE = True\n",
        "except ImportError:\n",
        "    CANTOR_FUSION_AVAILABLE = False\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ROPE FACTORY\n",
        "# =============================================================================\n",
        "\n",
        "class FallbackRoPE(TorchComponent):\n",
        "    \"\"\"Simple sinusoidal RoPE when geometric RoPE not available.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, scale: float = 1.0, max_len: int = 1024):\n",
        "        super().__init__(name)\n",
        "        self.scale = scale\n",
        "\n",
        "        # Precompute sinusoidal embeddings\n",
        "        pe = torch.zeros(max_len, dim)\n",
        "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
        "        div = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
        "        pe[:, 0::2] = torch.sin(pos * div * scale)\n",
        "        pe[:, 1::2] = torch.cos(pos * div * scale)\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, L, D = x.shape\n",
        "        return x + self.pe[:, :L, :D]\n",
        "\n",
        "\n",
        "def make_rope(rope_type: str, dim: int, scale: float = 1.0, name: str = 'rope') -> TorchComponent:\n",
        "    \"\"\"Create RoPE component by type.\"\"\"\n",
        "    if ROPE_AVAILABLE:\n",
        "        rope_classes = {\n",
        "            'cantor': CantorRoPE,\n",
        "            'beatrix': BeatrixRoPE,\n",
        "            'helix': HelixRoPE,\n",
        "            'simplex': SimplexRoPE,\n",
        "        }\n",
        "        cls = rope_classes.get(rope_type, CantorRoPE)\n",
        "        return cls(name=name, dim=dim, scale=scale)\n",
        "    else:\n",
        "        # Fallback with different scale per type\n",
        "        type_scales = {\n",
        "            'cantor': 1.0,\n",
        "            'beatrix': 0.5,\n",
        "            'helix': 2.0,\n",
        "            'simplex': 1.5,\n",
        "        }\n",
        "        effective_scale = scale * type_scales.get(rope_type, 1.0)\n",
        "        return FallbackRoPE(name=name, dim=dim, scale=effective_scale)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRANSFORMER TOWER\n",
        "# =============================================================================\n",
        "\n",
        "class TransformerBlock(TorchComponent):\n",
        "    \"\"\"\n",
        "    Transformer block with CantorMultiheadFusion (compile-friendly).\n",
        "\n",
        "    Falls back to nn.MultiheadAttention if CantorFusion not available.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, num_heads: int = 4, fusion_window: int = 32):\n",
        "        super().__init__(name)\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "\n",
        "        if CANTOR_FUSION_AVAILABLE:\n",
        "            # Use CantorMultiheadFusionV2 - fully vectorized, compile-friendly\n",
        "            self.attn = create_cantor_fusion_v2(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                fusion_window=fusion_window,\n",
        "                fusion_mode=\"weighted\",\n",
        "                k_simplex=4,\n",
        "                use_gating=False,\n",
        "                dropout=0.1,\n",
        "                hot_cache_sizes=(64, 128, 256, 512),\n",
        "            )\n",
        "            self.use_cantor = True\n",
        "        else:\n",
        "            # Fallback to standard attention\n",
        "            self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True, dropout=0.1)\n",
        "            self.use_cantor = False\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim * 4, dim),\n",
        "            nn.Dropout(0.1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Attention\n",
        "        normed = self.norm1(x)\n",
        "        if self.use_cantor:\n",
        "            attn_out = self.attn(normed)['output']\n",
        "        else:\n",
        "            attn_out = self.attn(normed, normed, normed)[0]\n",
        "        x = x + attn_out\n",
        "\n",
        "        # FFN\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GeometricTransTower(BaseTower):\n",
        "    \"\"\"Transformer tower with geometric RoPE and CantorFusion attention.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int,\n",
        "        depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        rope_type: str = 'cantor',\n",
        "        rope_scale: float = 1.0,\n",
        "        fingerprint_dim: int = 64,\n",
        "        fusion_window: int = 32,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        # Geometric RoPE\n",
        "        rope = make_rope(rope_type, dim, scale=rope_scale, name=f'{name}_rope')\n",
        "        self.attach('rope', rope)\n",
        "\n",
        "        # Address for routing\n",
        "        address = SphericalAddressComponent(f'{name}_addr', fingerprint_dim=fingerprint_dim)\n",
        "        self.attach('address', address)\n",
        "\n",
        "        # Transformer blocks with CantorFusion\n",
        "        for i in range(depth):\n",
        "            self.append(TransformerBlock(f'{name}_block_{i}', dim, num_heads, fusion_window))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # Apply geometric RoPE\n",
        "        x = self['rope'](x)\n",
        "\n",
        "        # Process through blocks\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "\n",
        "        # Pool and normalize\n",
        "        x = x.mean(dim=1)\n",
        "        return self['norm'](x)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONV TOWER\n",
        "# =============================================================================\n",
        "\n",
        "class ConvBlock(TorchComponent):\n",
        "    \"\"\"ConvNeXt-style block.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, kernel: int = 7, expand: int = 4):\n",
        "        super().__init__(name)\n",
        "        self.dw = nn.Conv2d(dim, dim, kernel, padding=kernel // 2, groups=dim)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "        self.pw1 = nn.Linear(dim, dim * expand)\n",
        "        self.pw2 = nn.Linear(dim * expand, dim)\n",
        "        self.gamma = nn.Parameter(torch.ones(dim) * 1e-6)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # x: [B, C, H, W]\n",
        "        shortcut = x\n",
        "        x = self.dw(x)\n",
        "        x = x.permute(0, 2, 3, 1)  # [B, H, W, C]\n",
        "        x = self.norm(x)\n",
        "        x = self.pw1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.pw2(x)\n",
        "        x = x * self.gamma\n",
        "        x = x.permute(0, 3, 1, 2)  # [B, C, H, W]\n",
        "        return shortcut + x\n",
        "\n",
        "\n",
        "class GeometricConvTower(BaseTower):\n",
        "    \"\"\"Conv tower with configurable architecture.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int,\n",
        "        spatial: int,\n",
        "        depth: int = 2,\n",
        "        kernel: int = 7,\n",
        "        expand: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        # Address\n",
        "        address = SphericalAddressComponent(f'{name}_addr', fingerprint_dim=fingerprint_dim)\n",
        "        self.attach('address', address)\n",
        "\n",
        "        # Store spatial size\n",
        "        self.objects['spatial'] = spatial\n",
        "\n",
        "        # Conv blocks\n",
        "        for i in range(depth):\n",
        "            self.append(ConvBlock(f'{name}_conv_{i}', dim, kernel, expand))\n",
        "\n",
        "        self.attach('norm', nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        B, L, D = x.shape\n",
        "        S = self.objects['spatial']\n",
        "\n",
        "        # Reshape: [B, L, D] -> [B, D, H, W]\n",
        "        x = x.transpose(1, 2).view(B, D, S, S)\n",
        "\n",
        "        # Conv blocks\n",
        "        for stage in self.stages:\n",
        "            x = stage(x)\n",
        "\n",
        "        # Pool: [B, D, H, W] -> [B, D]\n",
        "        x = x.mean(dim=(2, 3))\n",
        "        return self['norm'](x)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# WIDE COLLECTIVES\n",
        "# =============================================================================\n",
        "\n",
        "class TransformerCollective(WideRouter):\n",
        "    \"\"\"WideRouter collective of transformer towers with CantorFusion attention.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int,\n",
        "        configs: List[TransTowerConfig],\n",
        "        depth: int = 1,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        fusion_window: int = 32,\n",
        "    ):\n",
        "        super().__init__(name, strict=False, auto_discover=True)\n",
        "\n",
        "        self.objects['tower_groups'] = {}\n",
        "\n",
        "        # Build towers from configs\n",
        "        for cfg in configs:\n",
        "            tower = GeometricTransTower(\n",
        "                name=cfg.name,\n",
        "                dim=dim,\n",
        "                depth=cfg.depth or depth,\n",
        "                num_heads=num_heads,\n",
        "                rope_type=cfg.rope_type,\n",
        "                rope_scale=cfg.rope_scale,\n",
        "                fingerprint_dim=fingerprint_dim,\n",
        "                fusion_window=fusion_window,\n",
        "            )\n",
        "            self.attach(cfg.name, tower)\n",
        "\n",
        "            # Track groups\n",
        "            if cfg.group:\n",
        "                if cfg.group not in self.objects['tower_groups']:\n",
        "                    self.objects['tower_groups'][cfg.group] = []\n",
        "                self.objects['tower_groups'][cfg.group].append(cfg.name)\n",
        "\n",
        "        # Discover and register towers\n",
        "        self.discover_towers()\n",
        "\n",
        "        # Fusion\n",
        "        num_towers = len(self.tower_names)\n",
        "        self.attach('fusion', AdaptiveFusion(f'{name}_fusion', num_towers, dim))\n",
        "\n",
        "    @property\n",
        "    def tower_groups(self) -> Dict[str, List[str]]:\n",
        "        return self.objects.get('tower_groups', {})\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        opinions = self.wide_forward(x)\n",
        "        return self['fusion'](*opinions.values())\n",
        "\n",
        "\n",
        "class ConvCollective(WideRouter):\n",
        "    \"\"\"WideRouter collective of conv towers.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        dim: int,\n",
        "        spatial: int,\n",
        "        configs: List[Dict],\n",
        "        depth: int = 2,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False, auto_discover=True)\n",
        "\n",
        "        self.objects['tower_groups'] = {}\n",
        "\n",
        "        # Build towers from configs\n",
        "        for cfg in configs:\n",
        "            tower = GeometricConvTower(\n",
        "                name=cfg['name'],\n",
        "                dim=dim,\n",
        "                spatial=spatial,\n",
        "                depth=cfg.get('depth', depth),\n",
        "                kernel=cfg.get('kernel', 7),\n",
        "                expand=cfg.get('expand', 4),\n",
        "                fingerprint_dim=fingerprint_dim,\n",
        "            )\n",
        "            self.attach(cfg['name'], tower)\n",
        "\n",
        "            # Track groups\n",
        "            group = cfg.get('group')\n",
        "            if group:\n",
        "                if group not in self.objects['tower_groups']:\n",
        "                    self.objects['tower_groups'][group] = []\n",
        "                self.objects['tower_groups'][group].append(cfg['name'])\n",
        "\n",
        "        # Discover and register towers\n",
        "        self.discover_towers()\n",
        "\n",
        "        # Fusion\n",
        "        num_towers = len(self.tower_names)\n",
        "        self.attach('fusion', AdaptiveFusion(f'{name}_fusion', num_towers, dim))\n",
        "\n",
        "    @property\n",
        "    def tower_groups(self) -> Dict[str, List[str]]:\n",
        "        return self.objects.get('tower_groups', {})\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        opinions = self.wide_forward(x)\n",
        "        return self['fusion'](*opinions.values())\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CONFIG PRESETS\n",
        "# =============================================================================\n",
        "\n",
        "def preset_trans_configs() -> List[TransTowerConfig]:\n",
        "    \"\"\"Pos/neg pairs for transformer towers.\"\"\"\n",
        "    pairs = [\n",
        "        ('cantor', 'cantor'),\n",
        "        ('beatrix', 'beatrix'),\n",
        "        ('helix', 'helix'),\n",
        "        ('simplex', 'simplex'),\n",
        "    ]\n",
        "\n",
        "    configs = []\n",
        "    for rope_type, group in pairs:\n",
        "        configs.append(TransTowerConfig(\n",
        "            name=f'{rope_type}_pos',\n",
        "            rope_type=rope_type,\n",
        "            rope_scale=1.0,\n",
        "            group=f'{group}_{group}',\n",
        "        ))\n",
        "        configs.append(TransTowerConfig(\n",
        "            name=f'{rope_type}_neg',\n",
        "            rope_type=rope_type,\n",
        "            rope_scale=-1.0,\n",
        "            group=f'{group}_{group}',\n",
        "        ))\n",
        "\n",
        "    return configs\n",
        "\n",
        "\n",
        "def preset_conv_configs() -> List[Dict]:\n",
        "    \"\"\"Pos/neg pairs for conv towers.\"\"\"\n",
        "    archetypes = [\n",
        "        {'name': 'depth', 'depth': 4, 'kernel': 3, 'expand': 2},\n",
        "        {'name': 'frequency', 'depth': 2, 'kernel': 1, 'expand': 8},\n",
        "        {'name': 'coarse_fine', 'depth': 3, 'kernel': 7, 'expand': 4},\n",
        "        {'name': 'wide_resnet', 'depth': 2, 'kernel': 3, 'expand': 6},\n",
        "    ]\n",
        "\n",
        "    configs = []\n",
        "    for arch in archetypes:\n",
        "        base_name = arch['name']\n",
        "        # Positive\n",
        "        configs.append({\n",
        "            **arch,\n",
        "            'name': f'{base_name}_pos',\n",
        "            'group': base_name,\n",
        "        })\n",
        "        # Negative (inverted expand)\n",
        "        configs.append({\n",
        "            **arch,\n",
        "            'name': f'{base_name}_neg',\n",
        "            'expand': max(1, 8 - arch['expand']),\n",
        "            'group': base_name,\n",
        "        })\n",
        "\n",
        "    return configs\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"Curriculum: warmup → plateau → fadeout.\"\"\"\n",
        "\n",
        "    def __init__(self, warmup: int = 3, plateau: int = 10, fadeout_end: int = 48):\n",
        "        self.warmup = warmup\n",
        "        self.plateau_end = warmup + plateau\n",
        "        self.fadeout_end = fadeout_end\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        if epoch < self.warmup:\n",
        "            return (epoch + 1) / self.warmup, 0.0\n",
        "        elif epoch < self.plateau_end:\n",
        "            return 1.0, 0.0\n",
        "        elif epoch < self.fadeout_end:\n",
        "            progress = (epoch - self.plateau_end) / (self.fadeout_end - self.plateau_end)\n",
        "            return 1.0, min(1.0, progress)\n",
        "        else:\n",
        "            return 0.0, 1.0\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER (WideRouter Edition)\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives.\n",
        "\n",
        "    Uses:\n",
        "    - TransformerCollective (WideRouter) with CantorFusion attention\n",
        "    - ConvCollective (WideRouter) for conv section\n",
        "    - torch.compile for optimized execution (CantorFusion is compile-friendly)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 1,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        fusion_window: int = 64,  # For CantorFusion sparse attention\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 16 patches per side\n",
        "        seq_len = spatial * spatial  # 256 positions\n",
        "\n",
        "        # Config\n",
        "        self.attach('config', {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "        })\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter + CantorFusion) ===\n",
        "        trans_configs = preset_trans_configs()\n",
        "        trans_collective = TransformerCollective(\n",
        "            name=f'{name}_trans',\n",
        "            dim=dim,\n",
        "            configs=trans_configs,\n",
        "            depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_window=min(fusion_window, seq_len),  # Can't exceed sequence length\n",
        "        )\n",
        "        # Pre-analyze for compile\n",
        "        trans_collective.analyze_structure()\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter) ===\n",
        "        conv_configs = preset_conv_configs()\n",
        "        conv_collective = ConvCollective(\n",
        "            name=f'{name}_conv',\n",
        "            dim=dim,\n",
        "            spatial=spatial,\n",
        "            configs=conv_configs,\n",
        "            depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "        )\n",
        "        # Pre-analyze for compile\n",
        "        conv_collective.analyze_structure()\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', ClassifierHead(\n",
        "            f'{name}_head', dim * 3, num_classes, dim\n",
        "        ))\n",
        "\n",
        "    @property\n",
        "    def trans_groups(self) -> Dict[str, List[str]]:\n",
        "        return self['trans_collective'].tower_groups\n",
        "\n",
        "    @property\n",
        "    def conv_groups(self) -> Dict[str, List[str]]:\n",
        "        return self['conv_collective'].tower_groups\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tensor:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        trans_fused = self['trans_collective'](x)\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        conv_fused = self['conv_collective'](x)\n",
        "\n",
        "        # Expert\n",
        "        expert = self['expert'](expert_latents)\n",
        "\n",
        "        # Classify\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "\n",
        "    # Set schedule on base model\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        logits = router(img, exp)\n",
        "        loss = F.cross_entropy(logits, lbl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "        pbar.set_postfix(loss=f'{total_loss/total:.3f}', acc=f'{100*correct/total:.1f}%')\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        logits = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter + Compile)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Optimize for tensor cores\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 60\n",
        "    DIM = 256\n",
        "    COMPILE = CANTOR_FUSION_AVAILABLE  # Enable if CantorFusion available (compile-friendly)\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"CantorFusion: {CANTOR_FUSION_AVAILABLE}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        #transforms.RandomCrop(32, padding=4),\n",
        "        #transforms.RandomHorizontalFlip(),\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache DINO\n",
        "    cacher = DinoCacher(device=DEVICE)\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Attention: {'CantorFusion' if CANTOR_FUSION_AVAILABLE else 'nn.MultiheadAttention'}\")\n",
        "    print(f\"Trans towers: {len(router['trans_collective'].tower_names)}\")\n",
        "    print(f\"Conv towers: {len(router['conv_collective'].tower_names)}\")\n",
        "    print(f\"Trans groups: {router.trans_groups}\")\n",
        "    print(f\"Conv groups: {router.conv_groups}\")\n",
        "\n",
        "    # Compile if enabled\n",
        "    # CantorFusion is compile-friendly (zero loops, vectorized).\n",
        "    # Standard nn.MultiheadAttention has stride issues in backward pass.\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router (CantorFusion is compile-friendly)...\")\n",
        "        router = torch.compile(router, mode='reduce-overhead')\n",
        "        print(\"Compilation complete (will trace on first forward)\")\n",
        "\n",
        "    # Optimizer\n",
        "    from transformers import Adafactor\n",
        "    # When compiled, parameters are on _orig_mod; when not, on router directly\n",
        "    if COMPILE:\n",
        "        model_params = router._orig_mod.parameters()\n",
        "    else:\n",
        "        model_params = router.parameters()\n",
        "\n",
        "    opt = Adafactor(\n",
        "        model_params,\n",
        "        lr=None,\n",
        "        scale_parameter=True,\n",
        "        relative_step=True,\n",
        "        warmup_init=True,\n",
        "    )\n",
        "\n",
        "    sched = ExpertScheduler(warmup=3, plateau=10, fadeout_end=48)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    best = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        loss, train_acc = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, is_compiled=COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        # Phase label\n",
        "        if epoch < 3:\n",
        "            phase = f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < 13:\n",
        "            phase = \"PLATEAU\"\n",
        "        elif epoch < 48:\n",
        "            phase = f\"FADE d={drop:.2f}\"\n",
        "        else:\n",
        "            phase = \"OFF\"\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:12s} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final evaluation\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    final_with = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "    final_without = evaluate(router, test_loader, DEVICE, expert_on=False, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(f\"Final w/ expert: {final_with:.2f}%\")\n",
        "    print(f\"Final standalone: {final_without:.2f}%\")\n",
        "    print(f\"Retained: {final_without/best*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGYHcHRLQ-SU",
        "outputId": "0fcf6a26-47f9-4269-bb22-b64dd24c3f77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter + Compile)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "CantorFusion: True\n",
            "Compile: True\n",
            "Loading cache: dino_cache/dino_convnext_l_train.pt\n",
            "Loading cache: dino_cache/dino_convnext_l_test.pt\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.01s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "[CantorFusionV2] Pre-building hot cache for (64, 128, 256, 512)...\n",
            "[CantorFusionV2] ✓ Hot cache built in 0.00s\n",
            "  Cache stats: {'hot_entries': 16, 'warm_entries': 0, 'hits': 0, 'misses': 4, 'hit_rate': 0.0}\n",
            "\n",
            "Params: 17,997,158\n",
            "Attention: CantorFusion\n",
            "Trans towers: 8\n",
            "Conv towers: 8\n",
            "Trans groups: {'cantor_cantor': ['cantor_pos', 'cantor_neg'], 'beatrix_beatrix': ['beatrix_pos', 'beatrix_neg'], 'helix_helix': ['helix_pos', 'helix_neg'], 'simplex_simplex': ['simplex_pos', 'simplex_neg']}\n",
            "Conv groups: {'depth': ['depth_pos', 'depth_neg'], 'frequency': ['frequency_pos', 'frequency_neg'], 'coarse_fine': ['coarse_fine_pos', 'coarse_fine_neg'], 'wide_resnet': ['wide_resnet_pos', 'wide_resnet_neg']}\n",
            "\n",
            "Compiling router (CantorFusion is compile-friendly)...\n",
            "Compilation complete (will trace on first forward)\n",
            "\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33  | loss=4.6030 | train=1.4% | test=5.4% * | 210.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67  | loss=4.2568 | train=16.2% | test=53.6% * | 66.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00  | loss=1.7028 | train=72.4% | test=82.9% * | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PLATEAU      | loss=0.6538 | train=83.7% | test=85.7% * | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PLATEAU      | loss=0.5128 | train=86.3% | test=86.9% * | 64.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PLATEAU      | loss=0.4412 | train=87.8% | test=87.2% * | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | PLATEAU      | loss=0.3899 | train=89.0% | test=87.5% * | 64.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | PLATEAU      | loss=0.3448 | train=90.0% | test=87.9% * | 64.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | PLATEAU      | loss=0.3069 | train=91.0% | test=87.7% | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | PLATEAU      | loss=0.2682 | train=92.1% | test=88.0% * | 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | PLATEAU      | loss=0.2364 | train=93.0% | test=88.0% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | PLATEAU      | loss=0.2013 | train=93.9% | test=87.8% | 64.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | PLATEAU      | loss=0.1718 | train=94.7% | test=87.6% | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | FADE d=0.00  | loss=0.1460 | train=95.5% | test=87.3% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | FADE d=0.03  | loss=0.1280 | train=96.0% | test=87.7% | 69.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | FADE d=0.06  | loss=0.1166 | train=96.2% | test=87.3% | 69.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | FADE d=0.09  | loss=0.1035 | train=96.6% | test=87.2% | 68.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | FADE d=0.11  | loss=0.0969 | train=96.8% | test=87.6% | 70.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | FADE d=0.14  | loss=0.0940 | train=96.9% | test=87.1% | 69.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | FADE d=0.17  | loss=0.0954 | train=96.8% | test=87.4% | 69.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | FADE d=0.20  | loss=0.0922 | train=97.0% | test=87.3% | 69.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | FADE d=0.23  | loss=0.0906 | train=97.0% | test=87.0% | 69.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 23:   0%|          | 0/391 [00:00<?, ?it/s]W1217 16:15:08.068000 7943 torch/_dynamo/convert_frame.py:1358] [32/8] torch._dynamo hit config.recompile_limit (8)\n",
            "W1217 16:15:08.068000 7943 torch/_dynamo/convert_frame.py:1358] [32/8]    function: 'torch_dynamo_resume_in_forward_at_164' (/tmp/ipython-input-337084772.py:164)\n",
            "W1217 16:15:08.068000 7943 torch/_dynamo/convert_frame.py:1358] [32/8]    last reason: 32/7: ___as_tensor(___stack4).item() == 0.22857142984867096  # (unknown source ___as_tensor(___stack4).item(), please file a bug)\n",
            "W1217 16:15:08.068000 7943 torch/_dynamo/convert_frame.py:1358] [32/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "W1217 16:15:08.068000 7943 torch/_dynamo/convert_frame.py:1358] [32/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | FADE d=0.26  | loss=0.0967 | train=96.8% | test=87.1% | 65.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | FADE d=0.29  | loss=0.0959 | train=96.8% | test=86.9% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E25 | FADE d=0.31  | loss=0.1001 | train=96.6% | test=87.4% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E26 | FADE d=0.34  | loss=0.0997 | train=96.7% | test=87.4% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E27 | FADE d=0.37  | loss=0.0936 | train=96.9% | test=87.5% | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E28 | FADE d=0.40  | loss=0.0930 | train=97.0% | test=87.5% | 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E29 | FADE d=0.43  | loss=0.0902 | train=97.1% | test=87.5% | 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E30 | FADE d=0.46  | loss=0.0877 | train=97.1% | test=87.8% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E31 | FADE d=0.49  | loss=0.0869 | train=97.2% | test=87.5% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E32 | FADE d=0.51  | loss=0.0877 | train=97.0% | test=87.9% | 64.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E33 | FADE d=0.54  | loss=0.0874 | train=97.1% | test=87.8% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E34 | FADE d=0.57  | loss=0.0903 | train=97.1% | test=87.7% | 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E35 | FADE d=0.60  | loss=0.0889 | train=97.1% | test=87.8% | 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E36 | FADE d=0.63  | loss=0.0953 | train=96.9% | test=87.9% | 65.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E37 | FADE d=0.66  | loss=0.0984 | train=96.8% | test=87.9% | 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E38 | FADE d=0.69  | loss=0.1069 | train=96.6% | test=88.0% * | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E39 | FADE d=0.71  | loss=0.1143 | train=96.4% | test=88.1% * | 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E40 | FADE d=0.74  | loss=0.1260 | train=96.0% | test=88.1% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E41 | FADE d=0.77  | loss=0.1368 | train=95.7% | test=88.3% * | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E42 | FADE d=0.80  | loss=0.1596 | train=95.0% | test=88.3% | 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E43 | FADE d=0.83  | loss=0.1881 | train=94.4% | test=88.2% | 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E44 | FADE d=0.86  | loss=0.2233 | train=93.3% | test=88.3% | 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E45 | FADE d=0.89  | loss=0.2960 | train=91.2% | test=88.4% * | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E46 | FADE d=0.91  | loss=0.4392 | train=86.9% | test=88.5% * | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E47 | FADE d=0.94  | loss=0.7875 | train=76.9% | test=88.7% * | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E48 | FADE d=0.97  | loss=1.8270 | train=48.2% | test=87.9% | 65.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E49 | OFF          | loss=4.1077 | train=7.2% | test=87.8% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E50 | OFF          | loss=3.9721 | train=9.4% | test=88.0% | 64.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E51 | OFF          | loss=3.8789 | train=10.9% | test=88.0% | 64.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E52 | OFF          | loss=3.7923 | train=12.3% | test=88.2% | 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E53 | OFF          | loss=3.7114 | train=13.6% | test=88.4% | 64.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E54 | OFF          | loss=3.6590 | train=14.4% | test=88.1% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E55 | OFF          | loss=3.5826 | train=15.7% | test=88.0% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E56 | OFF          | loss=3.5423 | train=16.6% | test=88.2% | 64.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E57 | OFF          | loss=3.4902 | train=17.5% | test=87.9% | 64.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E58 | OFF          | loss=3.4445 | train=18.2% | test=87.9% | 64.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E59 | OFF          | loss=3.3948 | train=19.5% | test=87.9% | 64.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E60 | OFF          | loss=3.3507 | train=20.0% | test=88.1% | 64.7s\n",
            "\n",
            "======================================================================\n",
            "Best: 88.71%\n",
            "Final w/ expert: 88.06%\n",
            "Final standalone: 27.23%\n",
            "Retained: 30.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rehauled datacomponent multi-tower"
      ],
      "metadata": {
        "id": "QtYEij8vdw04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Expert curriculum with extended stress phase.\n",
        "\n",
        "    KEY INSIGHT from training: dropout 0.80-0.91 is the sweet spot!\n",
        "    Test accuracy peaks during high dropout - extend this phase.\n",
        "\n",
        "    Phases:\n",
        "        1. WARMUP (0-3): Scale ramps 0→1\n",
        "        2. PLATEAU (3-13): Full expert, no dropout\n",
        "        3. RAMP (13-28): Gradual dropout 0→0.85\n",
        "        4. STRESS (28-53): EXTENDED high dropout at 0.85 - this is the magic!\n",
        "        5. CRUISE (53+): Keep expert at 15% scale, maintain dropout\n",
        "\n",
        "    Never fully decouple - keep expert signal flowing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        plateau: int = 10,\n",
        "        ramp_epochs: int = 15,       # Gradual increase to target dropout\n",
        "        stress_epochs: int = 25,     # EXTENDED high-dropout phase\n",
        "        stress_dropout: float = 0.85,  # Sweet spot dropout\n",
        "        residual_scale: float = 0.15,  # Keep expert alive\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.plateau_end = warmup + plateau\n",
        "        self.stress_start = self.plateau_end + ramp_epochs\n",
        "        self.stress_end = self.stress_start + stress_epochs\n",
        "        self.stress_dropout = stress_dropout\n",
        "        self.residual_scale = residual_scale\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        if epoch < self.warmup:\n",
        "            # WARMUP: ramp scale up\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.plateau_end:\n",
        "            # PLATEAU: full expert, no dropout\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_start:\n",
        "            # RAMP: gradually increase dropout to target\n",
        "            progress = (epoch - self.plateau_end) / (self.stress_start - self.plateau_end)\n",
        "            dropout = progress * self.stress_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            # STRESS: high dropout, full scale - THE SWEET SPOT\n",
        "            return 1.0, self.stress_dropout\n",
        "\n",
        "        else:\n",
        "            # CRUISE: residual expert, maintain dropout - never fully off\n",
        "            return self.residual_scale, self.stress_dropout\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.plateau_end:\n",
        "            return \"PLATEAU\"\n",
        "        elif epoch < self.stress_start:\n",
        "            return f\"RAMP d={drop:.2f}\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"CRUISE s={scale:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Uses:\n",
        "    - ConfigurableCollective (WideRouter) from geometric_tower_builder\n",
        "    - ConvTowerCollective (WideRouter) from geometric_conv_tower_builder\n",
        "    - torch.compile for optimized execution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 8,\n",
        "        conv_depth: int = 8,\n",
        "        num_heads: int = 8,\n",
        "        fingerprint_dim: int = 128,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        # Input: trans_fused + conv_fused + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tensor:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        # Returns CollectiveOpinion with .fused attribute\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        # Returns (fused, opinions_dict)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Classify: [B, 3*D] -> [B, num_classes]\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        # Analyze structures before compile\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "\n",
        "        # Compile\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = router(img, exp)\n",
        "        loss = F.cross_entropy(logits, lbl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "        pbar.set_postfix(loss=f'{total_loss/total:.3f}', acc=f'{100*correct/total:.1f}%')\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        correct += router(img, exp).argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 70  # Extended for longer stress phase\n",
        "    DIM = 256\n",
        "    COMPILE = True  # Now safe - FrequencyComponent uses spatial-domain filtering\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # Optimizer\n",
        "    from transformers import Adafactor\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    opt = Adafactor(model_params, lr=None, scale_parameter=True, relative_step=True, warmup_init=True)\n",
        "\n",
        "    # Expert scheduler with extended stress phase\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        plateau=10,\n",
        "        ramp_epochs=15,       # Epochs 13-28: ramp dropout 0→0.85\n",
        "        stress_epochs=25,     # Epochs 28-53: HOLD at 0.85 dropout (sweet spot!)\n",
        "        stress_dropout=0.85,  # The magic number from previous run\n",
        "        residual_scale=0.15,  # Never fully off\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PLATEAU: E{sched.warmup+1:02d}-E{sched.plateau_end:02d}\")\n",
        "    print(f\"  RAMP:    E{sched.plateau_end+1:02d}-E{sched.stress_start:02d} (dropout 0→{sched.stress_dropout})\")\n",
        "    print(f\"  STRESS:  E{sched.stress_start+1:02d}-E{sched.stress_end:02d} (dropout={sched.stress_dropout} - SWEET SPOT)\")\n",
        "    print(f\"  CRUISE:  E{sched.stress_end+1:02d}+ (scale={sched.residual_scale}, dropout={sched.stress_dropout})\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        loss, train_acc = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test with different expert levels\n",
        "    final_full = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test with residual expert (the trained configuration)\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(sched.residual_scale, sched.stress_dropout)\n",
        "    final_residual = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone (no expert)\n",
        "    final_standalone = evaluate(router, test_loader, DEVICE, expert_on=False, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(f\"Final w/ full expert: {final_full:.2f}%\")\n",
        "    print(f\"Final w/ residual expert (s={sched.residual_scale}, d={sched.stress_dropout}): {final_residual:.2f}%\")\n",
        "    print(f\"Final standalone: {final_standalone:.2f}%\")\n",
        "    print(f\"Retained (residual): {final_residual/best*100:.1f}%\")\n",
        "    print(f\"Retained (standalone): {final_standalone/best*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "id": "EeC1OPUzdwBC",
        "outputId": "5bb4000e-f629-447a-88a4-83e73aceeb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 59,391,230\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PLATEAU: E04-E13\n",
            "  RAMP:    E14-E28 (dropout 0→0.85)\n",
            "  STRESS:  E29-E53 (dropout=0.85 - SWEET SPOT)\n",
            "  CRUISE:  E54+ (scale=0.15, dropout=0.85)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'tracer_output' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   3545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_function\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3546\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3547\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_method\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2813\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2814\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_prims_common/wrappers.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36mcudnn_batch_norm\u001b[0;34m(input, weight, bias, running_mean, running_var, training, exponential_average_factor, epsilon)\u001b[0m\n\u001b[1;32m   2245\u001b[0m ):\n\u001b[0;32m-> 2246\u001b[0;31m     a, b, c = aten.native_batch_norm(\n\u001b[0m\u001b[1;32m   2247\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_decomp/decompositions.py\u001b[0m in \u001b[0;36mnative_batch_norm_decomposition\u001b[0;34m(input, weight, bias, running_mean, running_var, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         \u001b[0;31m# HACK: batch norm consolidation should clean this up so this op doesn't take in a training arg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m         return aten._native_batch_norm_legit(\n\u001b[0m\u001b[1;32m   1960\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1254\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_call_overload_packet_from_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_stats.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m     def _cached_dispatch_impl(\n\u001b[0m\u001b[1;32m   1449\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0mguarded_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks)\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m             dynamo_output = compile_frame(\n\u001b[0m\u001b[1;32m   1152\u001b[0m                 \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_frame\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, restart_reasons, export, export_constraints, frame_state, distributed_state, package)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"compile_attempt_{attempt}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pt2_compile_event\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                 \u001b[0mbytecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         )\n\u001b[0;32m-> 1004\u001b[0;31m         tracer_output = trace_frame(\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtrace_frame\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, tf_mode_stack, one_graph, speculation_log, instructions, code_options, export, export_constraints, frame_state, distributed_state, package)\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mrun_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m         \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamoTracerOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mrun_tracer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/torch.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m         tensor_variable = wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   1517\u001b[0m             \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubclass_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_fx_proxy_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexample_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m         return _wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   2712\u001b[0m             \u001b[0mtarget_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36m_wrap_fx_proxy\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# cases properly below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mexample_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fake_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_non_graph_fake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   3375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_python_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3376\u001b[0;31m             ret_val = wrap_fake_exception(\n\u001b[0m\u001b[1;32m   3377\u001b[0m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2865\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupportedFakeTensorException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3376\u001b[0m             ret_val = wrap_fake_exception(\n\u001b[0;32m-> 3377\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3378\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   3533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3534\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mset_current_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-996179175.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-996179175.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    634\u001b[0m         \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 636\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOMPILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    637\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_compiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-996179175.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(router, loader, opt, device, epoch, sched, is_compiled)\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def __reduce__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m             result = self._torchdynamo_orig_backend(\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             result = self._inner_convert(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             result = _compile(\n\u001b[0m\u001b[1;32m    689\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtracer_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'tracer_output' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# conv large mega router + SGDCollective"
      ],
      "metadata": {
        "id": "l_43SnzR1BdO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Bidirectional dropout schedule with LR snap.\n",
        "\n",
        "    Phase 1 (high LR=0.1): Increase dropout to force standalone learning\n",
        "        - WARMUP:  E01-E03 (scale 0→1)\n",
        "        - PREP:    E04-E13 (full expert, no dropout)\n",
        "        - STRESS:  E14-E23 (dropout 0→0.85, high LR)\n",
        "\n",
        "    Phase 2 (low LR): Decrease dropout to refine with expert\n",
        "        - REFINE:  E24-E28 (dropout 0.85→0.2, low LR)\n",
        "\n",
        "    Key insight: Build standalone capability under stress, then\n",
        "    let expert guide refinement at low LR.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        prep: int = 10,\n",
        "        stress: int = 10,\n",
        "        refine: int = 5,\n",
        "        max_dropout: float = 0.85,\n",
        "        min_dropout: float = 0.2,\n",
        "        high_lr: float = 0.1,\n",
        "        low_lr: float = 0.01,\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.prep_end = warmup + prep\n",
        "        self.stress_end = self.prep_end + stress\n",
        "        self.refine_end = self.stress_end + refine\n",
        "\n",
        "        self.max_dropout = max_dropout\n",
        "        self.min_dropout = min_dropout\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout).\"\"\"\n",
        "        if epoch < self.warmup:\n",
        "            # WARMUP: ramp scale up\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.prep_end:\n",
        "            # PREP: full expert, no dropout\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            # STRESS: increase dropout (high LR phase)\n",
        "            progress = (epoch - self.prep_end) / (self.stress_end - self.prep_end)\n",
        "            dropout = progress * self.max_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        else:\n",
        "            # REFINE: decrease dropout (low LR phase)\n",
        "            progress = (epoch - self.stress_end) / (self.refine_end - self.stress_end)\n",
        "            progress = min(1.0, progress)\n",
        "            dropout = self.max_dropout - progress * (self.max_dropout - self.min_dropout)\n",
        "            return 1.0, dropout\n",
        "\n",
        "    def get_lr(self, epoch: int) -> float:\n",
        "        \"\"\"Get learning rate for epoch.\"\"\"\n",
        "        if epoch < self.stress_end:\n",
        "            return self.high_lr\n",
        "        else:\n",
        "            return self.low_lr\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        lr = self.get_lr(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.prep_end:\n",
        "            return \"PREP\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"REFINE d={drop:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Uses:\n",
        "    - ConfigurableCollective (WideRouter) from geometric_tower_builder\n",
        "    - ConvTowerCollective (WideRouter) from geometric_conv_tower_builder\n",
        "    - torch.compile for optimized execution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        # Input: trans_fused + conv_fused + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tensor:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        # Returns CollectiveOpinion with .fused attribute\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        # Returns (fused, opinions_dict)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Classify: [B, 3*D] -> [B, num_classes]\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        # Analyze structures before compile\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "\n",
        "        # Compile\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = router(img, exp)\n",
        "        loss = F.cross_entropy(logits, lbl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "        pbar.set_postfix(loss=f'{total_loss/total:.3f}', acc=f'{100*correct/total:.1f}%')\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        correct += router(img, exp).argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 14  # 3 warmup + 3 prep + 3 stress + 5 refine\n",
        "    DIM = 256\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # SGD with momentum\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    HIGH_LR = 0.1\n",
        "    LOW_LR = 0.001  # Lower for fine refinement\n",
        "    opt = torch.optim.SGD(model_params, lr=HIGH_LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Tight bidirectional dropout schedule\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        prep=3,         # Quick prep\n",
        "        stress=3,       # Quick stress ramp\n",
        "        refine=5,       # Refine with expert\n",
        "        max_dropout=0.85,\n",
        "        min_dropout=0.2,\n",
        "        high_lr=HIGH_LR,\n",
        "        low_lr=LOW_LR,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={HIGH_LR}, momentum=0.9)\")\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PREP:    E{sched.warmup+1:02d}-E{sched.prep_end:02d}\")\n",
        "    print(f\"  STRESS:  E{sched.prep_end+1:02d}-E{sched.stress_end:02d} (dropout 0→{sched.max_dropout}, LR={HIGH_LR})\")\n",
        "    print(f\"  REFINE:  E{sched.stress_end+1:02d}-E{sched.refine_end:02d} (dropout {sched.max_dropout}→{sched.min_dropout}, LR={LOW_LR})\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    current_lr = HIGH_LR\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        # LR snap at stress_end\n",
        "        new_lr = sched.get_lr(epoch)\n",
        "        if new_lr != current_lr:\n",
        "            print(f\"\\n*** LR SNAP: {current_lr} → {new_lr} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = new_lr\n",
        "            current_lr = new_lr\n",
        "\n",
        "        loss, train_acc = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | lr={current_lr:.3f} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test with full expert\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(1.0, 0.0)\n",
        "    final_full = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test with final dropout level\n",
        "    base.set_expert_schedule(1.0, sched.min_dropout)\n",
        "    final_refined = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone (no expert)\n",
        "    final_standalone = evaluate(router, test_loader, DEVICE, expert_on=False, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(f\"Final w/ full expert (d=0): {final_full:.2f}%\")\n",
        "    print(f\"Final w/ refined (d={sched.min_dropout}): {final_refined:.2f}%\")\n",
        "    print(f\"Final standalone: {final_standalone:.2f}%\")\n",
        "    print(f\"Retained: {final_standalone/best*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xk3woyjv1E-u",
        "outputId": "45a0ca7f-82d4-4994-f638-82290b336b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 17,952,926\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PREP:    E04-E06\n",
            "  STRESS:  E07-E09 (dropout 0→0.85, LR=0.1)\n",
            "  REFINE:  E10-E14 (dropout 0.85→0.2, LR=0.001)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33    | lr=0.100 | loss=1.0759 | train=74.1% | test=84.1% * | 84.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67    | lr=0.100 | loss=0.4895 | train=87.1% | test=87.1% * | 19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00    | lr=0.100 | loss=0.3281 | train=90.6% | test=87.1% * | 13.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PREP           | lr=0.100 | loss=0.2562 | train=92.3% | test=87.9% * | 13.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PREP           | lr=0.100 | loss=0.1989 | train=93.8% | test=87.3% | 13.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PREP           | lr=0.100 | loss=0.1689 | train=94.8% | test=87.2% | 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | STRESS d=0.00  | lr=0.100 | loss=0.1484 | train=95.4% | test=87.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | STRESS d=0.28  | lr=0.100 | loss=0.1501 | train=95.3% | test=86.8% | 16.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | STRESS d=0.57  | lr=0.100 | loss=0.1936 | train=94.2% | test=87.6% | 18.7s\n",
            "\n",
            "*** LR SNAP: 0.1 → 0.001 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | REFINE d=0.85  | lr=0.001 | loss=0.4936 | train=87.8% | test=88.0% * | 18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | REFINE d=0.72  | lr=0.001 | loss=0.1561 | train=95.3% | test=88.2% * | 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | REFINE d=0.59  | lr=0.001 | loss=0.1008 | train=96.8% | test=88.4% * | 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | REFINE d=0.46  | lr=0.001 | loss=0.0797 | train=97.5% | test=88.4% * | 18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | REFINE d=0.33  | lr=0.001 | loss=0.0705 | train=97.8% | test=88.5% * | 18.7s\n",
            "\n",
            "======================================================================\n",
            "Best: 88.45%\n",
            "Final w/ full expert (d=0): 88.45%\n",
            "Final w/ refined (d=0.2): 88.45%\n",
            "Final standalone: 2.63%\n",
            "Retained: 3.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multisnap SGD drop the bass on a megatower"
      ],
      "metadata": {
        "id": "0joFOO5x50qL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Bidirectional dropout schedule with LR snaps.\n",
        "\n",
        "    Phase 1 (high LR=0.1): Increase dropout to force standalone learning\n",
        "        - WARMUP:  E01-E03 (scale 0→1)\n",
        "        - PREP:    E04-E06 (full expert, no dropout)\n",
        "        - STRESS:  E07-E09 (dropout 0→0.85, high LR)\n",
        "\n",
        "    Phase 2 (low LR=0.001): Decrease dropout to refine with expert\n",
        "        - REFINE:  E10-E14 (dropout 0.85→0.2)\n",
        "\n",
        "    Phase 3 (ultra low LR=0.0001): Polish with minimal dropout\n",
        "        - POLISH:  E15-E24 (dropout 0.1, ultra low LR)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        prep: int = 3,\n",
        "        stress: int = 3,\n",
        "        refine: int = 5,\n",
        "        polish: int = 10,\n",
        "        max_dropout: float = 0.85,\n",
        "        min_dropout: float = 0.2,\n",
        "        polish_dropout: float = 0.1,\n",
        "        high_lr: float = 0.1,\n",
        "        low_lr: float = 0.001,\n",
        "        polish_lr: float = 0.0001,\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.prep_end = warmup + prep\n",
        "        self.stress_end = self.prep_end + stress\n",
        "        self.refine_end = self.stress_end + refine\n",
        "        self.polish_end = self.refine_end + polish\n",
        "\n",
        "        self.max_dropout = max_dropout\n",
        "        self.min_dropout = min_dropout\n",
        "        self.polish_dropout = polish_dropout\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.polish_lr = polish_lr\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout).\"\"\"\n",
        "        if epoch < self.warmup:\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.prep_end:\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            progress = (epoch - self.prep_end) / (self.stress_end - self.prep_end)\n",
        "            dropout = progress * self.max_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        elif epoch < self.refine_end:\n",
        "            progress = (epoch - self.stress_end) / (self.refine_end - self.stress_end)\n",
        "            dropout = self.max_dropout - progress * (self.max_dropout - self.min_dropout)\n",
        "            return 1.0, dropout\n",
        "\n",
        "        else:\n",
        "            # POLISH: low dropout, ultra low LR\n",
        "            return 1.0, self.polish_dropout\n",
        "\n",
        "    def get_lr(self, epoch: int) -> float:\n",
        "        \"\"\"Get learning rate for epoch.\"\"\"\n",
        "        if epoch < self.stress_end:\n",
        "            return self.high_lr\n",
        "        elif epoch < self.refine_end:\n",
        "            return self.low_lr\n",
        "        else:\n",
        "            return self.polish_lr\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.prep_end:\n",
        "            return \"PREP\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        elif epoch < self.refine_end:\n",
        "            return f\"REFINE d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"POLISH d={drop:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Uses:\n",
        "    - ConfigurableCollective (WideRouter) from geometric_tower_builder\n",
        "    - ConvTowerCollective (WideRouter) from geometric_conv_tower_builder\n",
        "    - torch.compile for optimized execution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        # Input: trans_fused + conv_fused + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tensor:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        # Returns CollectiveOpinion with .fused attribute\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        # Returns (fused, opinions_dict)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Classify: [B, 3*D] -> [B, num_classes]\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        return self['classifier'](combined)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        # Analyze structures before compile\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "\n",
        "        # Compile\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, total = 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "        logits = router(img, exp)\n",
        "        loss = F.cross_entropy(logits, lbl)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "        pbar.set_postfix(loss=f'{total_loss/total:.3f}', acc=f'{100*correct/total:.1f}%')\n",
        "\n",
        "    return total_loss / total, 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        correct += router(img, exp).argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 24  # 3 warmup + 3 prep + 3 stress + 5 refine + 10 polish\n",
        "    DIM = 256\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # SGD with momentum\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    HIGH_LR = 0.1\n",
        "    LOW_LR = 0.001\n",
        "    POLISH_LR = 0.0001\n",
        "    opt = torch.optim.SGD(model_params, lr=HIGH_LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Tight schedule with polish phase\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        prep=3,\n",
        "        stress=3,\n",
        "        refine=5,\n",
        "        polish=10,        # 10 more epochs to polish\n",
        "        max_dropout=0.85,\n",
        "        min_dropout=0.2,\n",
        "        polish_dropout=0.1,  # Low dropout for polish\n",
        "        high_lr=HIGH_LR,\n",
        "        low_lr=LOW_LR,\n",
        "        polish_lr=POLISH_LR,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={HIGH_LR}, momentum=0.9)\")\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PREP:    E{sched.warmup+1:02d}-E{sched.prep_end:02d}\")\n",
        "    print(f\"  STRESS:  E{sched.prep_end+1:02d}-E{sched.stress_end:02d} (dropout 0→{sched.max_dropout}, LR={HIGH_LR})\")\n",
        "    print(f\"  REFINE:  E{sched.stress_end+1:02d}-E{sched.refine_end:02d} (dropout {sched.max_dropout}→{sched.min_dropout}, LR={LOW_LR})\")\n",
        "    print(f\"  POLISH:  E{sched.refine_end+1:02d}-E{sched.polish_end:02d} (dropout={sched.polish_dropout}, LR={POLISH_LR})\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    current_lr = HIGH_LR\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        # LR snap at phase transitions\n",
        "        new_lr = sched.get_lr(epoch)\n",
        "        if new_lr != current_lr:\n",
        "            print(f\"\\n*** LR SNAP: {current_lr:.4f} → {new_lr:.4f} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = new_lr\n",
        "            current_lr = new_lr\n",
        "\n",
        "        loss, train_acc = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | lr={current_lr:.4f} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test with full expert\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(1.0, 0.0)\n",
        "    final_full = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test with polish dropout level\n",
        "    base.set_expert_schedule(1.0, sched.polish_dropout)\n",
        "    final_polished = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone (no expert)\n",
        "    final_standalone = evaluate(router, test_loader, DEVICE, expert_on=False, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(f\"Final w/ full expert (d=0): {final_full:.2f}%\")\n",
        "    print(f\"Final w/ polish (d={sched.polish_dropout}): {final_polished:.2f}%\")\n",
        "    print(f\"Final standalone: {final_standalone:.2f}%\")\n",
        "    print(f\"Retained: {final_standalone/best*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPzBUWR53Uz",
        "outputId": "f43b06b5-8d33-4fb7-bd25-e10be2c12734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 17,952,926\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PREP:    E04-E06\n",
            "  STRESS:  E07-E09 (dropout 0→0.85, LR=0.1)\n",
            "  REFINE:  E10-E14 (dropout 0.85→0.2, LR=0.001)\n",
            "  POLISH:  E15-E24 (dropout=0.1, LR=0.0001)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33    | lr=0.1000 | loss=1.0826 | train=74.2% | test=84.6% * | 86.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67    | lr=0.1000 | loss=0.4825 | train=87.3% | test=86.8% * | 15.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00    | lr=0.1000 | loss=0.3282 | train=90.4% | test=87.1% * | 13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PREP           | lr=0.1000 | loss=0.2519 | train=92.4% | test=87.6% * | 13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PREP           | lr=0.1000 | loss=0.2051 | train=93.8% | test=87.6% | 13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PREP           | lr=0.1000 | loss=0.1695 | train=94.6% | test=87.5% | 13.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | STRESS d=0.00  | lr=0.1000 | loss=0.1456 | train=95.3% | test=87.2% | 13.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | STRESS d=0.28  | lr=0.1000 | loss=0.1495 | train=95.4% | test=87.1% | 15.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | STRESS d=0.57  | lr=0.1000 | loss=0.1943 | train=94.1% | test=87.4% | 15.9s\n",
            "\n",
            "*** LR SNAP: 0.1000 → 0.0010 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | REFINE d=0.85  | lr=0.0010 | loss=0.5134 | train=87.5% | test=88.2% * | 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | REFINE d=0.72  | lr=0.0010 | loss=0.1654 | train=95.0% | test=88.5% * | 14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | REFINE d=0.59  | lr=0.0010 | loss=0.1080 | train=96.6% | test=88.5% * | 15.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | REFINE d=0.46  | lr=0.0010 | loss=0.0867 | train=97.3% | test=88.6% * | 15.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | REFINE d=0.33  | lr=0.0010 | loss=0.0744 | train=97.7% | test=88.7% * | 15.2s\n",
            "\n",
            "*** LR SNAP: 0.0010 → 0.0001 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | POLISH d=0.10  | lr=0.0001 | loss=0.0644 | train=98.1% | test=88.7% | 21.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | POLISH d=0.10  | lr=0.0001 | loss=0.0641 | train=98.1% | test=88.7% | 14.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | POLISH d=0.10  | lr=0.0001 | loss=0.0631 | train=98.1% | test=88.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | POLISH d=0.10  | lr=0.0001 | loss=0.0632 | train=98.1% | test=88.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | POLISH d=0.10  | lr=0.0001 | loss=0.0631 | train=98.1% | test=88.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | POLISH d=0.10  | lr=0.0001 | loss=0.0616 | train=98.2% | test=88.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | POLISH d=0.10  | lr=0.0001 | loss=0.0613 | train=98.1% | test=88.7% | 14.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | POLISH d=0.10  | lr=0.0001 | loss=0.0606 | train=98.1% | test=88.7% * | 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | POLISH d=0.10  | lr=0.0001 | loss=0.0596 | train=98.2% | test=88.7% | 13.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | POLISH d=0.10  | lr=0.0001 | loss=0.0593 | train=98.2% | test=88.7% | 13.9s\n",
            "\n",
            "======================================================================\n",
            "Best: 88.73%\n",
            "Final w/ full expert (d=0): 88.73%\n",
            "Final w/ polish (d=0.1): 88.73%\n",
            "Final standalone: 2.39%\n",
            "Retained: 2.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dual-head multi-collective - dual collective with expert guidance"
      ],
      "metadata": {
        "id": "c5P2L2CG-W4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Bidirectional dropout schedule with LR snaps.\n",
        "\n",
        "    Phase 1 (high LR=0.1): Increase dropout to force standalone learning\n",
        "        - WARMUP:  E01-E03 (scale 0→1)\n",
        "        - PREP:    E04-E06 (full expert, no dropout)\n",
        "        - STRESS:  E07-E09 (dropout 0→0.85, high LR)\n",
        "\n",
        "    Phase 2 (low LR=0.001): Decrease dropout to refine with expert\n",
        "        - REFINE:  E10-E14 (dropout 0.85→0.2)\n",
        "\n",
        "    Phase 3 (ultra low LR=0.0001): Polish with minimal dropout\n",
        "        - POLISH:  E15-E24 (dropout 0.1, ultra low LR)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        prep: int = 3,\n",
        "        stress: int = 3,\n",
        "        refine: int = 5,\n",
        "        polish: int = 10,\n",
        "        max_dropout: float = 0.85,\n",
        "        min_dropout: float = 0.2,\n",
        "        polish_dropout: float = 0.1,\n",
        "        high_lr: float = 0.1,\n",
        "        low_lr: float = 0.001,\n",
        "        polish_lr: float = 0.0001,\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.prep_end = warmup + prep\n",
        "        self.stress_end = self.prep_end + stress\n",
        "        self.refine_end = self.stress_end + refine\n",
        "        self.polish_end = self.refine_end + polish\n",
        "\n",
        "        self.max_dropout = max_dropout\n",
        "        self.min_dropout = min_dropout\n",
        "        self.polish_dropout = polish_dropout\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.polish_lr = polish_lr\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout).\"\"\"\n",
        "        if epoch < self.warmup:\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.prep_end:\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            progress = (epoch - self.prep_end) / (self.stress_end - self.prep_end)\n",
        "            dropout = progress * self.max_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        elif epoch < self.refine_end:\n",
        "            progress = (epoch - self.stress_end) / (self.refine_end - self.stress_end)\n",
        "            dropout = self.max_dropout - progress * (self.max_dropout - self.min_dropout)\n",
        "            return 1.0, dropout\n",
        "\n",
        "        else:\n",
        "            # POLISH: low dropout, ultra low LR\n",
        "            return 1.0, self.polish_dropout\n",
        "\n",
        "    def get_lr(self, epoch: int) -> float:\n",
        "        \"\"\"Get learning rate for epoch.\"\"\"\n",
        "        if epoch < self.stress_end:\n",
        "            return self.high_lr\n",
        "        elif epoch < self.refine_end:\n",
        "            return self.low_lr\n",
        "        else:\n",
        "            return self.polish_lr\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.prep_end:\n",
        "            return \"PREP\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        elif epoch < self.refine_end:\n",
        "            return f\"REFINE d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"POLISH d={drop:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Dual-head architecture:\n",
        "    - Main head: trans + conv + expert → full power classification\n",
        "    - Standalone head: trans + conv only → forces tower learning\n",
        "\n",
        "    Both heads trained simultaneously to ensure towers learn real features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === DUAL HEADS ===\n",
        "        # Main: trans + conv + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "        # Standalone: trans + conv = 2 * dim (forces towers to learn)\n",
        "        self.attach('standalone_head', ClassifierHead(f'{name}_standalone', dim * 2, num_classes, dim))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Main classifier: all three\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        # Standalone classifier: towers only (no expert)\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        standalone_logits = self['standalone_head'](standalone)\n",
        "\n",
        "        return logits, standalone_logits\n",
        "\n",
        "    def forward_standalone(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"Forward using only towers (no expert) for evaluation.\"\"\"\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused\n",
        "\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        return self['standalone_head'](standalone)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False, standalone_weight=0.5):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, correct_standalone, total = 0, 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Dual outputs\n",
        "        logits, standalone_logits = router(img, exp)\n",
        "\n",
        "        # Combined loss: main + weighted standalone\n",
        "        main_loss = F.cross_entropy(logits, lbl)\n",
        "        standalone_loss = F.cross_entropy(standalone_logits, lbl)\n",
        "        loss = main_loss + standalone_weight * standalone_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        correct_standalone += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            acc=f'{100*correct/total:.1f}%',\n",
        "            solo=f'{100*correct_standalone/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss / total, 100 * correct / total, 100 * correct_standalone / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        logits, _ = router(img, exp)  # Main head\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_standalone(router, loader, device, is_compiled=False):\n",
        "    \"\"\"Evaluate using only the standalone head (no expert).\"\"\"\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        # Get both outputs, use standalone\n",
        "        _, standalone_logits = router(img, exp.to(device))\n",
        "        correct += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 24  # 3 warmup + 3 prep + 3 stress + 5 refine + 10 polish\n",
        "    DIM = 256\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # SGD with momentum\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    HIGH_LR = 0.1\n",
        "    LOW_LR = 0.001\n",
        "    POLISH_LR = 0.0001\n",
        "    opt = torch.optim.SGD(model_params, lr=HIGH_LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Tight schedule with polish phase\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        prep=3,\n",
        "        stress=3,\n",
        "        refine=5,\n",
        "        polish=10,        # 10 more epochs to polish\n",
        "        max_dropout=0.85,\n",
        "        min_dropout=0.2,\n",
        "        polish_dropout=0.1,  # Low dropout for polish\n",
        "        high_lr=HIGH_LR,\n",
        "        low_lr=LOW_LR,\n",
        "        polish_lr=POLISH_LR,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={HIGH_LR}, momentum=0.9)\")\n",
        "    print(f\"Dual-head: main (w/ expert) + standalone (towers only), weight=0.5\")\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PREP:    E{sched.warmup+1:02d}-E{sched.prep_end:02d}\")\n",
        "    print(f\"  STRESS:  E{sched.prep_end+1:02d}-E{sched.stress_end:02d} (dropout 0→{sched.max_dropout}, LR={HIGH_LR})\")\n",
        "    print(f\"  REFINE:  E{sched.stress_end+1:02d}-E{sched.refine_end:02d} (dropout {sched.max_dropout}→{sched.min_dropout}, LR={LOW_LR})\")\n",
        "    print(f\"  POLISH:  E{sched.refine_end+1:02d}-E{sched.polish_end:02d} (dropout={sched.polish_dropout}, LR={POLISH_LR})\")\n",
        "    print(f\"Markers: * = best main, ^ = best standalone\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    best_standalone = 0\n",
        "    current_lr = HIGH_LR\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        # LR snap at phase transitions\n",
        "        new_lr = sched.get_lr(epoch)\n",
        "        if new_lr != current_lr:\n",
        "            print(f\"\\n*** LR SNAP: {current_lr:.4f} → {new_lr:.4f} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = new_lr\n",
        "            current_lr = new_lr\n",
        "\n",
        "        loss, train_acc, train_solo = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "        test_solo = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        solo_marker = \" ^\" if test_solo > best_standalone else \"\"\n",
        "        best = max(best, test_acc)\n",
        "        best_standalone = max(best_standalone, test_solo)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | lr={current_lr:.4f} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | solo={test_solo:.1f}%{solo_marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test main head with full expert\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(1.0, 0.0)\n",
        "    final_main = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone head (towers only - the whole point!)\n",
        "    final_standalone = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best main: {best:.2f}%\")\n",
        "    print(f\"Best standalone: {best_standalone:.2f}%\")\n",
        "    print(f\"Final main (w/ expert): {final_main:.2f}%\")\n",
        "    print(f\"Final standalone (towers only): {final_standalone:.2f}%\")\n",
        "    print(f\"Tower retention: {final_standalone/final_main*100:.1f}%\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxoqUHN9-mp5",
        "outputId": "3cfb8f6e-a559-464a-a7a2-336bb78a7561"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 18,110,978\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "Dual-head: main (w/ expert) + standalone (towers only), weight=0.5\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PREP:    E04-E06\n",
            "  STRESS:  E07-E09 (dropout 0→0.85, LR=0.1)\n",
            "  REFINE:  E10-E14 (dropout 0.85→0.2, LR=0.001)\n",
            "  POLISH:  E15-E24 (dropout=0.1, LR=0.0001)\n",
            "Markers: * = best main, ^ = best standalone\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33    | lr=0.1000 | loss=3.2778 | train=74.2% | test=85.0% * | solo=6.4% ^ | 132.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67    | lr=0.1000 | loss=2.6174 | train=87.0% | test=86.4% * | solo=8.6% ^ | 17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00    | lr=0.1000 | loss=2.3943 | train=90.5% | test=87.4% * | solo=11.1% ^ | 15.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PREP           | lr=0.1000 | loss=2.2701 | train=92.5% | test=87.6% * | solo=11.8% ^ | 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PREP           | lr=0.1000 | loss=2.1827 | train=93.7% | test=87.7% * | solo=14.4% ^ | 15.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PREP           | lr=0.1000 | loss=2.1040 | train=94.8% | test=87.4% | solo=15.8% ^ | 15.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | STRESS d=0.00  | lr=0.1000 | loss=2.0526 | train=95.6% | test=87.5% | solo=17.3% ^ | 15.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | STRESS d=0.28  | lr=0.1000 | loss=2.0284 | train=95.4% | test=87.4% | solo=17.9% ^ | 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | STRESS d=0.57  | lr=0.1000 | loss=2.0474 | train=94.1% | test=87.8% * | solo=19.0% ^ | 17.4s\n",
            "\n",
            "*** LR SNAP: 0.1000 → 0.0010 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | REFINE d=0.85  | lr=0.0010 | loss=2.2687 | train=87.9% | test=88.4% * | solo=21.1% ^ | 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | REFINE d=0.72  | lr=0.0010 | loss=1.9291 | train=95.3% | test=88.5% * | solo=21.8% ^ | 17.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | REFINE d=0.59  | lr=0.0010 | loss=1.8659 | train=96.8% | test=88.7% * | solo=22.2% ^ | 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | REFINE d=0.46  | lr=0.0010 | loss=1.8388 | train=97.4% | test=88.7% | solo=22.6% ^ | 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | REFINE d=0.33  | lr=0.0010 | loss=1.8258 | train=97.8% | test=88.7% | solo=22.7% ^ | 17.1s\n",
            "\n",
            "*** LR SNAP: 0.0010 → 0.0001 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | POLISH d=0.10  | lr=0.0001 | loss=1.8134 | train=98.1% | test=88.7% | solo=22.8% ^ | 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | POLISH d=0.10  | lr=0.0001 | loss=1.8146 | train=98.1% | test=88.7% | solo=22.7% | 16.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | POLISH d=0.10  | lr=0.0001 | loss=1.8120 | train=98.2% | test=88.7% | solo=22.7% | 15.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | POLISH d=0.10  | lr=0.0001 | loss=1.8149 | train=98.1% | test=88.7% | solo=22.8% | 19.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | POLISH d=0.10  | lr=0.0001 | loss=1.8107 | train=98.2% | test=88.7% | solo=22.8% ^ | 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | POLISH d=0.10  | lr=0.0001 | loss=1.8083 | train=98.2% | test=88.7% | solo=22.8% | 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | POLISH d=0.10  | lr=0.0001 | loss=1.8094 | train=98.2% | test=88.6% | solo=22.8% | 15.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | POLISH d=0.10  | lr=0.0001 | loss=1.8054 | train=98.2% | test=88.6% | solo=22.9% ^ | 16.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | POLISH d=0.10  | lr=0.0001 | loss=1.8086 | train=98.2% | test=88.6% | solo=22.8% | 15.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | POLISH d=0.10  | lr=0.0001 | loss=1.8036 | train=98.2% | test=88.6% | solo=22.7% | 15.9s\n",
            "\n",
            "======================================================================\n",
            "Best main: 88.68%\n",
            "Best standalone: 22.85%\n",
            "Final main (w/ expert): 88.63%\n",
            "Final standalone (towers only): 22.74%\n",
            "Tower retention: 25.7%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8000 head experiment - massive collective"
      ],
      "metadata": {
        "id": "-2_GU3rHCKub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Bidirectional dropout schedule with LR snaps.\n",
        "\n",
        "    Phase 1 (high LR=0.1): Increase dropout to force standalone learning\n",
        "        - WARMUP:  E01-E03 (scale 0→1)\n",
        "        - PREP:    E04-E06 (full expert, no dropout)\n",
        "        - STRESS:  E07-E09 (dropout 0→0.85, high LR)\n",
        "\n",
        "    Phase 2 (low LR=0.001): Decrease dropout to refine with expert\n",
        "        - REFINE:  E10-E14 (dropout 0.85→0.2)\n",
        "\n",
        "    Phase 3 (ultra low LR=0.0001): Polish with minimal dropout\n",
        "        - POLISH:  E15-E24 (dropout 0.1, ultra low LR)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        prep: int = 3,\n",
        "        stress: int = 3,\n",
        "        refine: int = 5,\n",
        "        polish: int = 10,\n",
        "        max_dropout: float = 0.85,\n",
        "        min_dropout: float = 0.2,\n",
        "        polish_dropout: float = 0.1,\n",
        "        high_lr: float = 0.1,\n",
        "        low_lr: float = 0.001,\n",
        "        polish_lr: float = 0.0001,\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.prep_end = warmup + prep\n",
        "        self.stress_end = self.prep_end + stress\n",
        "        self.refine_end = self.stress_end + refine\n",
        "        self.polish_end = self.refine_end + polish\n",
        "\n",
        "        self.max_dropout = max_dropout\n",
        "        self.min_dropout = min_dropout\n",
        "        self.polish_dropout = polish_dropout\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.polish_lr = polish_lr\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout).\"\"\"\n",
        "        if epoch < self.warmup:\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.prep_end:\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            progress = (epoch - self.prep_end) / (self.stress_end - self.prep_end)\n",
        "            dropout = progress * self.max_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        elif epoch < self.refine_end:\n",
        "            progress = (epoch - self.stress_end) / (self.refine_end - self.stress_end)\n",
        "            dropout = self.max_dropout - progress * (self.max_dropout - self.min_dropout)\n",
        "            return 1.0, dropout\n",
        "\n",
        "        else:\n",
        "            # POLISH: low dropout, ultra low LR\n",
        "            return 1.0, self.polish_dropout\n",
        "\n",
        "    def get_lr(self, epoch: int) -> float:\n",
        "        \"\"\"Get learning rate for epoch.\"\"\"\n",
        "        if epoch < self.stress_end:\n",
        "            return self.high_lr\n",
        "        elif epoch < self.refine_end:\n",
        "            return self.low_lr\n",
        "        else:\n",
        "            return self.polish_lr\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.prep_end:\n",
        "            return \"PREP\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        elif epoch < self.refine_end:\n",
        "            return f\"REFINE d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"POLISH d={drop:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Dual-head architecture:\n",
        "    - Main head: trans + conv + expert → full power classification\n",
        "    - Standalone head: trans + conv only → forces tower learning\n",
        "\n",
        "    Both heads trained simultaneously to ensure towers learn real features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === DUAL HEADS ===\n",
        "        # Main: trans + conv + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "        # Standalone: trans + conv = 2 * dim → 8000 hidden (massive capacity)\n",
        "        self.attach('standalone_head', ClassifierHead(f'{name}_standalone', dim * 2, num_classes, 8000))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Main classifier: all three\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        # Standalone classifier: towers only (no expert)\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        standalone_logits = self['standalone_head'](standalone)\n",
        "\n",
        "        return logits, standalone_logits\n",
        "\n",
        "    def forward_standalone(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"Forward using only towers (no expert) for evaluation.\"\"\"\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused\n",
        "\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        return self['standalone_head'](standalone)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False, standalone_weight=0.5):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, correct_standalone, total = 0, 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Dual outputs\n",
        "        logits, standalone_logits = router(img, exp)\n",
        "\n",
        "        # Combined loss: main + weighted standalone\n",
        "        main_loss = F.cross_entropy(logits, lbl)\n",
        "        standalone_loss = F.cross_entropy(standalone_logits, lbl)\n",
        "        loss = main_loss + standalone_weight * standalone_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        correct_standalone += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            acc=f'{100*correct/total:.1f}%',\n",
        "            solo=f'{100*correct_standalone/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss / total, 100 * correct / total, 100 * correct_standalone / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        logits, _ = router(img, exp)  # Main head\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_standalone(router, loader, device, is_compiled=False):\n",
        "    \"\"\"Evaluate using only the standalone head (no expert).\"\"\"\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        # Get both outputs, use standalone\n",
        "        _, standalone_logits = router(img, exp.to(device))\n",
        "        correct += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TowerStats:\n",
        "    \"\"\"Statistics for a single tower.\"\"\"\n",
        "    name: str\n",
        "    opinion_norm_mean: float = 0.0\n",
        "    opinion_norm_std: float = 0.0\n",
        "    opinion_var: float = 0.0\n",
        "    fingerprint: Optional[Tensor] = None\n",
        "    fusion_weight_mean: float = 0.0\n",
        "    ablation_accuracy: float = 0.0\n",
        "    contribution_score: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PairStats:\n",
        "    \"\"\"Statistics for a pos/neg tower pair.\"\"\"\n",
        "    pos_name: str\n",
        "    neg_name: str\n",
        "    opinion_correlation: float = 0.0\n",
        "    opinion_cosine: float = 0.0\n",
        "    complementarity: float = 0.0\n",
        "    combined_norm: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiagnosticReport:\n",
        "    \"\"\"Complete diagnostic report.\"\"\"\n",
        "    tower_stats: Dict[str, TowerStats] = field(default_factory=dict)\n",
        "    pair_stats: List[PairStats] = field(default_factory=list)\n",
        "    fingerprint_similarity_matrix: Optional[Tensor] = None\n",
        "    tower_correlation_matrix: Optional[Tensor] = None\n",
        "    fusion_weights: Optional[Tensor] = None\n",
        "    class_tower_preferences: Optional[Tensor] = None\n",
        "    full_accuracy: float = 0.0\n",
        "    per_tower_ablation: Dict[str, float] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Comprehensive tower diagnostic analyzer.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def collect_opinions(self, max_batches: int = 50) -> Dict:\n",
        "        self.router.eval()\n",
        "        self.base.set_expert_schedule(1.0, 0.0)\n",
        "\n",
        "        trans_opinions = defaultdict(list)\n",
        "        conv_opinions = defaultdict(list)\n",
        "        labels_all = []\n",
        "        fusion_weights_all = []\n",
        "\n",
        "        for i, (img, exp, lbl) in enumerate(self.loader):\n",
        "            if i >= max_batches:\n",
        "                break\n",
        "            img, exp, lbl = img.to(self.device), exp.to(self.device), lbl.to(self.device)\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            trans_out = self.base['trans_collective'](x)\n",
        "            for name, top in trans_out.opinions.items():\n",
        "                trans_opinions[name].append(top.opinion.cpu())\n",
        "            if trans_out.weights is not None:\n",
        "                fusion_weights_all.append(trans_out.weights.cpu())\n",
        "\n",
        "            conv_fused, conv_ops = self.base['conv_collective'](x)\n",
        "            for name, op in conv_ops.items():\n",
        "                conv_opinions[name].append(op.cpu())\n",
        "            labels_all.append(lbl.cpu())\n",
        "\n",
        "        return {\n",
        "            'trans': {k: torch.cat(v, dim=0) for k, v in trans_opinions.items()},\n",
        "            'conv': {k: torch.cat(v, dim=0) for k, v in conv_opinions.items()},\n",
        "            'labels': torch.cat(labels_all, dim=0),\n",
        "            'fusion_weights': torch.cat(fusion_weights_all, dim=0) if fusion_weights_all else None,\n",
        "        }\n",
        "\n",
        "    def analyze_tower_stats(self, opinions: Dict[str, Tensor]) -> Dict[str, TowerStats]:\n",
        "        stats = {}\n",
        "        for name, op in opinions.items():\n",
        "            norms = op.norm(dim=-1)\n",
        "            stats[name] = TowerStats(\n",
        "                name=name,\n",
        "                opinion_norm_mean=norms.mean().item(),\n",
        "                opinion_norm_std=norms.std().item(),\n",
        "                opinion_var=op.var(dim=0).mean().item(),\n",
        "            )\n",
        "        return stats\n",
        "\n",
        "    def analyze_fingerprints(self) -> Tuple[Dict[str, Tensor], Tensor, List[str]]:\n",
        "        fingerprints = {}\n",
        "        for name in self.base['trans_collective'].tower_names:\n",
        "            fingerprints[name] = self.base['trans_collective'][name].fingerprint.detach().cpu()\n",
        "        for name in self.base['conv_collective'].tower_names:\n",
        "            fingerprints[name] = self.base['conv_collective'][name].fingerprint.detach().cpu()\n",
        "\n",
        "        names = list(fingerprints.keys())\n",
        "        fps = torch.stack([fingerprints[name] for name in names])\n",
        "        fps = F.normalize(fps, dim=-1)\n",
        "        sim_matrix = fps @ fps.T\n",
        "        return fingerprints, sim_matrix, names\n",
        "\n",
        "    def analyze_pairs(self, opinions: Dict[str, Tensor]) -> List[PairStats]:\n",
        "        pairs = []\n",
        "        for pos_name in [n for n in opinions.keys() if '_pos' in n]:\n",
        "            neg_name = pos_name.replace('_pos', '_neg')\n",
        "            if neg_name not in opinions:\n",
        "                continue\n",
        "            pos_op, neg_op = opinions[pos_name], opinions[neg_name]\n",
        "            pos_flat, neg_flat = pos_op.reshape(-1), neg_op.reshape(-1)\n",
        "            pos_c, neg_c = pos_flat - pos_flat.mean(), neg_flat - neg_flat.mean()\n",
        "            corr = (pos_c * neg_c).sum() / (pos_c.norm() * neg_c.norm() + 1e-8)\n",
        "            cosine = F.cosine_similarity(pos_op, neg_op, dim=-1).mean()\n",
        "            pairs.append(PairStats(\n",
        "                pos_name=pos_name, neg_name=neg_name,\n",
        "                opinion_correlation=corr.item(), opinion_cosine=cosine.item(),\n",
        "                complementarity=(1.0 - cosine.abs()).item(),\n",
        "                combined_norm=(pos_op + neg_op).norm(dim=-1).mean().item(),\n",
        "            ))\n",
        "        return pairs\n",
        "\n",
        "    def compute_tower_correlations(self, opinions: Dict[str, Tensor]) -> Tuple[Tensor, List[str]]:\n",
        "        names = list(opinions.keys())\n",
        "        ops = torch.stack([opinions[name] for name in names])\n",
        "        ops_flat = ops.reshape(len(names), -1)\n",
        "        ops_c = ops_flat - ops_flat.mean(dim=1, keepdim=True)\n",
        "        norms = ops_c.norm(dim=1, keepdim=True)\n",
        "        return (ops_c @ ops_c.T) / (norms @ norms.T + 1e-8), names\n",
        "\n",
        "    def analyze_class_preferences(self, opinions: Dict[str, Tensor], labels: Tensor, num_classes: int = 100):\n",
        "        names = list(opinions.keys())\n",
        "        ops = torch.stack([opinions[name] for name in names], dim=1)\n",
        "        norms = ops.norm(dim=-1)\n",
        "        class_prefs = torch.zeros(num_classes, len(names))\n",
        "        class_counts = torch.zeros(num_classes)\n",
        "        for i, lbl in enumerate(labels):\n",
        "            class_prefs[lbl] += norms[i]\n",
        "            class_counts[lbl] += 1\n",
        "        return class_prefs / (class_counts.unsqueeze(1) + 1e-8), names\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ablation_study(self, max_batches: int = 20) -> Dict[str, float]:\n",
        "        self.router.eval()\n",
        "        self.base.set_expert_schedule(1.0, 0.0)\n",
        "        all_towers = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "\n",
        "        total, correct_full = 0, 0\n",
        "        correct_ablated = {name: 0 for name in all_towers}\n",
        "\n",
        "        for i, (img, exp, lbl) in enumerate(self.loader):\n",
        "            if i >= max_batches:\n",
        "                break\n",
        "            img, exp, lbl = img.to(self.device), exp.to(self.device), lbl.to(self.device)\n",
        "            total += lbl.size(0)\n",
        "\n",
        "            logits, _ = self.router(img, exp)\n",
        "            correct_full += logits.argmax(1).eq(lbl).sum().item()\n",
        "\n",
        "            x = self.base['patch_embed'](img)\n",
        "            trans_out = self.base['trans_collective'](x)\n",
        "            conv_fused, conv_ops = self.base['conv_collective'](x)\n",
        "            expert = self.base['expert'](exp)\n",
        "\n",
        "            for ablate_name in all_towers:\n",
        "                trans_mod = [torch.zeros_like(trans_out.opinions[n].opinion) if n == ablate_name\n",
        "                            else trans_out.opinions[n].opinion for n in self.base['trans_collective'].tower_names]\n",
        "                conv_mod = [torch.zeros_like(conv_ops[n]) if n == ablate_name\n",
        "                           else conv_ops[n] for n in self.base['conv_collective'].tower_names]\n",
        "                trans_fused = self.base['trans_collective']['fusion'](*trans_mod)\n",
        "                conv_fused_m = self.base['conv_collective']['fusion'](*conv_mod)\n",
        "                logits_m = self.base['classifier'](torch.cat([trans_fused, conv_fused_m, expert], dim=-1))\n",
        "                correct_ablated[ablate_name] += logits_m.argmax(1).eq(lbl).sum().item()\n",
        "\n",
        "        results = {'full': 100 * correct_full / total}\n",
        "        results.update({n: 100 * correct_ablated[n] / total for n in all_towers})\n",
        "        return results\n",
        "\n",
        "    def full_analysis(self, max_batches: int = 50) -> DiagnosticReport:\n",
        "        print(\"Collecting tower opinions...\")\n",
        "        data = self.collect_opinions(max_batches)\n",
        "        all_opinions = {**data['trans'], **data['conv']}\n",
        "\n",
        "        print(\"Analyzing tower statistics...\")\n",
        "        tower_stats = self.analyze_tower_stats(all_opinions)\n",
        "\n",
        "        print(\"Analyzing fingerprints...\")\n",
        "        fingerprints, fp_sim_matrix, fp_names = self.analyze_fingerprints()\n",
        "        for name, fp in fingerprints.items():\n",
        "            if name in tower_stats:\n",
        "                tower_stats[name].fingerprint = fp\n",
        "\n",
        "        print(\"Analyzing pos/neg pairs...\")\n",
        "        pair_stats = self.analyze_pairs(all_opinions)\n",
        "\n",
        "        print(\"Computing tower correlations...\")\n",
        "        corr_matrix, _ = self.compute_tower_correlations(all_opinions)\n",
        "\n",
        "        print(\"Analyzing class preferences...\")\n",
        "        class_prefs, _ = self.analyze_class_preferences(all_opinions, data['labels'])\n",
        "\n",
        "        print(\"Running ablation study...\")\n",
        "        ablation = self.ablation_study(max_batches=20)\n",
        "\n",
        "        full_acc = ablation['full']\n",
        "        for name, stats in tower_stats.items():\n",
        "            if name in ablation:\n",
        "                stats.ablation_accuracy = ablation[name]\n",
        "                stats.contribution_score = full_acc - ablation[name]\n",
        "\n",
        "        if data['fusion_weights'] is not None:\n",
        "            fw_mean = data['fusion_weights'].mean(dim=0)\n",
        "            for i, name in enumerate(self.base['trans_collective'].tower_names):\n",
        "                if name in tower_stats:\n",
        "                    tower_stats[name].fusion_weight_mean = fw_mean[i].item()\n",
        "\n",
        "        return DiagnosticReport(\n",
        "            tower_stats=tower_stats, pair_stats=pair_stats,\n",
        "            fingerprint_similarity_matrix=fp_sim_matrix,\n",
        "            tower_correlation_matrix=corr_matrix,\n",
        "            fusion_weights=data['fusion_weights'].mean(dim=0) if data['fusion_weights'] is not None else None,\n",
        "            class_tower_preferences=class_prefs,\n",
        "            full_accuracy=full_acc, per_tower_ablation=ablation,\n",
        "        )\n",
        "\n",
        "    def print_report(self, report: DiagnosticReport):\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TOWER DIAGNOSTIC REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nOVERALL ACCURACY\\n{'='*40}\")\n",
        "        print(f\"Full model: {report.full_accuracy:.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPER-TOWER STATISTICS\\n{'='*40}\")\n",
        "        print(f\"{'Tower':<20} {'Norm μ':>8} {'Norm σ':>8} {'Var':>8} {'Contrib':>8} {'Ablated':>8}\")\n",
        "        print(\"-\" * 72)\n",
        "        for ts in sorted(report.tower_stats.values(), key=lambda x: x.contribution_score, reverse=True):\n",
        "            print(f\"{ts.name:<20} {ts.opinion_norm_mean:>8.3f} {ts.opinion_norm_std:>8.3f} \"\n",
        "                  f\"{ts.opinion_var:>8.4f} {ts.contribution_score:>+7.2f}% {ts.ablation_accuracy:>7.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPOS/NEG PAIR DYNAMICS\\n{'='*40}\")\n",
        "        print(f\"{'Pair':<25} {'Corr':>8} {'Cosine':>8} {'Compl':>8} {'CombNorm':>8}\")\n",
        "        print(\"-\" * 65)\n",
        "        for ps in report.pair_stats:\n",
        "            print(f\"{ps.pos_name.replace('_pos', ''):<25} {ps.opinion_correlation:>8.3f} {ps.opinion_cosine:>8.3f} \"\n",
        "                  f\"{ps.complementarity:>8.3f} {ps.combined_norm:>8.3f}\")\n",
        "\n",
        "        if report.fusion_weights is not None:\n",
        "            print(f\"\\n{'='*40}\\nFUSION WEIGHTS (Transformer)\\n{'='*40}\")\n",
        "            for i, name in enumerate(self.base['trans_collective'].tower_names):\n",
        "                w = report.fusion_weights[i].item()\n",
        "                print(f\"{name:<20} {w:.3f} {'█' * int(w * 50)}\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nTOP 5 CONTRIBUTORS\\n{'='*40}\")\n",
        "        contrib = sorted([(n, report.full_accuracy - a) for n, a in report.per_tower_ablation.items() if n != 'full'],\n",
        "                        key=lambda x: x[1], reverse=True)[:5]\n",
        "        for name, drop in contrib:\n",
        "            print(f\"{name:<20} -{drop:.2f}% when removed\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPOTENTIALLY REDUNDANT\\n{'='*40}\")\n",
        "        redundant = [(n, report.full_accuracy - a) for n, a in report.per_tower_ablation.items()\n",
        "                     if n != 'full' and abs(report.full_accuracy - a) < 0.1]\n",
        "        if redundant:\n",
        "            for name, drop in redundant:\n",
        "                print(f\"{name:<20} {drop:+.2f}% (negligible)\")\n",
        "        else:\n",
        "            print(\"None - all towers contribute\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nFINGERPRINT SIMILARITY\\n{'='*40}\")\n",
        "        fp_sim = report.fingerprint_similarity_matrix\n",
        "        fp_names = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "        sims = [(fp_names[i], fp_names[j], fp_sim[i, j].item())\n",
        "                for i in range(len(fp_names)) for j in range(i+1, len(fp_names))]\n",
        "        sims.sort(key=lambda x: x[2], reverse=True)\n",
        "        print(\"Most similar:\")\n",
        "        for n1, n2, s in sims[:5]:\n",
        "            print(f\"  {n1:<18} <-> {n2:<18} : {s:.3f}\")\n",
        "        print(\"Most different:\")\n",
        "        for n1, n2, s in sims[-5:]:\n",
        "            print(f\"  {n1:<18} <-> {n2:<18} : {s:.3f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def plot_report(self, report: DiagnosticReport, save_path: str = None):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Tower Diagnostic Report', fontsize=14, fontweight='bold')\n",
        "\n",
        "        fp_names = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "\n",
        "        # 1. Tower Contribution\n",
        "        ax = axes[0, 0]\n",
        "        names = [n.replace('_', '\\n') for n in report.per_tower_ablation.keys() if n != 'full']\n",
        "        contribs = [report.full_accuracy - a for n, a in report.per_tower_ablation.items() if n != 'full']\n",
        "        colors = ['steelblue' if '_pos' in n else 'coral' for n in report.per_tower_ablation.keys() if n != 'full']\n",
        "        ax.bar(range(len(names)), contribs, color=colors)\n",
        "        ax.set_xticks(range(len(names)))\n",
        "        ax.set_xticklabels(names, fontsize=7, rotation=45, ha='right')\n",
        "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        ax.set_ylabel('Accuracy Drop (%)')\n",
        "        ax.set_title('Tower Contribution')\n",
        "\n",
        "        # 2. Opinion Norms\n",
        "        ax = axes[0, 1]\n",
        "        norms = [ts.opinion_norm_mean for ts in report.tower_stats.values()]\n",
        "        stds = [ts.opinion_norm_std for ts in report.tower_stats.values()]\n",
        "        ax.bar(range(len(norms)), norms, yerr=stds, capsize=3, color='teal', alpha=0.7)\n",
        "        ax.set_xticks(range(len(norms)))\n",
        "        ax.set_xticklabels([ts.name.replace('_', '\\n') for ts in report.tower_stats.values()], fontsize=7, rotation=45, ha='right')\n",
        "        ax.set_ylabel('Opinion Norm')\n",
        "        ax.set_title('Tower Opinion Magnitudes')\n",
        "\n",
        "        # 3. Fingerprint Similarity\n",
        "        ax = axes[0, 2]\n",
        "        im = ax.imshow(report.fingerprint_similarity_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "        ax.set_xticks(range(len(fp_names)))\n",
        "        ax.set_yticks(range(len(fp_names)))\n",
        "        ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=90)\n",
        "        ax.set_yticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6)\n",
        "        ax.set_title('Fingerprint Similarity')\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "        # 4. Opinion Correlation\n",
        "        ax = axes[1, 0]\n",
        "        im = ax.imshow(report.tower_correlation_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "        ax.set_xticks(range(len(fp_names)))\n",
        "        ax.set_yticks(range(len(fp_names)))\n",
        "        ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=90)\n",
        "        ax.set_yticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6)\n",
        "        ax.set_title('Opinion Correlation')\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "        # 5. Pos/Neg Pair Dynamics\n",
        "        ax = axes[1, 1]\n",
        "        pair_names = [ps.pos_name.replace('_pos', '') for ps in report.pair_stats]\n",
        "        x = np.arange(len(pair_names))\n",
        "        ax.bar(x - 0.25, [ps.opinion_correlation for ps in report.pair_stats], 0.25, label='Corr', color='steelblue')\n",
        "        ax.bar(x, [ps.opinion_cosine for ps in report.pair_stats], 0.25, label='Cosine', color='coral')\n",
        "        ax.bar(x + 0.25, [ps.complementarity for ps in report.pair_stats], 0.25, label='Compl', color='seagreen')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(pair_names, rotation=45, ha='right')\n",
        "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        ax.set_title('Pos/Neg Pair Dynamics')\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "        # 6. Fusion Weights or Class Variance\n",
        "        ax = axes[1, 2]\n",
        "        if report.fusion_weights is not None:\n",
        "            trans_names = list(self.base['trans_collective'].tower_names)\n",
        "            colors = ['steelblue' if '_pos' in n else 'coral' for n in trans_names]\n",
        "            ax.barh(range(len(trans_names)), report.fusion_weights.numpy(), color=colors)\n",
        "            ax.set_yticks(range(len(trans_names)))\n",
        "            ax.set_yticklabels(trans_names, fontsize=8)\n",
        "            ax.set_title('Fusion Weights')\n",
        "            ax.invert_yaxis()\n",
        "        else:\n",
        "            var = report.class_tower_preferences.var(dim=0).numpy()\n",
        "            ax.bar(range(len(fp_names)), var, color='purple', alpha=0.7)\n",
        "            ax.set_xticks(range(len(fp_names)))\n",
        "            ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=45, ha='right')\n",
        "            ax.set_title('Class Preference Variance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"Saved plot to {save_path}\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 24  # 3 warmup + 3 prep + 3 stress + 5 refine + 10 polish\n",
        "    DIM = 256\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # SGD with momentum\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    HIGH_LR = 0.1\n",
        "    LOW_LR = 0.001\n",
        "    POLISH_LR = 0.0001\n",
        "    opt = torch.optim.SGD(model_params, lr=HIGH_LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Tight schedule with polish phase\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        prep=3,\n",
        "        stress=3,\n",
        "        refine=5,\n",
        "        polish=10,        # 10 more epochs to polish\n",
        "        max_dropout=0.85,\n",
        "        min_dropout=0.2,\n",
        "        polish_dropout=0.1,  # Low dropout for polish\n",
        "        high_lr=HIGH_LR,\n",
        "        low_lr=LOW_LR,\n",
        "        polish_lr=POLISH_LR,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={HIGH_LR}, momentum=0.9)\")\n",
        "    print(f\"Dual-head: main (256 hidden) + standalone (8000 hidden), weight=0.5\")\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PREP:    E{sched.warmup+1:02d}-E{sched.prep_end:02d}\")\n",
        "    print(f\"  STRESS:  E{sched.prep_end+1:02d}-E{sched.stress_end:02d} (dropout 0→{sched.max_dropout}, LR={HIGH_LR})\")\n",
        "    print(f\"  REFINE:  E{sched.stress_end+1:02d}-E{sched.refine_end:02d} (dropout {sched.max_dropout}→{sched.min_dropout}, LR={LOW_LR})\")\n",
        "    if EPOCHS > sched.refine_end:\n",
        "        print(f\"  POLISH:  E{sched.refine_end+1:02d}-E{min(EPOCHS, sched.polish_end):02d} (dropout={sched.polish_dropout}, LR={POLISH_LR})\")\n",
        "    print(f\"Markers: * = best main, ^ = best standalone\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    best_standalone = 0\n",
        "    current_lr = HIGH_LR\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        # LR snap at phase transitions\n",
        "        new_lr = sched.get_lr(epoch)\n",
        "        if new_lr != current_lr:\n",
        "            print(f\"\\n*** LR SNAP: {current_lr:.4f} → {new_lr:.4f} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = new_lr\n",
        "            current_lr = new_lr\n",
        "\n",
        "        loss, train_acc, train_solo = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "        test_solo = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        solo_marker = \" ^\" if test_solo > best_standalone else \"\"\n",
        "        best = max(best, test_acc)\n",
        "        best_standalone = max(best_standalone, test_solo)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | lr={current_lr:.4f} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | solo={test_solo:.1f}%{solo_marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test main head with full expert\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(1.0, 0.0)\n",
        "    final_main = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone head (towers only - the whole point!)\n",
        "    final_standalone = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best main: {best:.2f}%\")\n",
        "    print(f\"Best standalone: {best_standalone:.2f}%\")\n",
        "    print(f\"Final main (w/ expert): {final_main:.2f}%\")\n",
        "    print(f\"Final standalone (towers only): {final_standalone:.2f}%\")\n",
        "    print(f\"Tower retention: {final_standalone/final_main*100:.1f}%\")\n",
        "\n",
        "    # === RUN DIAGNOSTICS ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RUNNING TOWER DIAGNOSTICS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "    report = diag.full_analysis(max_batches=50)\n",
        "    diag.print_report(report)\n",
        "\n",
        "    # Save plot\n",
        "    try:\n",
        "        fig = diag.plot_report(report, save_path='/mnt/user-data/outputs/tower_diagnostics.png')\n",
        "        print(\"\\nDiagnostic plot saved to /mnt/user-data/outputs/tower_diagnostics.png\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not generate plot: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASS-vKB3CKZa",
        "outputId": "e90548a3-9190-4571-ca58-602736d42912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 22,858,050\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "Dual-head: main (256 hidden) + standalone (8000 hidden), weight=0.5\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PREP:    E04-E06\n",
            "  STRESS:  E07-E09 (dropout 0→0.85, LR=0.1)\n",
            "  REFINE:  E10-E14 (dropout 0.85→0.2, LR=0.001)\n",
            "  POLISH:  E15-E24 (dropout=0.1, LR=0.0001)\n",
            "Markers: * = best main, ^ = best standalone\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | WARM s=0.33    | lr=0.1000 | loss=3.2984 | train=74.5% | test=85.5% * | solo=7.0% ^ | 95.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | WARM s=0.67    | lr=0.1000 | loss=2.6283 | train=86.5% | test=87.0% * | solo=10.1% ^ | 18.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | WARM s=1.00    | lr=0.1000 | loss=2.3512 | train=90.4% | test=87.4% * | solo=13.3% ^ | 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | PREP           | lr=0.1000 | loss=2.2079 | train=92.7% | test=87.6% * | solo=15.7% ^ | 16.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | PREP           | lr=0.1000 | loss=2.1114 | train=94.0% | test=87.6% | solo=16.6% ^ | 22.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | PREP           | lr=0.1000 | loss=2.0469 | train=94.8% | test=87.6% * | solo=17.2% ^ | 16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | STRESS d=0.00  | lr=0.1000 | loss=1.9800 | train=95.6% | test=87.4% | solo=20.6% ^ | 16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | STRESS d=0.28  | lr=0.1000 | loss=1.9551 | train=95.6% | test=87.3% | solo=21.4% ^ | 18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | STRESS d=0.57  | lr=0.1000 | loss=1.9735 | train=94.1% | test=87.7% * | solo=22.9% ^ | 18.9s\n",
            "\n",
            "*** LR SNAP: 0.1000 → 0.0010 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | REFINE d=0.85  | lr=0.0010 | loss=2.1715 | train=88.3% | test=88.5% * | solo=25.7% ^ | 18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | REFINE d=0.72  | lr=0.0010 | loss=1.8296 | train=95.6% | test=88.7% * | solo=26.3% ^ | 18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | REFINE d=0.59  | lr=0.0010 | loss=1.7722 | train=96.8% | test=88.7% | solo=26.7% ^ | 18.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | REFINE d=0.46  | lr=0.0010 | loss=1.7426 | train=97.5% | test=88.8% * | solo=26.9% ^ | 18.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | REFINE d=0.33  | lr=0.0010 | loss=1.7242 | train=97.9% | test=88.8% * | solo=26.8% | 18.8s\n",
            "\n",
            "*** LR SNAP: 0.0010 → 0.0001 ***\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | POLISH d=0.10  | lr=0.0001 | loss=1.7124 | train=98.2% | test=88.9% * | solo=26.8% | 18.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | POLISH d=0.10  | lr=0.0001 | loss=1.7127 | train=98.2% | test=88.9% * | solo=26.9% | 18.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | POLISH d=0.10  | lr=0.0001 | loss=1.7132 | train=98.2% | test=88.9% | solo=26.9% | 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | POLISH d=0.10  | lr=0.0001 | loss=1.7058 | train=98.2% | test=88.9% * | solo=26.9% | 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | POLISH d=0.10  | lr=0.0001 | loss=1.7146 | train=98.2% | test=88.9% * | solo=27.0% ^ | 17.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | POLISH d=0.10  | lr=0.0001 | loss=1.7100 | train=98.2% | test=88.9% | solo=27.0% | 17.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | POLISH d=0.10  | lr=0.0001 | loss=1.7061 | train=98.2% | test=88.9% | solo=27.1% ^ | 17.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | POLISH d=0.10  | lr=0.0001 | loss=1.7077 | train=98.3% | test=88.9% | solo=27.0% | 16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | POLISH d=0.10  | lr=0.0001 | loss=1.7022 | train=98.3% | test=88.9% | solo=27.1% | 16.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | POLISH d=0.10  | lr=0.0001 | loss=1.7063 | train=98.3% | test=88.8% | solo=27.1% | 16.9s\n",
            "\n",
            "======================================================================\n",
            "Best main: 88.92%\n",
            "Best standalone: 27.06%\n",
            "Final main (w/ expert): 88.85%\n",
            "Final standalone (towers only): 27.05%\n",
            "Tower retention: 30.4%\n",
            "\n",
            "======================================================================\n",
            "RUNNING TOWER DIAGNOSTICS\n",
            "======================================================================\n",
            "Collecting tower opinions...\n",
            "Analyzing tower statistics...\n",
            "Analyzing fingerprints...\n",
            "Analyzing pos/neg pairs...\n",
            "Computing tower correlations...\n",
            "Analyzing class preferences...\n",
            "Running ablation study...\n",
            "\n",
            "================================================================================\n",
            "TOWER DIAGNOSTIC REPORT\n",
            "================================================================================\n",
            "\n",
            "========================================\n",
            "OVERALL ACCURACY\n",
            "========================================\n",
            "Full model: 89.18%\n",
            "\n",
            "========================================\n",
            "PER-TOWER STATISTICS\n",
            "========================================\n",
            "Tower                  Norm μ   Norm σ      Var  Contrib  Ablated\n",
            "------------------------------------------------------------------------\n",
            "cantor_neg              9.981    1.735   0.3330   +0.12%   89.06%\n",
            "depth_pos               9.016    1.006   0.1256   +0.08%   89.10%\n",
            "helix_pos               4.296    1.310   0.0653   +0.04%   89.14%\n",
            "helix_neg               8.348    1.731   0.2217   +0.04%   89.14%\n",
            "cantor_pos              5.632    1.865   0.1273   +0.00%   89.18%\n",
            "beatrix_pos             6.212    2.153   0.1140   +0.00%   89.18%\n",
            "beatrix_neg             4.466    1.732   0.0637   +0.00%   89.18%\n",
            "simplex_pos             5.318    1.705   0.1022   +0.00%   89.18%\n",
            "depth_neg               9.307    1.403   0.1656   +0.00%   89.18%\n",
            "coarse_fine_pos         9.125    0.766   0.1376   +0.00%   89.18%\n",
            "simplex_neg             7.634    2.042   0.1829   -0.04%   89.22%\n",
            "frequency_pos           9.985    1.099   0.2188   -0.04%   89.22%\n",
            "coarse_fine_neg        10.166    1.888   0.2175   -0.04%   89.22%\n",
            "wide_resnet_neg         9.213    1.159   0.0978   -0.04%   89.22%\n",
            "frequency_neg          12.094    1.166   0.0772   -0.08%   89.26%\n",
            "wide_resnet_pos         8.111    0.827   0.1899   -0.08%   89.26%\n",
            "\n",
            "========================================\n",
            "POS/NEG PAIR DYNAMICS\n",
            "========================================\n",
            "Pair                          Corr   Cosine    Compl CombNorm\n",
            "-----------------------------------------------------------------\n",
            "cantor                       0.146    0.163    0.837   12.274\n",
            "beatrix                      0.290    0.303    0.697    8.729\n",
            "helix                       -0.007   -0.008    0.992    9.415\n",
            "simplex                      0.067    0.074    0.926    9.671\n",
            "depth                        0.489    0.483    0.517   15.825\n",
            "frequency                    0.437    0.414    0.586   18.660\n",
            "coarse_fine                  0.326    0.326    0.674   15.757\n",
            "wide_resnet                  0.273    0.256    0.744   13.772\n",
            "\n",
            "========================================\n",
            "TOP 5 CONTRIBUTORS\n",
            "========================================\n",
            "cantor_neg           -0.12% when removed\n",
            "depth_pos            -0.08% when removed\n",
            "helix_pos            -0.04% when removed\n",
            "helix_neg            -0.04% when removed\n",
            "cantor_pos           -0.00% when removed\n",
            "\n",
            "========================================\n",
            "POTENTIALLY REDUNDANT\n",
            "========================================\n",
            "cantor_pos           +0.00% (negligible)\n",
            "beatrix_pos          +0.00% (negligible)\n",
            "beatrix_neg          +0.00% (negligible)\n",
            "helix_pos            +0.04% (negligible)\n",
            "helix_neg            +0.04% (negligible)\n",
            "simplex_pos          +0.00% (negligible)\n",
            "simplex_neg          -0.04% (negligible)\n",
            "depth_pos            +0.08% (negligible)\n",
            "depth_neg            +0.00% (negligible)\n",
            "frequency_pos        -0.04% (negligible)\n",
            "frequency_neg        -0.08% (negligible)\n",
            "coarse_fine_pos      +0.00% (negligible)\n",
            "coarse_fine_neg      -0.04% (negligible)\n",
            "wide_resnet_pos      -0.08% (negligible)\n",
            "wide_resnet_neg      -0.04% (negligible)\n",
            "\n",
            "========================================\n",
            "FINGERPRINT SIMILARITY\n",
            "========================================\n",
            "Most similar:\n",
            "  cantor_neg         <-> beatrix_neg        : 1.000\n",
            "  cantor_pos         <-> beatrix_pos        : 1.000\n",
            "  cantor_neg         <-> depth_neg          : 0.992\n",
            "  beatrix_neg        <-> depth_neg          : 0.992\n",
            "  cantor_neg         <-> frequency_neg      : 0.991\n",
            "Most different:\n",
            "  depth_pos          <-> coarse_fine_neg    : -0.162\n",
            "  cantor_pos         <-> depth_pos          : -0.169\n",
            "  beatrix_pos        <-> depth_pos          : -0.169\n",
            "  frequency_pos      <-> coarse_fine_pos    : -0.201\n",
            "  depth_pos          <-> coarse_fine_pos    : -0.240\n",
            "================================================================================\n",
            "\n",
            "Could not generate plot: [Errno 2] No such file or directory: '/mnt/user-data/outputs/tower_diagnostics.png'\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1800x1200 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12 transformer towers, 8 conv towers, sgd mega run"
      ],
      "metadata": {
        "id": "XoCI65GKUhvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Hybrid Router (WideRouter Edition)\n",
        "=============================================\n",
        "\n",
        "Uses WideRouter-based collectives from tower builders for compile-optimized execution.\n",
        "Integrates EncoderDataComponent for staged caching with VRAM management.\n",
        "\n",
        "Architecture:\n",
        "    Image → PatchEmbed → [B, L, D]\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConfigurableCollective (WideRouter)\n",
        "    │  8 ConfigurableTowers → Fusion    │\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    ┌───────────────────────────────────┐\n",
        "    │  ConvTowerCollective (WideRouter) │\n",
        "    │  8 ConfigurableConvTowers → Fusion│\n",
        "    └───────────────────────────────────┘\n",
        "                ↓\n",
        "    [trans_fused + conv_fused + expert] → classifier\n",
        "\n",
        "Caching Workflow:\n",
        "    1. VisionCacher loads encoder → caches all images → unloads encoder\n",
        "    2. Training uses only cached latents (no vision encoder in VRAM)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from dataclasses import dataclass, field\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "\n",
        "        Workflow:\n",
        "        - Load encoder to VRAM\n",
        "        - Encode all images\n",
        "        - Save to disk\n",
        "        - Unload encoder (free VRAM for training)\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            # Use MultiVisionEncode for registry models\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            # Save and unload\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            # Direct HuggingFace loading for custom models\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "class ExpertProj(TorchComponent):\n",
        "    \"\"\"Expert projection with curriculum scheduling.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, expert_dim: int, out_dim: int):\n",
        "        super().__init__(name)\n",
        "        self.norm = nn.LayerNorm(expert_dim)\n",
        "        self.fc1 = nn.Linear(expert_dim, out_dim)\n",
        "        self.fc2 = nn.Linear(out_dim, out_dim)\n",
        "        self.register_buffer('scale', torch.tensor(1.0))\n",
        "        self.register_buffer('dropout', torch.tensor(0.0))\n",
        "\n",
        "    def set_schedule(self, scale: float, dropout: float):\n",
        "        self.scale.fill_(scale)\n",
        "        self.dropout.fill_(dropout)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = F.gelu(self.fc1(self.norm(x)))\n",
        "        x = self.fc2(x)\n",
        "        # Graph break here is fine - avoids recompiles when dropout changes\n",
        "        if self.training and self.dropout > 0:\n",
        "            mask = torch.bernoulli(torch.full_like(x, 1 - self.dropout.item()))\n",
        "            x = x * mask / (1 - self.dropout.item() + 1e-8)\n",
        "        return x * self.scale\n",
        "\n",
        "\n",
        "class ClassifierHead(TorchComponent):\n",
        "    \"\"\"Classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, num_classes: int, hidden: int = None):\n",
        "        super().__init__(name)\n",
        "        hidden = hidden or in_dim // 3\n",
        "        self.norm = nn.LayerNorm(in_dim)\n",
        "        self.fc1 = nn.Linear(in_dim, hidden)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.fc2 = nn.Linear(hidden, num_classes)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.fc2(self.drop(F.gelu(self.fc1(self.norm(x)))))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# EXPERT SCHEDULER\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertScheduler:\n",
        "    \"\"\"\n",
        "    Bidirectional dropout schedule with LR snaps.\n",
        "\n",
        "    Phase 1 (high LR=0.1): Increase dropout to force standalone learning\n",
        "        - WARMUP:  E01-E03 (scale 0→1)\n",
        "        - PREP:    E04-E06 (full expert, no dropout)\n",
        "        - STRESS:  E07-E09 (dropout 0→0.85, high LR)\n",
        "\n",
        "    Phase 2 (low LR=0.001): Decrease dropout to refine with expert\n",
        "        - REFINE:  E10-E14 (dropout 0.85→0.2)\n",
        "\n",
        "    Phase 3 (ultra low LR=0.0001): Polish with minimal dropout\n",
        "        - POLISH:  E15-E24 (dropout 0.1, ultra low LR)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        warmup: int = 3,\n",
        "        prep: int = 3,\n",
        "        stress: int = 3,\n",
        "        refine: int = 5,\n",
        "        polish: int = 10,\n",
        "        max_dropout: float = 0.85,\n",
        "        min_dropout: float = 0.2,\n",
        "        polish_dropout: float = 0.1,\n",
        "        high_lr: float = 0.1,\n",
        "        low_lr: float = 0.001,\n",
        "        polish_lr: float = 0.0001,\n",
        "    ):\n",
        "        self.warmup = warmup\n",
        "        self.prep_end = warmup + prep\n",
        "        self.stress_end = self.prep_end + stress\n",
        "        self.refine_end = self.stress_end + refine\n",
        "        self.polish_end = self.refine_end + polish\n",
        "\n",
        "        self.max_dropout = max_dropout\n",
        "        self.min_dropout = min_dropout\n",
        "        self.polish_dropout = polish_dropout\n",
        "        self.high_lr = high_lr\n",
        "        self.low_lr = low_lr\n",
        "        self.polish_lr = polish_lr\n",
        "\n",
        "    def __call__(self, epoch: int) -> Tuple[float, float]:\n",
        "        \"\"\"Returns (scale, dropout).\"\"\"\n",
        "        if epoch < self.warmup:\n",
        "            scale = (epoch + 1) / self.warmup\n",
        "            return scale, 0.0\n",
        "\n",
        "        elif epoch < self.prep_end:\n",
        "            return 1.0, 0.0\n",
        "\n",
        "        elif epoch < self.stress_end:\n",
        "            progress = (epoch - self.prep_end) / (self.stress_end - self.prep_end)\n",
        "            dropout = progress * self.max_dropout\n",
        "            return 1.0, dropout\n",
        "\n",
        "        elif epoch < self.refine_end:\n",
        "            progress = (epoch - self.stress_end) / (self.refine_end - self.stress_end)\n",
        "            dropout = self.max_dropout - progress * (self.max_dropout - self.min_dropout)\n",
        "            return 1.0, dropout\n",
        "\n",
        "        else:\n",
        "            # POLISH: low dropout, ultra low LR\n",
        "            return 1.0, self.polish_dropout\n",
        "\n",
        "    def get_lr(self, epoch: int) -> float:\n",
        "        \"\"\"Get learning rate for epoch.\"\"\"\n",
        "        if epoch < self.stress_end:\n",
        "            return self.high_lr\n",
        "        elif epoch < self.refine_end:\n",
        "            return self.low_lr\n",
        "        else:\n",
        "            return self.polish_lr\n",
        "\n",
        "    def phase_name(self, epoch: int) -> str:\n",
        "        \"\"\"Get phase name for logging.\"\"\"\n",
        "        scale, drop = self(epoch)\n",
        "        if epoch < self.warmup:\n",
        "            return f\"WARM s={scale:.2f}\"\n",
        "        elif epoch < self.prep_end:\n",
        "            return \"PREP\"\n",
        "        elif epoch < self.stress_end:\n",
        "            return f\"STRESS d={drop:.2f}\"\n",
        "        elif epoch < self.refine_end:\n",
        "            return f\"REFINE d={drop:.2f}\"\n",
        "        else:\n",
        "            return f\"POLISH d={drop:.2f}\"\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR HYBRID ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class CIFARHybridRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier using WideRouter collectives from tower builders.\n",
        "\n",
        "    Dual-head architecture:\n",
        "    - Main head: trans + conv + expert → full power classification\n",
        "    - Standalone head: trans + conv only → forces tower learning\n",
        "\n",
        "    Both heads trained simultaneously to ensure towers learn real features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'cifar_hybrid',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size  # 4 patches per side for CIFAR\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim,\n",
        "            'patch_size': patch_size,\n",
        "            'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'expert_dim': expert_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (WideRouter from builder) ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal'])\n",
        "\n",
        "        # Add 4 unsupervised standard RoPE towers with theta scale offsets\n",
        "        # These provide multi-scale frequency baselines without geometric structure\n",
        "        theta_scales = [0.25, 0.5, 0.75, 1.0]\n",
        "        for scale in theta_scales:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_{int(scale*100):03d}',\n",
        "                rope='standard',\n",
        "                address='standard',\n",
        "                inverted=False,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs,\n",
        "            dim=dim,\n",
        "            default_depth=trans_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (WideRouter from builder) ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs,\n",
        "            dim=dim,\n",
        "            default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            spatial_size=spatial,\n",
        "            name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT ===\n",
        "        self.attach('expert', ExpertProj(f'{name}_expert', expert_dim, dim))\n",
        "\n",
        "        # === DUAL HEADS ===\n",
        "        # Main: trans + conv + expert = 3 * dim\n",
        "        self.attach('classifier', ClassifierHead(f'{name}_head', dim * 3, num_classes, dim))\n",
        "        # Standalone: trans + conv = 2 * dim → 8000 hidden (massive capacity)\n",
        "        self.attach('standalone_head', ClassifierHead(f'{name}_standalone', dim * 2, num_classes, 8000))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    def set_expert_schedule(self, scale: float, dropout: float):\n",
        "        self['expert'].set_schedule(scale, dropout)\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor) -> Tuple[Tensor, Tensor]:\n",
        "        # Patch embed: [B, 3, 32, 32] -> [B, L, D]\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective (WideRouter)\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused  # [B, D]\n",
        "\n",
        "        # Conv collective (WideRouter)\n",
        "        conv_fused, _ = self['conv_collective'](x)  # [B, D]\n",
        "\n",
        "        # Expert pathway\n",
        "        expert = self['expert'](expert_latents)  # [B, D]\n",
        "\n",
        "        # Main classifier: all three\n",
        "        combined = torch.cat([trans_fused, conv_fused, expert], dim=-1)\n",
        "        logits = self['classifier'](combined)\n",
        "\n",
        "        # Standalone classifier: towers only (no expert)\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        standalone_logits = self['standalone_head'](standalone)\n",
        "\n",
        "        return logits, standalone_logits\n",
        "\n",
        "    def forward_standalone(self, images: Tensor) -> Tensor:\n",
        "        \"\"\"Forward using only towers (no expert) for evaluation.\"\"\"\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused\n",
        "\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "\n",
        "        standalone = torch.cat([trans_fused, conv_fused], dim=-1)\n",
        "        return self['standalone_head'](standalone)\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'CIFARHybridRouter':\n",
        "        \"\"\"Prepare collectives and compile the router.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, epoch, sched, is_compiled=False, standalone_weight=0.5):\n",
        "    router.train()\n",
        "    scale, drop = sched(epoch)\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(scale, drop)\n",
        "\n",
        "    total_loss, correct, correct_standalone, total = 0, 0, 0, 0\n",
        "    pbar = tqdm(loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "    for img, exp, lbl in pbar:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Dual outputs\n",
        "        logits, standalone_logits = router(img, exp)\n",
        "\n",
        "        # Combined loss: main + weighted standalone\n",
        "        main_loss = F.cross_entropy(logits, lbl)\n",
        "        standalone_loss = F.cross_entropy(standalone_logits, lbl)\n",
        "        loss = main_loss + standalone_weight * standalone_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        correct_standalone += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            acc=f'{100*correct/total:.1f}%',\n",
        "            solo=f'{100*correct_standalone/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss / total, 100 * correct / total, 100 * correct_standalone / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, expert_on=True, is_compiled=False):\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "    base.set_expert_schedule(1.0 if expert_on else 0.0, 0.0 if expert_on else 1.0)\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "        logits, _ = router(img, exp)  # Main head\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_standalone(router, loader, device, is_compiled=False):\n",
        "    \"\"\"Evaluate using only the standalone head (no expert).\"\"\"\n",
        "    router.eval()\n",
        "    base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    correct, total = 0, 0\n",
        "    for img, exp, lbl in loader:\n",
        "        img, lbl = img.to(device), lbl.to(device)\n",
        "        # Get both outputs, use standalone\n",
        "        _, standalone_logits = router(img, exp.to(device))\n",
        "        correct += standalone_logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS\n",
        "# =============================================================================\n",
        "\n",
        "@dataclass\n",
        "class TowerStats:\n",
        "    \"\"\"Statistics for a single tower.\"\"\"\n",
        "    name: str\n",
        "    opinion_norm_mean: float = 0.0\n",
        "    opinion_norm_std: float = 0.0\n",
        "    opinion_var: float = 0.0\n",
        "    fingerprint: Optional[Tensor] = None\n",
        "    fusion_weight_mean: float = 0.0\n",
        "    ablation_accuracy: float = 0.0\n",
        "    contribution_score: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PairStats:\n",
        "    \"\"\"Statistics for a pos/neg tower pair.\"\"\"\n",
        "    pos_name: str\n",
        "    neg_name: str\n",
        "    opinion_correlation: float = 0.0\n",
        "    opinion_cosine: float = 0.0\n",
        "    complementarity: float = 0.0\n",
        "    combined_norm: float = 0.0\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DiagnosticReport:\n",
        "    \"\"\"Complete diagnostic report.\"\"\"\n",
        "    tower_stats: Dict[str, TowerStats] = field(default_factory=dict)\n",
        "    pair_stats: List[PairStats] = field(default_factory=list)\n",
        "    fingerprint_similarity_matrix: Optional[Tensor] = None\n",
        "    tower_correlation_matrix: Optional[Tensor] = None\n",
        "    fusion_weights: Optional[Tensor] = None\n",
        "    class_tower_preferences: Optional[Tensor] = None\n",
        "    full_accuracy: float = 0.0\n",
        "    per_tower_ablation: Dict[str, float] = field(default_factory=dict)\n",
        "\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Comprehensive tower diagnostic analyzer.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def collect_opinions(self, max_batches: int = 50) -> Dict:\n",
        "        self.router.eval()\n",
        "        self.base.set_expert_schedule(1.0, 0.0)\n",
        "\n",
        "        trans_opinions = defaultdict(list)\n",
        "        conv_opinions = defaultdict(list)\n",
        "        labels_all = []\n",
        "        fusion_weights_all = []\n",
        "\n",
        "        for i, (img, exp, lbl) in enumerate(self.loader):\n",
        "            if i >= max_batches:\n",
        "                break\n",
        "            img, exp, lbl = img.to(self.device), exp.to(self.device), lbl.to(self.device)\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            trans_out = self.base['trans_collective'](x)\n",
        "            for name, top in trans_out.opinions.items():\n",
        "                trans_opinions[name].append(top.opinion.cpu())\n",
        "            if trans_out.weights is not None:\n",
        "                fusion_weights_all.append(trans_out.weights.cpu())\n",
        "\n",
        "            conv_fused, conv_ops = self.base['conv_collective'](x)\n",
        "            for name, op in conv_ops.items():\n",
        "                conv_opinions[name].append(op.cpu())\n",
        "            labels_all.append(lbl.cpu())\n",
        "\n",
        "        return {\n",
        "            'trans': {k: torch.cat(v, dim=0) for k, v in trans_opinions.items()},\n",
        "            'conv': {k: torch.cat(v, dim=0) for k, v in conv_opinions.items()},\n",
        "            'labels': torch.cat(labels_all, dim=0),\n",
        "            'fusion_weights': torch.cat(fusion_weights_all, dim=0) if fusion_weights_all else None,\n",
        "        }\n",
        "\n",
        "    def analyze_tower_stats(self, opinions: Dict[str, Tensor]) -> Dict[str, TowerStats]:\n",
        "        stats = {}\n",
        "        for name, op in opinions.items():\n",
        "            norms = op.norm(dim=-1)\n",
        "            stats[name] = TowerStats(\n",
        "                name=name,\n",
        "                opinion_norm_mean=norms.mean().item(),\n",
        "                opinion_norm_std=norms.std().item(),\n",
        "                opinion_var=op.var(dim=0).mean().item(),\n",
        "            )\n",
        "        return stats\n",
        "\n",
        "    def analyze_fingerprints(self) -> Tuple[Dict[str, Tensor], Tensor, List[str]]:\n",
        "        fingerprints = {}\n",
        "        for name in self.base['trans_collective'].tower_names:\n",
        "            fingerprints[name] = self.base['trans_collective'][name].fingerprint.detach().cpu()\n",
        "        for name in self.base['conv_collective'].tower_names:\n",
        "            fingerprints[name] = self.base['conv_collective'][name].fingerprint.detach().cpu()\n",
        "\n",
        "        names = list(fingerprints.keys())\n",
        "        fps = torch.stack([fingerprints[name] for name in names])\n",
        "        fps = F.normalize(fps, dim=-1)\n",
        "        sim_matrix = fps @ fps.T\n",
        "        return fingerprints, sim_matrix, names\n",
        "\n",
        "    def analyze_pairs(self, opinions: Dict[str, Tensor]) -> List[PairStats]:\n",
        "        pairs = []\n",
        "        for pos_name in [n for n in opinions.keys() if '_pos' in n]:\n",
        "            neg_name = pos_name.replace('_pos', '_neg')\n",
        "            if neg_name not in opinions:\n",
        "                continue\n",
        "            pos_op, neg_op = opinions[pos_name], opinions[neg_name]\n",
        "            pos_flat, neg_flat = pos_op.reshape(-1), neg_op.reshape(-1)\n",
        "            pos_c, neg_c = pos_flat - pos_flat.mean(), neg_flat - neg_flat.mean()\n",
        "            corr = (pos_c * neg_c).sum() / (pos_c.norm() * neg_c.norm() + 1e-8)\n",
        "            cosine = F.cosine_similarity(pos_op, neg_op, dim=-1).mean()\n",
        "            pairs.append(PairStats(\n",
        "                pos_name=pos_name, neg_name=neg_name,\n",
        "                opinion_correlation=corr.item(), opinion_cosine=cosine.item(),\n",
        "                complementarity=(1.0 - cosine.abs()).item(),\n",
        "                combined_norm=(pos_op + neg_op).norm(dim=-1).mean().item(),\n",
        "            ))\n",
        "        return pairs\n",
        "\n",
        "    def compute_tower_correlations(self, opinions: Dict[str, Tensor]) -> Tuple[Tensor, List[str]]:\n",
        "        names = list(opinions.keys())\n",
        "        ops = torch.stack([opinions[name] for name in names])\n",
        "        ops_flat = ops.reshape(len(names), -1)\n",
        "        ops_c = ops_flat - ops_flat.mean(dim=1, keepdim=True)\n",
        "        norms = ops_c.norm(dim=1, keepdim=True)\n",
        "        return (ops_c @ ops_c.T) / (norms @ norms.T + 1e-8), names\n",
        "\n",
        "    def analyze_class_preferences(self, opinions: Dict[str, Tensor], labels: Tensor, num_classes: int = 100):\n",
        "        names = list(opinions.keys())\n",
        "        ops = torch.stack([opinions[name] for name in names], dim=1)\n",
        "        norms = ops.norm(dim=-1)\n",
        "        class_prefs = torch.zeros(num_classes, len(names))\n",
        "        class_counts = torch.zeros(num_classes)\n",
        "        for i, lbl in enumerate(labels):\n",
        "            class_prefs[lbl] += norms[i]\n",
        "            class_counts[lbl] += 1\n",
        "        return class_prefs / (class_counts.unsqueeze(1) + 1e-8), names\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def ablation_study(self, max_batches: int = 20) -> Dict[str, float]:\n",
        "        self.router.eval()\n",
        "        self.base.set_expert_schedule(1.0, 0.0)\n",
        "        all_towers = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "\n",
        "        total, correct_full = 0, 0\n",
        "        correct_ablated = {name: 0 for name in all_towers}\n",
        "\n",
        "        for i, (img, exp, lbl) in enumerate(self.loader):\n",
        "            if i >= max_batches:\n",
        "                break\n",
        "            img, exp, lbl = img.to(self.device), exp.to(self.device), lbl.to(self.device)\n",
        "            total += lbl.size(0)\n",
        "\n",
        "            logits, _ = self.router(img, exp)\n",
        "            correct_full += logits.argmax(1).eq(lbl).sum().item()\n",
        "\n",
        "            x = self.base['patch_embed'](img)\n",
        "            trans_out = self.base['trans_collective'](x)\n",
        "            conv_fused, conv_ops = self.base['conv_collective'](x)\n",
        "            expert = self.base['expert'](exp)\n",
        "\n",
        "            for ablate_name in all_towers:\n",
        "                trans_mod = [torch.zeros_like(trans_out.opinions[n].opinion) if n == ablate_name\n",
        "                            else trans_out.opinions[n].opinion for n in self.base['trans_collective'].tower_names]\n",
        "                conv_mod = [torch.zeros_like(conv_ops[n]) if n == ablate_name\n",
        "                           else conv_ops[n] for n in self.base['conv_collective'].tower_names]\n",
        "                trans_fused = self.base['trans_collective']['fusion'](*trans_mod)\n",
        "                conv_fused_m = self.base['conv_collective']['fusion'](*conv_mod)\n",
        "                logits_m = self.base['classifier'](torch.cat([trans_fused, conv_fused_m, expert], dim=-1))\n",
        "                correct_ablated[ablate_name] += logits_m.argmax(1).eq(lbl).sum().item()\n",
        "\n",
        "        results = {'full': 100 * correct_full / total}\n",
        "        results.update({n: 100 * correct_ablated[n] / total for n in all_towers})\n",
        "        return results\n",
        "\n",
        "    def full_analysis(self, max_batches: int = 50) -> DiagnosticReport:\n",
        "        print(\"Collecting tower opinions...\")\n",
        "        data = self.collect_opinions(max_batches)\n",
        "        all_opinions = {**data['trans'], **data['conv']}\n",
        "\n",
        "        print(\"Analyzing tower statistics...\")\n",
        "        tower_stats = self.analyze_tower_stats(all_opinions)\n",
        "\n",
        "        print(\"Analyzing fingerprints...\")\n",
        "        fingerprints, fp_sim_matrix, fp_names = self.analyze_fingerprints()\n",
        "        for name, fp in fingerprints.items():\n",
        "            if name in tower_stats:\n",
        "                tower_stats[name].fingerprint = fp\n",
        "\n",
        "        print(\"Analyzing pos/neg pairs...\")\n",
        "        pair_stats = self.analyze_pairs(all_opinions)\n",
        "\n",
        "        print(\"Computing tower correlations...\")\n",
        "        corr_matrix, _ = self.compute_tower_correlations(all_opinions)\n",
        "\n",
        "        print(\"Analyzing class preferences...\")\n",
        "        class_prefs, _ = self.analyze_class_preferences(all_opinions, data['labels'])\n",
        "\n",
        "        print(\"Running ablation study...\")\n",
        "        ablation = self.ablation_study(max_batches=20)\n",
        "\n",
        "        full_acc = ablation['full']\n",
        "        for name, stats in tower_stats.items():\n",
        "            if name in ablation:\n",
        "                stats.ablation_accuracy = ablation[name]\n",
        "                stats.contribution_score = full_acc - ablation[name]\n",
        "\n",
        "        if data['fusion_weights'] is not None:\n",
        "            fw_mean = data['fusion_weights'].mean(dim=0)\n",
        "            for i, name in enumerate(self.base['trans_collective'].tower_names):\n",
        "                if name in tower_stats:\n",
        "                    tower_stats[name].fusion_weight_mean = fw_mean[i].item()\n",
        "\n",
        "        return DiagnosticReport(\n",
        "            tower_stats=tower_stats, pair_stats=pair_stats,\n",
        "            fingerprint_similarity_matrix=fp_sim_matrix,\n",
        "            tower_correlation_matrix=corr_matrix,\n",
        "            fusion_weights=data['fusion_weights'].mean(dim=0) if data['fusion_weights'] is not None else None,\n",
        "            class_tower_preferences=class_prefs,\n",
        "            full_accuracy=full_acc, per_tower_ablation=ablation,\n",
        "        )\n",
        "\n",
        "    def print_report(self, report: DiagnosticReport):\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"TOWER DIAGNOSTIC REPORT\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nOVERALL ACCURACY\\n{'='*40}\")\n",
        "        print(f\"Full model: {report.full_accuracy:.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPER-TOWER STATISTICS\\n{'='*40}\")\n",
        "        print(f\"{'Tower':<20} {'Norm μ':>8} {'Norm σ':>8} {'Var':>8} {'Contrib':>8} {'Ablated':>8}\")\n",
        "        print(\"-\" * 72)\n",
        "        for ts in sorted(report.tower_stats.values(), key=lambda x: x.contribution_score, reverse=True):\n",
        "            print(f\"{ts.name:<20} {ts.opinion_norm_mean:>8.3f} {ts.opinion_norm_std:>8.3f} \"\n",
        "                  f\"{ts.opinion_var:>8.4f} {ts.contribution_score:>+7.2f}% {ts.ablation_accuracy:>7.2f}%\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPOS/NEG PAIR DYNAMICS\\n{'='*40}\")\n",
        "        print(f\"{'Pair':<25} {'Corr':>8} {'Cosine':>8} {'Compl':>8} {'CombNorm':>8}\")\n",
        "        print(\"-\" * 65)\n",
        "        for ps in report.pair_stats:\n",
        "            print(f\"{ps.pos_name.replace('_pos', ''):<25} {ps.opinion_correlation:>8.3f} {ps.opinion_cosine:>8.3f} \"\n",
        "                  f\"{ps.complementarity:>8.3f} {ps.combined_norm:>8.3f}\")\n",
        "\n",
        "        if report.fusion_weights is not None:\n",
        "            print(f\"\\n{'='*40}\\nFUSION WEIGHTS (Transformer)\\n{'='*40}\")\n",
        "            for i, name in enumerate(self.base['trans_collective'].tower_names):\n",
        "                w = report.fusion_weights[i].item()\n",
        "                print(f\"{name:<20} {w:.3f} {'█' * int(w * 50)}\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nTOP 5 CONTRIBUTORS\\n{'='*40}\")\n",
        "        contrib = sorted([(n, report.full_accuracy - a) for n, a in report.per_tower_ablation.items() if n != 'full'],\n",
        "                        key=lambda x: x[1], reverse=True)[:5]\n",
        "        for name, drop in contrib:\n",
        "            print(f\"{name:<20} -{drop:.2f}% when removed\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nPOTENTIALLY REDUNDANT\\n{'='*40}\")\n",
        "        redundant = [(n, report.full_accuracy - a) for n, a in report.per_tower_ablation.items()\n",
        "                     if n != 'full' and abs(report.full_accuracy - a) < 0.1]\n",
        "        if redundant:\n",
        "            for name, drop in redundant:\n",
        "                print(f\"{name:<20} {drop:+.2f}% (negligible)\")\n",
        "        else:\n",
        "            print(\"None - all towers contribute\")\n",
        "\n",
        "        print(f\"\\n{'='*40}\\nFINGERPRINT SIMILARITY\\n{'='*40}\")\n",
        "        fp_sim = report.fingerprint_similarity_matrix\n",
        "        fp_names = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "        sims = [(fp_names[i], fp_names[j], fp_sim[i, j].item())\n",
        "                for i in range(len(fp_names)) for j in range(i+1, len(fp_names))]\n",
        "        sims.sort(key=lambda x: x[2], reverse=True)\n",
        "        print(\"Most similar:\")\n",
        "        for n1, n2, s in sims[:5]:\n",
        "            print(f\"  {n1:<18} <-> {n2:<18} : {s:.3f}\")\n",
        "        print(\"Most different:\")\n",
        "        for n1, n2, s in sims[-5:]:\n",
        "            print(f\"  {n1:<18} <-> {n2:<18} : {s:.3f}\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "    def plot_report(self, report: DiagnosticReport, save_path: str = None):\n",
        "        import matplotlib.pyplot as plt\n",
        "        import numpy as np\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        fig.suptitle('Tower Diagnostic Report', fontsize=14, fontweight='bold')\n",
        "\n",
        "        fp_names = list(self.base['trans_collective'].tower_names) + list(self.base['conv_collective'].tower_names)\n",
        "\n",
        "        # 1. Tower Contribution\n",
        "        ax = axes[0, 0]\n",
        "        names = [n.replace('_', '\\n') for n in report.per_tower_ablation.keys() if n != 'full']\n",
        "        contribs = [report.full_accuracy - a for n, a in report.per_tower_ablation.items() if n != 'full']\n",
        "        colors = ['steelblue' if '_pos' in n else 'coral' for n in report.per_tower_ablation.keys() if n != 'full']\n",
        "        ax.bar(range(len(names)), contribs, color=colors)\n",
        "        ax.set_xticks(range(len(names)))\n",
        "        ax.set_xticklabels(names, fontsize=7, rotation=45, ha='right')\n",
        "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        ax.set_ylabel('Accuracy Drop (%)')\n",
        "        ax.set_title('Tower Contribution')\n",
        "\n",
        "        # 2. Opinion Norms\n",
        "        ax = axes[0, 1]\n",
        "        norms = [ts.opinion_norm_mean for ts in report.tower_stats.values()]\n",
        "        stds = [ts.opinion_norm_std for ts in report.tower_stats.values()]\n",
        "        ax.bar(range(len(norms)), norms, yerr=stds, capsize=3, color='teal', alpha=0.7)\n",
        "        ax.set_xticks(range(len(norms)))\n",
        "        ax.set_xticklabels([ts.name.replace('_', '\\n') for ts in report.tower_stats.values()], fontsize=7, rotation=45, ha='right')\n",
        "        ax.set_ylabel('Opinion Norm')\n",
        "        ax.set_title('Tower Opinion Magnitudes')\n",
        "\n",
        "        # 3. Fingerprint Similarity\n",
        "        ax = axes[0, 2]\n",
        "        im = ax.imshow(report.fingerprint_similarity_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "        ax.set_xticks(range(len(fp_names)))\n",
        "        ax.set_yticks(range(len(fp_names)))\n",
        "        ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=90)\n",
        "        ax.set_yticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6)\n",
        "        ax.set_title('Fingerprint Similarity')\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "        # 4. Opinion Correlation\n",
        "        ax = axes[1, 0]\n",
        "        im = ax.imshow(report.tower_correlation_matrix.numpy(), cmap='RdBu_r', vmin=-1, vmax=1)\n",
        "        ax.set_xticks(range(len(fp_names)))\n",
        "        ax.set_yticks(range(len(fp_names)))\n",
        "        ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=90)\n",
        "        ax.set_yticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6)\n",
        "        ax.set_title('Opinion Correlation')\n",
        "        plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "        # 5. Pos/Neg Pair Dynamics\n",
        "        ax = axes[1, 1]\n",
        "        pair_names = [ps.pos_name.replace('_pos', '') for ps in report.pair_stats]\n",
        "        x = np.arange(len(pair_names))\n",
        "        ax.bar(x - 0.25, [ps.opinion_correlation for ps in report.pair_stats], 0.25, label='Corr', color='steelblue')\n",
        "        ax.bar(x, [ps.opinion_cosine for ps in report.pair_stats], 0.25, label='Cosine', color='coral')\n",
        "        ax.bar(x + 0.25, [ps.complementarity for ps in report.pair_stats], 0.25, label='Compl', color='seagreen')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(pair_names, rotation=45, ha='right')\n",
        "        ax.axhline(y=0, color='black', linewidth=0.5)\n",
        "        ax.set_title('Pos/Neg Pair Dynamics')\n",
        "        ax.legend(fontsize=8)\n",
        "\n",
        "        # 6. Fusion Weights or Class Variance\n",
        "        ax = axes[1, 2]\n",
        "        if report.fusion_weights is not None:\n",
        "            trans_names = list(self.base['trans_collective'].tower_names)\n",
        "            colors = ['steelblue' if '_pos' in n else 'coral' for n in trans_names]\n",
        "            ax.barh(range(len(trans_names)), report.fusion_weights.numpy(), color=colors)\n",
        "            ax.set_yticks(range(len(trans_names)))\n",
        "            ax.set_yticklabels(trans_names, fontsize=8)\n",
        "            ax.set_title('Fusion Weights')\n",
        "            ax.invert_yaxis()\n",
        "        else:\n",
        "            var = report.class_tower_preferences.var(dim=0).numpy()\n",
        "            ax.bar(range(len(fp_names)), var, color='purple', alpha=0.7)\n",
        "            ax.set_xticks(range(len(fp_names)))\n",
        "            ax.set_xticklabels([n.replace('_', '\\n') for n in fp_names], fontsize=6, rotation=45, ha='right')\n",
        "            ax.set_title('Class Preference Variance')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        if save_path:\n",
        "            plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "            print(f\"Saved plot to {save_path}\")\n",
        "        return fig\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Hybrid Router (WideRouter Collectives)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 24  # 3 warmup + 3 prep + 3 stress + 5 refine + 10 polish\n",
        "    DIM = 256\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Vision encoder for expert pathway\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoder: {VISION_ENCODER}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features (model unloaded after caching)\n",
        "    cacher = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat = cacher.build_cache(train_base, 'train')\n",
        "    test_lat = cacher.build_cache(test_base, 'test')\n",
        "\n",
        "    expert_dim = cacher.dim\n",
        "    print(f\"Expert dim: {expert_dim}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat)\n",
        "    test_ds = CachedDataset(test_base, test_lat)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=4)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=4)\n",
        "\n",
        "    # Build router\n",
        "    router = CIFARHybridRouter(dim=DIM, expert_dim=expert_dim)\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nCompiling router with WideRouter optimizations...\")\n",
        "        router = router.prepare_and_compile(mode='reduce-overhead')\n",
        "        print(\"Compilation complete\")\n",
        "\n",
        "    # SGD with momentum\n",
        "    model_params = router._orig_mod.parameters() if COMPILE else router.parameters()\n",
        "    HIGH_LR = 0.1\n",
        "    LOW_LR = 0.001\n",
        "    POLISH_LR = 0.0001\n",
        "    opt = torch.optim.SGD(model_params, lr=HIGH_LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Tight schedule with polish phase\n",
        "    sched = ExpertScheduler(\n",
        "        warmup=3,\n",
        "        prep=3,\n",
        "        stress=3,\n",
        "        refine=5,\n",
        "        polish=10,        # 10 more epochs to polish\n",
        "        max_dropout=0.85,\n",
        "        min_dropout=0.2,\n",
        "        polish_dropout=0.1,  # Low dropout for polish\n",
        "        high_lr=HIGH_LR,\n",
        "        low_lr=LOW_LR,\n",
        "        polish_lr=POLISH_LR,\n",
        "    )\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={HIGH_LR}, momentum=0.9)\")\n",
        "    print(f\"Dual-head: main (256 hidden) + standalone (8000 hidden), weight=0.5\")\n",
        "    print(f\"Schedule:\")\n",
        "    print(f\"  WARMUP:  E01-E{sched.warmup:02d}\")\n",
        "    print(f\"  PREP:    E{sched.warmup+1:02d}-E{sched.prep_end:02d}\")\n",
        "    print(f\"  STRESS:  E{sched.prep_end+1:02d}-E{sched.stress_end:02d} (dropout 0→{sched.max_dropout}, LR={HIGH_LR})\")\n",
        "    print(f\"  REFINE:  E{sched.stress_end+1:02d}-E{sched.refine_end:02d} (dropout {sched.max_dropout}→{sched.min_dropout}, LR={LOW_LR})\")\n",
        "    print(f\"  POLISH:  E{sched.refine_end+1:02d}-E{sched.polish_end:02d} (dropout={sched.polish_dropout}, LR={POLISH_LR})\")\n",
        "    print(f\"Markers: * = best main, ^ = best standalone\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best = 0\n",
        "    best_standalone = 0\n",
        "    current_lr = HIGH_LR\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "        scale, drop = sched(epoch)\n",
        "\n",
        "        # LR snap at phase transitions\n",
        "        new_lr = sched.get_lr(epoch)\n",
        "        if new_lr != current_lr:\n",
        "            print(f\"\\n*** LR SNAP: {current_lr:.4f} → {new_lr:.4f} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = new_lr\n",
        "            current_lr = new_lr\n",
        "\n",
        "        loss, train_acc, train_solo = train_epoch(router, train_loader, opt, DEVICE, epoch, sched, COMPILE)\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "        test_solo = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        solo_marker = \" ^\" if test_solo > best_standalone else \"\"\n",
        "        best = max(best, test_acc)\n",
        "        best_standalone = max(best_standalone, test_solo)\n",
        "\n",
        "        phase = sched.phase_name(epoch)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | {phase:14s} | lr={current_lr:.4f} | loss={loss:.4f} | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | solo={test_solo:.1f}%{solo_marker} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    # Test main head with full expert\n",
        "    base = router._orig_mod if COMPILE else router\n",
        "    base.set_expert_schedule(1.0, 0.0)\n",
        "    final_main = evaluate(router, test_loader, DEVICE, expert_on=True, is_compiled=COMPILE)\n",
        "\n",
        "    # Test standalone head (towers only - the whole point!)\n",
        "    final_standalone = evaluate_standalone(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "\n",
        "    print(f\"Best main: {best:.2f}%\")\n",
        "    print(f\"Best standalone: {best_standalone:.2f}%\")\n",
        "    print(f\"Final main (w/ expert): {final_main:.2f}%\")\n",
        "    print(f\"Final standalone (towers only): {final_standalone:.2f}%\")\n",
        "    print(f\"Tower retention: {final_standalone/final_main*100:.1f}%\")\n",
        "\n",
        "    # === RUN DIAGNOSTICS ===\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"RUNNING TOWER DIAGNOSTICS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=COMPILE)\n",
        "    report = diag.full_analysis(max_batches=50)\n",
        "    diag.print_report(report)\n",
        "\n",
        "    # Save plot\n",
        "    try:\n",
        "        fig = diag.plot_report(report, save_path='/mnt/user-data/outputs/tower_diagnostics.png')\n",
        "        print(\"\\nDiagnostic plot saved to /mnt/user-data/outputs/tower_diagnostics.png\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nCould not generate plot: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JAtrMD4JBi3a",
        "outputId": "2ed20358-a4c4-4851-8de1-aa5acf58ae9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Hybrid Router (WideRouter Collectives)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoder: dinov3_convnext_large\n",
            "Compile: True\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "Expert dim: 1536\n",
            "\n",
            "Params: 32,748,990\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg', 'sinusoidal_pos', 'sinusoidal_neg', 'standard_025', 'standard_050', 'standard_075', 'standard_100']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling router with WideRouter optimizations...\n",
            "Compilation complete\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "Dual-head: main (256 hidden) + standalone (8000 hidden), weight=0.5\n",
            "Schedule:\n",
            "  WARMUP:  E01-E03\n",
            "  PREP:    E04-E06\n",
            "  STRESS:  E07-E09 (dropout 0→0.85, LR=0.1)\n",
            "  REFINE:  E10-E14 (dropout 0.85→0.2, LR=0.001)\n",
            "  POLISH:  E15-E24 (dropout=0.1, LR=0.0001)\n",
            "Markers: * = best main, ^ = best standalone\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1:   0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "cannot access local variable 'tracer_output' where it is not associated with a value",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m             \u001b[0mguarded_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_utils_internal.py\u001b[0m in \u001b[0;36mwrapper_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mStrobelightCompileTimeProfiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_inner\u001b[0;34m(code, one_graph, hooks)\u001b[0m\n\u001b[1;32m   1116\u001b[0m             \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileTimeInstructionCounter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_compile_inner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile_inner\u001b[0;34m(code, one_graph, hooks)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m             dynamo_output = compile_frame(\n\u001b[0m\u001b[1;32m   1152\u001b[0m                 \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mcompile_frame\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, restart_reasons, export, export_constraints, frame_state, distributed_state, package)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mdynamo_timed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"compile_attempt_{attempt}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_pt2_compile_event\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m                 \u001b[0mbytecode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_code_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1033\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0mtracer_output\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/bytecode_transformation.py\u001b[0m in \u001b[0;36mtransform_code_object\u001b[0;34m(code, transformations, safe)\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m     \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_and_assemble_instructions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstructions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(instructions, code_options)\u001b[0m\n\u001b[1;32m   1003\u001b[0m         )\n\u001b[0;32m-> 1004\u001b[0;31m         tracer_output = trace_frame(\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mtrace_frame\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, tf_mode_stack, one_graph, speculation_log, instructions, code_options, export, export_constraints, frame_state, distributed_state, package)\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 815\u001b[0;31m         \u001b[0mrun_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    816\u001b[0m         \u001b[0mtracer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDynamoTracerOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36mrun_tracer\u001b[0;34m()\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtracing_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_current_tx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                 \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnspecializeRestartAnalysis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minvoke_and_store_as_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/nn_module.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             return variables.UserFunctionVariable(fn, source=source).call_function(\n\u001b[0m\u001b[1;32m   1011\u001b[0m                 \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/functions.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m    341\u001b[0m     ) -> \"VariableTracker\":\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_user_function_return\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_user_function_return\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mInliningInstructionTranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call\u001b[0;34m(cls, parent, func, args, kwargs)\u001b[0m\n\u001b[1;32m   4111\u001b[0m             \u001b[0mtracer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_inline_tracer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minline_call_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36minline_call_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   4314\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mstrict_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4315\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObservedException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1486\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1487\u001b[0;31m                     \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1488\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopcode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m    903\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0minner_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mCALL\u001b[0;34m(self, inst)\u001b[0m\n\u001b[1;32m   3410\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mCALL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInstruction\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3411\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inst, call_kw)\u001b[0m\n\u001b[1;32m   3404\u001b[0m             \u001b[0;31m# a subsequent call may have self.kw_names set to an old value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3405\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3406\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/symbolic_convert.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   1265\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Attempt to trace forbidden callable {inner_fn}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/lazy.py\u001b[0m in \u001b[0;36mrealize_and_forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m     ) -> Any:\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrealize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/torch.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, tx, args, kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m         tensor_variable = wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   1517\u001b[0m             \u001b[0mtx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy\u001b[0;34m(tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2644\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msubclass_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrap_fx_proxy_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTensorVariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36mwrap_fx_proxy_cls\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2710\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexample_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2711\u001b[0;31m         return _wrap_fx_proxy(\n\u001b[0m\u001b[1;32m   2712\u001b[0m             \u001b[0mtarget_cls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubclass_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/variables/builder.py\u001b[0m in \u001b[0;36m_wrap_fx_proxy\u001b[0;34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[0m\n\u001b[1;32m   2808\u001b[0m         \u001b[0;31m# cases properly below.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2809\u001b[0;31m         \u001b[0mexample_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_fake_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproxy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_non_graph_fake\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mget_fake_value\u001b[0;34m(node, tx, allow_non_graph_fake)\u001b[0m\n\u001b[1;32m   3375\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfake_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable_python_dispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3376\u001b[0;31m             ret_val = wrap_fake_exception(\n\u001b[0m\u001b[1;32m   3377\u001b[0m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mwrap_fake_exception\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m   2863\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2864\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2865\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupportedFakeTensorException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   3376\u001b[0m             ret_val = wrap_fake_exception(\n\u001b[0;32m-> 3377\u001b[0;31m                 \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrun_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3378\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/utils.py\u001b[0m in \u001b[0;36mrun_node\u001b[0;34m(tracer, node, args, kwargs, nnmodule)\u001b[0m\n\u001b[1;32m   3545\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_function\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3546\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[operator]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3547\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"call_method\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_stats.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_call_counter\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__torch_dispatch__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   2095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2096\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_dispatch_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_cached_dispatch_impl\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m             \u001b[0;31m# We have a cache entry.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_from_cache_entry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m             \u001b[0mFakeTensorMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_hits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_output_from_cache_entry\u001b[0;34m(self, state, entry, key, func, args)\u001b[0m\n\u001b[1;32m   2016\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2017\u001b[0;31m             return self._get_output_tensor_from_cache_entry(\n\u001b[0m\u001b[1;32m   2018\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m_get_output_tensor_from_cache_entry\u001b[0;34m(self, state, entry, key, func, args)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1994\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mFakeTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_subclasses/fake_tensor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, fake_mode, elem, device, constant, real_tensor, pytype, dispatch_keys)\u001b[0m\n\u001b[1;32m    718\u001b[0m     ) -> Self:\n\u001b[0;32m--> 719\u001b[0;31m         self = Tensor._make_subclass(\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1966091161.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1966091161.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_lr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_solo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msched\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCOMPILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m         \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_compiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mtest_solo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_standalone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_compiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCOMPILE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1966091161.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(router, loader, opt, device, epoch, sched, is_compiled, standalone_weight)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Dual outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 559\u001b[0;31m         \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstandalone_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;31m# Combined loss: main + weighted standalone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def __reduce__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, frame_state)\u001b[0m\n\u001b[1;32m   1872\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_lock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_disable_current_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1873\u001b[0m             \u001b[0;31m# skip=1: skip this frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1874\u001b[0;31m             result = self._torchdynamo_orig_backend(\n\u001b[0m\u001b[1;32m   1875\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         \u001b[0mcounters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"frames\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1624\u001b[0;31m             result = self._inner_convert(\n\u001b[0m\u001b[1;32m   1625\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_entry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1626\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcompile_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCompileContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             result = _compile(\n\u001b[0m\u001b[1;32m    689\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_code\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m                 \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_globals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/convert_frame.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip, package, convert_frame_box)\u001b[0m\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1509\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mtracer_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1510\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtracer_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'tracer_output' where it is not associated with a value"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to verify cache alignment\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "def verify_cache_alignment(cache_path, encoder_name, split='train', num_samples=5):\n",
        "    \"\"\"Check if cached embeddings match fresh encodings.\"\"\"\n",
        "    from geofractal.router.components.encoder_data_component import MultiVisionEncode\n",
        "\n",
        "    # Load cache\n",
        "    cache = torch.load(cache_path, weights_only=True)\n",
        "    print(f\"Cache shape: {cache.shape}\")\n",
        "\n",
        "    # Load dataset (same way as during caching)\n",
        "    ds = datasets.CIFAR100('./data', train=(split=='train'), transform=transforms.ToTensor())\n",
        "\n",
        "    # Create fresh encoder\n",
        "    encoder = MultiVisionEncode(\n",
        "        encoders=[encoder_name],\n",
        "        dataset_name='verify',\n",
        "        device='cuda',\n",
        "        cache_enabled=False,\n",
        "        pool_output=True,\n",
        "    )\n",
        "\n",
        "    # Check specific indices\n",
        "    test_indices = [0, 100, 1000, 10000, len(ds)-1]\n",
        "\n",
        "    print(f\"\\n{'idx':<8} {'label':<8} {'cosine_sim':<12} {'status'}\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    for idx in test_indices:\n",
        "        img, label = ds[idx]\n",
        "        with torch.no_grad():\n",
        "            fresh = encoder.encode(img.unsqueeze(0).cuda())\n",
        "\n",
        "        cached = cache[idx].cuda()\n",
        "        sim = F.cosine_similarity(fresh, cached.unsqueeze(0)).item()\n",
        "\n",
        "        status = \"✓ ALIGNED\" if sim > 0.99 else \"✗ MISALIGNED\"\n",
        "        print(f\"{idx:<8} {label:<8} {sim:<12.4f} {status}\")\n",
        "\n",
        "    encoder.unload_all()\n",
        "    return sim > 0.99\n",
        "\n",
        "# Run verification\n",
        "verify_cache_alignment(\n",
        "    'encoder_cache/cifar100/dinov3_convnext_large_train.pt',\n",
        "    'dinov3_convnext_large',\n",
        "    'train'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "id": "CB-fMogBdCAD",
        "outputId": "5ed74b1d-9311-4af8-d679-ed600fbe1dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cache shape: torch.Size([50000, 1536])\n",
            "\n",
            "============================================================\n",
            "Initializing MultiVisionEncode\n",
            "============================================================\n",
            "Dataset: verify\n",
            "Encoders: ['dinov3_convnext_large']\n",
            "  Loading dinov3_convnext_large...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No path specified for dinov3_convnext_large",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3153204304.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Run verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m verify_cache_alignment(\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;34m'encoder_cache/cifar100/dinov3_convnext_large_train.pt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;34m'dinov3_convnext_large'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3153204304.py\u001b[0m in \u001b[0;36mverify_cache_alignment\u001b[0;34m(cache_path, encoder_name, split, num_samples)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Create fresh encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     encoder = MultiVisionEncode(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mencoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'verify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/components/encoder_data_component.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoders, dataset_name, device, dtype, cache_enabled, cache_dir, concatenate, image_size, pool_output)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1819\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menc_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoders\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1820\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m         \u001b[0;31m# Calculate combined dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/components/encoder_data_component.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(self, encoder_name, local_path, hf_repo, weights_path, config_override)\u001b[0m\n\u001b[1;32m   1463\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No path specified for {encoder_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;31m# Load tokenizer/processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No path specified for dinov3_convnext_large"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# single opinion shared space kl-divergence tower"
      ],
      "metadata": {
        "id": "Dy2ZV7pGhEyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# big birtha"
      ],
      "metadata": {
        "id": "knJbR0Ljl_Fw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Unified Fusion Router\n",
        "================================\n",
        "\n",
        "Dual expert architecture with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "Architecture:\n",
        "    Expert1 (ConvNeXt, ×0.4) ──┐\n",
        "    Expert2 (ViT-L, ×0.4)    ──┼─ averaged ─┐\n",
        "                                            ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "    Trans  (×0.6)            ──────────────┤\n",
        "    Conv   (×1.0)            ──────────────┘\n",
        "\n",
        "Loss: CrossEntropy + 0.001 × KL-divergence\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# === DISABLE TENSORFLOW GPU (conflicts with PyTorch in Colab) ===\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''  # No GPU for TF\n",
        "\n",
        "# Suppress all torch logging\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "        'dinov3_vitl16': {\n",
        "            'hf_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "    def load_or_create(self) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Load or create train/test caches.\"\"\"\n",
        "        norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "        tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "        train_ds = datasets.CIFAR100('./data', train=True, transform=tf, download=True)\n",
        "        test_ds = datasets.CIFAR100('./data', train=False, transform=tf, download=True)\n",
        "\n",
        "        return self.build_cache(train_ds, 'train'), self.build_cache(test_ds, 'test')\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents (one or more experts) + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor, latents2: Tensor = None):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "        self.latents2 = latents2  # Optional second expert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        if self.latents2 is not None:\n",
        "            return img, self.latents[i], self.latents2[i], label\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UNIFIED FUSION ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedFusionRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "    Architecture:\n",
        "        Expert1 (convnext) ──┐\n",
        "        Expert2 (vit)      ──┼─ combined ─┐\n",
        "                                          ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "        Trans  (×0.6)      ──────────────┤\n",
        "        Conv   (×1.0)      ──────────────┘\n",
        "\n",
        "    Single head learns shared fusion space. KL regularization encourages\n",
        "    structured latent representations.\n",
        "    \"\"\"\n",
        "\n",
        "    EXPERT_SCALE = 0.4\n",
        "    TRANS_SCALE = 0.6\n",
        "    CONV_SCALE = 1.0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'unified_fusion',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        expert_dim2: int = None,  # Second expert (optional)\n",
        "        trans_depth: int = 1,\n",
        "        conv_depth: int = 1,\n",
        "        num_heads: int = 8,\n",
        "        fingerprint_dim: int = 64,\n",
        "        latent_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "        self.has_expert2 = expert_dim2 is not None\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes, 'expert_dim': expert_dim,\n",
        "            'expert_dim2': expert_dim2, 'latent_dim': latent_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal'])\n",
        "        for scale in [0.25, 0.5, 0.75, 1.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_{int(scale*100):03d}',\n",
        "                rope='standard', address='standard', inverted=False,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT PROJECTIONS ===\n",
        "        self.attach('expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(expert_dim),\n",
        "            nn.Linear(expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        if self.has_expert2:\n",
        "            self.attach('expert_proj2', nn.Sequential(\n",
        "                nn.LayerNorm(expert_dim2),\n",
        "                nn.Linear(expert_dim2, dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim, dim),\n",
        "            ))\n",
        "\n",
        "        # === GATED FUSION (3 inputs: expert, trans, conv) ===\n",
        "        self.attach('fusion', GatedFusion(f'{name}_fusion', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # === BOTTLENECK (VAE-style) ===\n",
        "        self.attach('mu_proj', nn.Linear(dim, latent_dim))\n",
        "        self.attach('logvar_proj', nn.Linear(dim, latent_dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.Linear(latent_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Scale buffers\n",
        "        self.register_buffer('expert_scale', torch.tensor(self.EXPERT_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"VAE reparameterization trick.\"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            return mu + torch.randn_like(std) * std\n",
        "        return mu\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor, expert_latents2: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale\n",
        "\n",
        "        # Conv collective\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused * self.conv_scale\n",
        "\n",
        "        # Expert(s)\n",
        "        expert1 = self['expert_proj'](expert_latents)\n",
        "        if self.has_expert2 and expert_latents2 is not None:\n",
        "            expert2 = self['expert_proj2'](expert_latents2)\n",
        "            expert = (expert1 + expert2) * 0.5 * self.expert_scale  # Average and scale\n",
        "        else:\n",
        "            expert = expert1 * self.expert_scale\n",
        "\n",
        "        # Gated fusion\n",
        "        combined = self['fusion'](expert, trans_fused, conv_fused)\n",
        "\n",
        "        # Bottleneck\n",
        "        mu = self['mu_proj'](combined)\n",
        "        logvar = self['logvar_proj'](combined)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self['classifier'](z)\n",
        "\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'UnifiedFusionRouter':\n",
        "        \"\"\"Prepare collectives and compile.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS - Output Norm Analysis\n",
        "# =============================================================================\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Measure tower output norms to assess relative contributions.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False, dual_expert=True):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.dual_expert = dual_expert\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def measure_output_norms(self) -> Dict[str, float]:\n",
        "        \"\"\"Measure mean L2 norm of each tower's output over test set.\"\"\"\n",
        "        self.router.eval()\n",
        "\n",
        "        # Accumulators\n",
        "        trans_norms = {name: [] for name in self.base['trans_collective'].tower_names}\n",
        "        conv_norms = {name: [] for name in self.base['conv_collective'].tower_names}\n",
        "\n",
        "        for batch in self.loader:\n",
        "            if self.dual_expert:\n",
        "                img, _, _, _ = batch  # img, exp1, exp2, lbl\n",
        "            else:\n",
        "                img, _, _ = batch  # img, exp, lbl\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            # Get patch embeddings\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            # Measure trans tower outputs\n",
        "            trans_collective = self.base['trans_collective']\n",
        "            for name in trans_collective.tower_names:\n",
        "                out = trans_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                # Mean L2 norm across batch\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                trans_norms[name].append(norm)\n",
        "\n",
        "            # Measure conv tower outputs\n",
        "            conv_collective = self.base['conv_collective']\n",
        "            for name in conv_collective.tower_names:\n",
        "                out = conv_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                conv_norms[name].append(norm)\n",
        "\n",
        "        # Average across batches\n",
        "        results = {}\n",
        "        for name, norms in trans_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'trans'}\n",
        "        for name, norms in conv_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'conv'}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print output norm analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOWER OUTPUT NORMS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        norms = self.measure_output_norms()\n",
        "\n",
        "        # Separate by collective\n",
        "        trans = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'trans']\n",
        "        conv = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'conv']\n",
        "\n",
        "        trans.sort(key=lambda x: x[1], reverse=True)\n",
        "        conv.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTrans towers (scale={self.base.TRANS_SCALE}):\")\n",
        "        for name, norm in trans:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        print(f\"\\nConv towers (scale={self.base.CONV_SCALE}):\")\n",
        "        for name, norm in conv:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        # Summary\n",
        "        trans_mean = sum(n for _, n in trans) / len(trans) if trans else 0\n",
        "        conv_mean = sum(n for _, n in conv) / len(conv) if conv else 0\n",
        "\n",
        "        print(f\"\\n\" + \"-\" * 40)\n",
        "        print(f\"Trans mean norm: {trans_mean:.3f} (×{self.base.TRANS_SCALE} = {trans_mean * self.base.TRANS_SCALE:.3f})\")\n",
        "        print(f\"Conv mean norm:  {conv_mean:.3f} (×{self.base.CONV_SCALE} = {conv_mean * self.base.CONV_SCALE:.3f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def kl_divergence(mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "    \"\"\"KL divergence from N(mu, sigma) to N(0, 1).\"\"\"\n",
        "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def train_epoch(router, loader, opt, device, is_compiled=False, kl_weight=0.001, dual_expert=False):\n",
        "    \"\"\"Train one epoch with CE + KL loss.\"\"\"\n",
        "    router.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ce_loss = F.cross_entropy(logits, lbl)\n",
        "        kl_loss = kl_divergence(mu, logvar)\n",
        "        loss = ce_loss + kl_weight * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        total_ce += ce_loss.item() * lbl.size(0)\n",
        "        total_kl += kl_loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            ce=f'{total_ce/total:.3f}',\n",
        "            kl=f'{total_kl/total:.4f}',\n",
        "            acc=f'{100*correct/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss/total, total_ce/total, total_kl/total, 100*correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, dual_expert=False):\n",
        "    \"\"\"Evaluate accuracy.\"\"\"\n",
        "    router.eval()\n",
        "    correct, total = 0, 0\n",
        "    for batch in loader:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Unified Fusion Router\")\n",
        "    print(\"GatedFusion + KL Bottleneck + Dual Experts + SGD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 20\n",
        "    DIM = 256\n",
        "    LATENT_DIM = 512\n",
        "    KL_WEIGHT = 0.001\n",
        "    LR = 0.1\n",
        "    LR_DROP = 0.001  # Drop to this after LR_DROP_EPOCH\n",
        "    LR_DROP_EPOCH = 10\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitl16'  # Second expert\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoders: {VISION_ENCODER}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} → {LR_DROP} at epoch {LR_DROP_EPOCH}\")\n",
        "    print(f\"Scaling: Expert={UnifiedFusionRouter.EXPERT_SCALE}, Trans={UnifiedFusionRouter.TRANS_SCALE}, Conv={UnifiedFusionRouter.CONV_SCALE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features - Expert 1 (ConvNeXt)\n",
        "    cacher1 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat1 = cacher1.build_cache(train_base, 'train')\n",
        "    test_lat1 = cacher1.build_cache(test_base, 'test')\n",
        "    expert_dim = cacher1.dim\n",
        "\n",
        "    # Cache vision features - Expert 2 (ViT-L)\n",
        "    cacher2 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER_2,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat2 = cacher2.build_cache(train_base, 'train')\n",
        "    test_lat2 = cacher2.build_cache(test_base, 'test')\n",
        "    expert_dim2 = cacher2.dim\n",
        "\n",
        "    print(f\"Expert dims: {expert_dim} + {expert_dim2}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat1, train_lat2)\n",
        "    test_ds = CachedDataset(test_base, test_lat1, test_lat2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Build router\n",
        "    router = UnifiedFusionRouter(\n",
        "        dim=DIM,\n",
        "        expert_dim=expert_dim,\n",
        "        expert_dim2=expert_dim2,\n",
        "        latent_dim=LATENT_DIM\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if COMPILE:\n",
        "        # 'default' is safer, 'reduce-overhead' uses CUDA graphs (can conflict with TF)\n",
        "        COMPILE_MODE = 'default'  # Use 'default' for stability, 'reduce-overhead' for speed\n",
        "        print(f\"\\nCompiling with mode='{COMPILE_MODE}'...\")\n",
        "        sys.stdout.flush()\n",
        "        try:\n",
        "            router = router.prepare_and_compile(mode=COMPILE_MODE)\n",
        "            print(\"Compilation done\")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Compilation failed: {e}\")\n",
        "            print(\"Falling back to eager mode\")\n",
        "            COMPILE = False\n",
        "\n",
        "    # Optimizer - check if router is actually compiled\n",
        "    is_compiled = hasattr(router, '_orig_mod')\n",
        "    model_params = router._orig_mod.parameters() if is_compiled else router.parameters()\n",
        "    opt = torch.optim.SGD(model_params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={LR}, momentum=0.9)\")\n",
        "    print(f\"LR Schedule: {LR} for {LR_DROP_EPOCH} epochs → {LR_DROP} for {EPOCHS - LR_DROP_EPOCH} epochs\")\n",
        "    print(f\"Loss: CE + {KL_WEIGHT}×KL\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    best = 0\n",
        "    current_lr = LR\n",
        "    DUAL_EXPERT = True  # Using two experts\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # LR drop at specified epoch\n",
        "        if epoch == LR_DROP_EPOCH:\n",
        "            current_lr = LR_DROP\n",
        "            for param_group in opt.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "            print(f\"\\n>>> LR dropped to {current_lr} <<<\\n\")\n",
        "\n",
        "        loss, ce, kl, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled, kl_weight=KL_WEIGHT, dual_expert=DUAL_EXPERT\n",
        "        )\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, dual_expert=DUAL_EXPERT)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | lr={current_lr:.6f} | loss={loss:.3f} (ce={ce:.3f} kl={kl:.4f}) | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Diagnostics\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=is_compiled, dual_expert=DUAL_EXPERT)\n",
        "    diag.print_summary()\n",
        "\n",
        "    return router\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkx-Iio3hIqN",
        "outputId": "ad17fb55-3cb9-4e02-93f3-c754cfe7fd15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Unified Fusion Router\n",
            "GatedFusion + KL Bottleneck + Dual Experts + SGD\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoders: dinov3_convnext_large, dinov3_vitl16\n",
            "Compile: True\n",
            "Epochs: 20, Batch: 128\n",
            "LR: 0.1 → 0.001 at epoch 10\n",
            "Scaling: Expert=0.4, Trans=0.6, Conv=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "VisionCacher: dinov3_vitl16 (1024d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_test.pt\n",
            "Expert dims: 1536 + 1024\n",
            "\n",
            "Params: 16,763,543\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg', 'sinusoidal_pos', 'sinusoidal_neg', 'standard_025', 'standard_050', 'standard_075', 'standard_100']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling with mode='default'...\n",
            "Compilation done\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "LR Schedule: 0.1 for 10 epochs → 0.001 for 10 epochs\n",
            "Loss: CE + 0.001×KL\n",
            "Compiled: True\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 390/391 [02:22<00:00,  7.06it/s, acc=50.9%, ce=2.093, kl=0.3403, loss=2.093]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.100000 | loss=2.090 (ce=2.090 kl=0.3408) | train=50.9% | test=88.1% * | 304.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.100000 | loss=0.439 (ce=0.439 kl=0.8183) | train=88.7% | test=90.3% * | 60.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.100000 | loss=0.295 (ce=0.294 kl=1.0107) | train=91.9% | test=90.2% | 59.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.100000 | loss=0.212 (ce=0.211 kl=1.1000) | train=94.1% | test=89.7% | 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.100000 | loss=0.166 (ce=0.165 kl=1.1353) | train=95.2% | test=90.7% * | 59.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.100000 | loss=0.130 (ce=0.128 kl=1.1762) | train=96.2% | test=90.6% | 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.100000 | loss=0.104 (ce=0.103 kl=1.1600) | train=96.9% | test=91.0% * | 60.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.100000 | loss=0.076 (ce=0.075 kl=1.1458) | train=97.6% | test=91.3% * | 60.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.100000 | loss=0.070 (ce=0.069 kl=1.1480) | train=97.8% | test=90.8% | 60.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.100000 | loss=0.056 (ce=0.055 kl=1.1315) | train=98.3% | test=90.5% | 59.7s\n",
            "\n",
            ">>> LR dropped to 0.001 <<<\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.001000 | loss=0.036 (ce=0.035 kl=1.1273) | train=99.0% | test=91.1% | 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.001000 | loss=0.025 (ce=0.024 kl=1.1232) | train=99.3% | test=91.3% * | 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.001000 | loss=0.022 (ce=0.020 kl=1.1233) | train=99.4% | test=91.4% * | 61.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.001000 | loss=0.019 (ce=0.018 kl=1.1252) | train=99.5% | test=91.5% * | 59.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.001000 | loss=0.018 (ce=0.017 kl=1.1273) | train=99.5% | test=91.5% * | 59.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.001000 | loss=0.016 (ce=0.015 kl=1.1299) | train=99.5% | test=91.6% * | 59.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.001000 | loss=0.014 (ce=0.013 kl=1.1312) | train=99.6% | test=91.6% * | 59.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.001000 | loss=0.015 (ce=0.013 kl=1.1329) | train=99.6% | test=91.6% * | 61.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.001000 | loss=0.014 (ce=0.013 kl=1.1347) | train=99.6% | test=91.7% * | 60.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.001000 | loss=0.014 (ce=0.012 kl=1.1373) | train=99.6% | test=91.7% * | 59.8s\n",
            "\n",
            "======================================================================\n",
            "Best: 91.72%\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TOWER OUTPUT NORMS\n",
            "============================================================\n",
            "\n",
            "Trans towers (scale=0.6):\n",
            "  cantor_neg              2.587\n",
            "  standard_025            2.517\n",
            "  cantor_pos              2.505\n",
            "  simplex_neg             2.488\n",
            "  standard_100            2.476\n",
            "  beatrix_pos             2.455\n",
            "  helix_neg               2.426\n",
            "  sinusoidal_pos          2.418\n",
            "  simplex_pos             2.412\n",
            "  sinusoidal_neg          2.408\n",
            "  standard_075            2.398\n",
            "  helix_pos               2.392\n",
            "  standard_050            2.375\n",
            "  beatrix_neg             2.334\n",
            "\n",
            "Conv towers (scale=1.0):\n",
            "  frequency_pos          11.035\n",
            "  frequency_neg           8.974\n",
            "  depth_neg               7.961\n",
            "  wide_resnet_neg         6.977\n",
            "  wide_resnet_pos         6.870\n",
            "  coarse_fine_neg         6.744\n",
            "  depth_pos               6.628\n",
            "  coarse_fine_pos         6.418\n",
            "\n",
            "----------------------------------------\n",
            "Trans mean norm: 2.442 (×0.6 = 1.465)\n",
            "Conv mean norm:  7.701 (×1.0 = 7.701)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# deep big birtha 8 depth 88 towers"
      ],
      "metadata": {
        "id": "aGrfnT-XKsiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Unified Fusion Router\n",
        "================================\n",
        "\n",
        "Dual expert architecture with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "Architecture:\n",
        "    Expert1 (ConvNeXt, ×0.4) ──┐\n",
        "    Expert2 (ViT-L, ×0.4)    ──┼─ averaged ─┐\n",
        "                                            ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "    Trans  (×0.6)            ──────────────┤\n",
        "    Conv   (×1.0)            ──────────────┘\n",
        "\n",
        "Loss: CrossEntropy + 0.001 × KL-divergence\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# === DISABLE TENSORFLOW GPU (conflicts with PyTorch in Colab) ===\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''  # No GPU for TF\n",
        "\n",
        "# Suppress all torch logging\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "        'dinov3_vitl16': {\n",
        "            'hf_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "    def load_or_create(self) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Load or create train/test caches.\"\"\"\n",
        "        norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "        tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "        train_ds = datasets.CIFAR100('./data', train=True, transform=tf, download=True)\n",
        "        test_ds = datasets.CIFAR100('./data', train=False, transform=tf, download=True)\n",
        "\n",
        "        return self.build_cache(train_ds, 'train'), self.build_cache(test_ds, 'test')\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents (one or more experts) + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor, latents2: Tensor = None):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "        self.latents2 = latents2  # Optional second expert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        if self.latents2 is not None:\n",
        "            return img, self.latents[i], self.latents2[i], label\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UNIFIED FUSION ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedFusionRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "    Architecture:\n",
        "        Expert1 (convnext) ──┐\n",
        "        Expert2 (vit)      ──┼─ combined ─┐\n",
        "                                          ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "        Trans  (×0.6)      ──────────────┤\n",
        "        Conv   (×1.0)      ──────────────┘\n",
        "\n",
        "    Single head learns shared fusion space. KL regularization encourages\n",
        "    structured latent representations.\n",
        "    \"\"\"\n",
        "\n",
        "    EXPERT_SCALE = 0.4\n",
        "    TRANS_SCALE = 0.6\n",
        "    CONV_SCALE = 1.0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'unified_fusion',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        expert_dim2: int = None,  # Second expert (optional)\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        latent_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "        self.has_expert2 = expert_dim2 is not None\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes, 'expert_dim': expert_dim,\n",
        "            'expert_dim2': expert_dim2, 'latent_dim': latent_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE ===\n",
        "        trans_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal'])\n",
        "        for scale in [0.25, 0.5, 0.75, 1.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_{int(scale*100):03d}',\n",
        "                rope='standard', address='standard', inverted=False,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE ===\n",
        "        conv_configs = preset_conv_pos_neg()\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT PROJECTIONS ===\n",
        "        self.attach('expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(expert_dim),\n",
        "            nn.Linear(expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        if self.has_expert2:\n",
        "            self.attach('expert_proj2', nn.Sequential(\n",
        "                nn.LayerNorm(expert_dim2),\n",
        "                nn.Linear(expert_dim2, dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim, dim),\n",
        "            ))\n",
        "\n",
        "        # === GATED FUSION (3 inputs: expert, trans, conv) ===\n",
        "        self.attach('fusion', GatedFusion(f'{name}_fusion', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # === BOTTLENECK (VAE-style) ===\n",
        "        self.attach('mu_proj', nn.Linear(dim, latent_dim))\n",
        "        self.attach('logvar_proj', nn.Linear(dim, latent_dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.Linear(latent_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Scale buffers\n",
        "        self.register_buffer('expert_scale', torch.tensor(self.EXPERT_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"VAE reparameterization trick.\"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            return mu + torch.randn_like(std) * std\n",
        "        return mu\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor, expert_latents2: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale\n",
        "\n",
        "        # Conv collective\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused * self.conv_scale\n",
        "\n",
        "        # Expert(s)\n",
        "        expert1 = self['expert_proj'](expert_latents)\n",
        "        if self.has_expert2 and expert_latents2 is not None:\n",
        "            expert2 = self['expert_proj2'](expert_latents2)\n",
        "            expert = (expert1 + expert2) * 0.5 * self.expert_scale  # Average and scale\n",
        "        else:\n",
        "            expert = expert1 * self.expert_scale\n",
        "\n",
        "        # Gated fusion\n",
        "        combined = self['fusion'](expert, trans_fused, conv_fused)\n",
        "\n",
        "        # Bottleneck\n",
        "        mu = self['mu_proj'](combined)\n",
        "        logvar = self['logvar_proj'](combined)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self['classifier'](z)\n",
        "\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'UnifiedFusionRouter':\n",
        "        \"\"\"Prepare collectives and compile.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS - Output Norm Analysis\n",
        "# =============================================================================\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Measure tower output norms to assess relative contributions.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False, dual_expert=True):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.dual_expert = dual_expert\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def measure_output_norms(self) -> Dict[str, float]:\n",
        "        \"\"\"Measure mean L2 norm of each tower's output over test set.\"\"\"\n",
        "        self.router.eval()\n",
        "\n",
        "        # Accumulators\n",
        "        trans_norms = {name: [] for name in self.base['trans_collective'].tower_names}\n",
        "        conv_norms = {name: [] for name in self.base['conv_collective'].tower_names}\n",
        "\n",
        "        for batch in self.loader:\n",
        "            if self.dual_expert:\n",
        "                img, _, _, _ = batch  # img, exp1, exp2, lbl\n",
        "            else:\n",
        "                img, _, _ = batch  # img, exp, lbl\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            # Get patch embeddings\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            # Measure trans tower outputs\n",
        "            trans_collective = self.base['trans_collective']\n",
        "            for name in trans_collective.tower_names:\n",
        "                out = trans_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                # Mean L2 norm across batch\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                trans_norms[name].append(norm)\n",
        "\n",
        "            # Measure conv tower outputs\n",
        "            conv_collective = self.base['conv_collective']\n",
        "            for name in conv_collective.tower_names:\n",
        "                out = conv_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                conv_norms[name].append(norm)\n",
        "\n",
        "        # Average across batches\n",
        "        results = {}\n",
        "        for name, norms in trans_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'trans'}\n",
        "        for name, norms in conv_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'conv'}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print output norm analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOWER OUTPUT NORMS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        norms = self.measure_output_norms()\n",
        "\n",
        "        # Separate by collective\n",
        "        trans = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'trans']\n",
        "        conv = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'conv']\n",
        "\n",
        "        trans.sort(key=lambda x: x[1], reverse=True)\n",
        "        conv.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTrans towers (scale={self.base.TRANS_SCALE}):\")\n",
        "        for name, norm in trans:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        print(f\"\\nConv towers (scale={self.base.CONV_SCALE}):\")\n",
        "        for name, norm in conv:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        # Summary\n",
        "        trans_mean = sum(n for _, n in trans) / len(trans) if trans else 0\n",
        "        conv_mean = sum(n for _, n in conv) / len(conv) if conv else 0\n",
        "\n",
        "        print(f\"\\n\" + \"-\" * 40)\n",
        "        print(f\"Trans mean norm: {trans_mean:.3f} (×{self.base.TRANS_SCALE} = {trans_mean * self.base.TRANS_SCALE:.3f})\")\n",
        "        print(f\"Conv mean norm:  {conv_mean:.3f} (×{self.base.CONV_SCALE} = {conv_mean * self.base.CONV_SCALE:.3f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def kl_divergence(mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "    \"\"\"KL divergence from N(mu, sigma) to N(0, 1).\"\"\"\n",
        "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def train_epoch(router, loader, opt, device, is_compiled=False, kl_weight=0.001, dual_expert=False):\n",
        "    \"\"\"Train one epoch with CE + KL loss.\"\"\"\n",
        "    router.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ce_loss = F.cross_entropy(logits, lbl)\n",
        "        kl_loss = kl_divergence(mu, logvar)\n",
        "        loss = ce_loss + kl_weight * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        total_ce += ce_loss.item() * lbl.size(0)\n",
        "        total_kl += kl_loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            ce=f'{total_ce/total:.3f}',\n",
        "            kl=f'{total_kl/total:.4f}',\n",
        "            acc=f'{100*correct/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss/total, total_ce/total, total_kl/total, 100*correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, dual_expert=False):\n",
        "    \"\"\"Evaluate accuracy.\"\"\"\n",
        "    router.eval()\n",
        "    correct, total = 0, 0\n",
        "    for batch in loader:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Unified Fusion Router\")\n",
        "    print(\"GatedFusion + KL Bottleneck + Dual Experts + SGD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 20\n",
        "    DIM = 2048\n",
        "    LATENT_DIM = 2048\n",
        "    KL_WEIGHT = 0.001\n",
        "    LR = 0.1\n",
        "    LR_DROP = 0.001  # Drop to this after LR_DROP_EPOCH\n",
        "    LR_DROP_EPOCH = 10\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitl16'  # Second expert\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoders: {VISION_ENCODER}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} → {LR_DROP} at epoch {LR_DROP_EPOCH}\")\n",
        "    print(f\"Scaling: Expert={UnifiedFusionRouter.EXPERT_SCALE}, Trans={UnifiedFusionRouter.TRANS_SCALE}, Conv={UnifiedFusionRouter.CONV_SCALE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features - Expert 1 (ConvNeXt)\n",
        "    cacher1 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat1 = cacher1.build_cache(train_base, 'train')\n",
        "    test_lat1 = cacher1.build_cache(test_base, 'test')\n",
        "    expert_dim = cacher1.dim\n",
        "\n",
        "    # Cache vision features - Expert 2 (ViT-L)\n",
        "    cacher2 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER_2,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat2 = cacher2.build_cache(train_base, 'train')\n",
        "    test_lat2 = cacher2.build_cache(test_base, 'test')\n",
        "    expert_dim2 = cacher2.dim\n",
        "\n",
        "    print(f\"Expert dims: {expert_dim} + {expert_dim2}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat1, train_lat2)\n",
        "    test_ds = CachedDataset(test_base, test_lat1, test_lat2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Build router\n",
        "    router = UnifiedFusionRouter(\n",
        "        dim=DIM,\n",
        "        expert_dim=expert_dim,\n",
        "        expert_dim2=expert_dim2,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        patch_size=4,\n",
        "        trans_depth=4,\n",
        "        conv_depth=4,\n",
        "        num_heads=8,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if COMPILE:\n",
        "        # 'default' is safer, 'reduce-overhead' uses CUDA graphs (can conflict with TF)\n",
        "        COMPILE_MODE = 'default'  # Use 'default' for stability, 'reduce-overhead' for speed\n",
        "        print(f\"\\nCompiling with mode='{COMPILE_MODE}'...\")\n",
        "        sys.stdout.flush()\n",
        "        try:\n",
        "            router = router.prepare_and_compile(mode=COMPILE_MODE)\n",
        "            print(\"Compilation done\")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Compilation failed: {e}\")\n",
        "            print(\"Falling back to eager mode\")\n",
        "            COMPILE = False\n",
        "\n",
        "    # Optimizer - check if router is actually compiled\n",
        "    is_compiled = hasattr(router, '_orig_mod')\n",
        "    model_params = router._orig_mod.parameters() if is_compiled else router.parameters()\n",
        "    opt = torch.optim.SGD(model_params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={LR}, momentum=0.9)\")\n",
        "    print(f\"LR Schedule: {LR} for {LR_DROP_EPOCH} epochs → {LR_DROP} for {EPOCHS - LR_DROP_EPOCH} epochs\")\n",
        "    print(f\"Loss: CE + {KL_WEIGHT}×KL\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    best = 0\n",
        "    current_lr = LR\n",
        "    DUAL_EXPERT = True  # Using two experts\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # LR drop at specified epoch\n",
        "        if epoch == LR_DROP_EPOCH:\n",
        "            current_lr = LR_DROP\n",
        "            for param_group in opt.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "            print(f\"\\n>>> LR dropped to {current_lr} <<<\\n\")\n",
        "\n",
        "        loss, ce, kl, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled, kl_weight=KL_WEIGHT, dual_expert=DUAL_EXPERT\n",
        "        )\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, dual_expert=DUAL_EXPERT)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | lr={current_lr:.6f} | loss={loss:.3f} (ce={ce:.3f} kl={kl:.4f}) | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Diagnostics\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=is_compiled, dual_expert=DUAL_EXPERT)\n",
        "    diag.print_summary()\n",
        "\n",
        "    return router\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6iqsZnQmAFM",
        "outputId": "78adef1d-9fb6-4967-bb56-441fe5b87e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Unified Fusion Router\n",
            "GatedFusion + KL Bottleneck + Dual Experts + SGD\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoders: dinov3_convnext_large, dinov3_vitl16\n",
            "Compile: True\n",
            "Epochs: 20, Batch: 128\n",
            "LR: 0.1 → 0.001 at epoch 10\n",
            "Scaling: Expert=0.4, Trans=0.6, Conv=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "VisionCacher: dinov3_vitl16 (1024d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_test.pt\n",
            "Expert dims: 1536 + 1024\n",
            "\n",
            "Params: 774,484,521\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg', 'sinusoidal_pos', 'sinusoidal_neg', 'standard_025', 'standard_050', 'standard_075', 'standard_100']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg']\n",
            "\n",
            "Compiling with mode='default'...\n",
            "Compilation done\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "LR Schedule: 0.1 for 10 epochs → 0.001 for 10 epochs\n",
            "Loss: CE + 0.001×KL\n",
            "Compiled: True\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 390/391 [08:00<00:00,  1.72it/s, acc=59.4%, ce=1.725, kl=0.1939, loss=1.725]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.100000 | loss=1.723 (ce=1.723 kl=0.1941) | train=59.5% | test=87.3% * | 954.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.100000 | loss=0.399 (ce=0.398 kl=0.4136) | train=89.6% | test=90.1% * | 243.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.100000 | loss=0.263 (ce=0.262 kl=0.5169) | train=92.8% | test=90.3% * | 242.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.100000 | loss=0.187 (ce=0.186 kl=0.5887) | train=94.7% | test=90.5% * | 242.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.100000 | loss=0.139 (ce=0.138 kl=0.6217) | train=96.0% | test=90.9% * | 242.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.100000 | loss=0.105 (ce=0.104 kl=0.6574) | train=96.9% | test=91.2% * | 242.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.100000 | loss=0.083 (ce=0.082 kl=0.6708) | train=97.5% | test=91.5% * | 240.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.100000 | loss=0.064 (ce=0.064 kl=0.6597) | train=98.0% | test=91.4% | 242.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.100000 | loss=0.060 (ce=0.059 kl=0.6541) | train=98.2% | test=91.6% * | 242.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.100000 | loss=0.046 (ce=0.045 kl=0.6157) | train=98.6% | test=91.7% * | 242.0s\n",
            "\n",
            ">>> LR dropped to 0.001 <<<\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.001000 | loss=0.024 (ce=0.023 kl=0.6178) | train=99.2% | test=91.9% * | 242.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.001000 | loss=0.018 (ce=0.017 kl=0.6152) | train=99.4% | test=92.1% * | 240.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.001000 | loss=0.017 (ce=0.016 kl=0.6142) | train=99.5% | test=92.1% * | 242.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.001000 | loss=0.015 (ce=0.014 kl=0.6145) | train=99.6% | test=92.1% * | 242.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.001000 | loss=0.014 (ce=0.013 kl=0.6154) | train=99.6% | test=92.2% * | 242.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.001000 | loss=0.013 (ce=0.012 kl=0.6166) | train=99.6% | test=92.2% * | 242.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.001000 | loss=0.012 (ce=0.011 kl=0.6179) | train=99.6% | test=92.2% * | 242.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.001000 | loss=0.012 (ce=0.011 kl=0.6194) | train=99.6% | test=92.2% | 240.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.001000 | loss=0.011 (ce=0.010 kl=0.6213) | train=99.7% | test=92.2% | 242.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.001000 | loss=0.010 (ce=0.010 kl=0.6232) | train=99.7% | test=92.2% | 242.1s\n",
            "\n",
            "======================================================================\n",
            "Best: 92.23%\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TOWER OUTPUT NORMS\n",
            "============================================================\n",
            "\n",
            "Trans towers (scale=0.6):\n",
            "  cantor_pos              5.434\n",
            "  standard_075            5.407\n",
            "  beatrix_pos             5.364\n",
            "  sinusoidal_neg          5.363\n",
            "  cantor_neg              5.347\n",
            "  standard_025            5.314\n",
            "  simplex_neg             5.283\n",
            "  beatrix_neg             5.276\n",
            "  simplex_pos             5.271\n",
            "  sinusoidal_pos          5.269\n",
            "  standard_050            5.263\n",
            "  standard_100            5.261\n",
            "  helix_pos               5.248\n",
            "  helix_neg               5.158\n",
            "\n",
            "Conv towers (scale=1.0):\n",
            "  depth_neg              17.126\n",
            "  coarse_fine_neg        16.420\n",
            "  depth_pos              13.486\n",
            "  frequency_neg          13.338\n",
            "  coarse_fine_pos        12.424\n",
            "  frequency_pos          12.083\n",
            "  wide_resnet_neg        11.325\n",
            "  wide_resnet_pos        11.036\n",
            "\n",
            "----------------------------------------\n",
            "Trans mean norm: 5.304 (×0.6 = 3.182)\n",
            "Conv mean norm:  13.405 (×1.0 = 13.405)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# can't fit through the kitchen door - megawide tower"
      ],
      "metadata": {
        "id": "C1EuFhh8KzRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Unified Fusion Router\n",
        "================================\n",
        "\n",
        "Dual expert architecture with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "Architecture:\n",
        "    Expert1 (ConvNeXt, ×0.4) ──┐\n",
        "    Expert2 (ViT-L, ×0.4)    ──┼─ averaged ─┐\n",
        "                                            ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "    Trans  (×0.6)            ──────────────┤\n",
        "    Conv   (×1.0)            ──────────────┘\n",
        "\n",
        "Loss: CrossEntropy + 0.001 × KL-divergence\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# === DISABLE TENSORFLOW GPU (conflicts with PyTorch in Colab) ===\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''  # No GPU for TF\n",
        "\n",
        "# Suppress all torch logging\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "        'dinov3_vitl16': {\n",
        "            'hf_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "    def load_or_create(self) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Load or create train/test caches.\"\"\"\n",
        "        norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "        tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "        train_ds = datasets.CIFAR100('./data', train=True, transform=tf, download=True)\n",
        "        test_ds = datasets.CIFAR100('./data', train=False, transform=tf, download=True)\n",
        "\n",
        "        return self.build_cache(train_ds, 'train'), self.build_cache(test_ds, 'test')\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents (one or more experts) + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor, latents2: Tensor = None):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "        self.latents2 = latents2  # Optional second expert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        if self.latents2 is not None:\n",
        "            return img, self.latents[i], self.latents2[i], label\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UNIFIED FUSION ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedFusionRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "    Architecture:\n",
        "        Expert1 (convnext) ──┐\n",
        "        Expert2 (vit)      ──┼─ combined ─┐\n",
        "                                          ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "        Trans  (×0.6)      ──────────────┤\n",
        "        Conv   (×1.0)      ──────────────┘\n",
        "\n",
        "    Single head learns shared fusion space. KL regularization encourages\n",
        "    structured latent representations.\n",
        "    \"\"\"\n",
        "\n",
        "    EXPERT_SCALE = 0.4\n",
        "    TRANS_SCALE = 0.6\n",
        "    CONV_SCALE = 1.0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'unified_fusion',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        expert_dim2: int = None,  # Second expert (optional)\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        latent_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "        self.has_expert2 = expert_dim2 is not None\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes, 'expert_dim': expert_dim,\n",
        "            'expert_dim2': expert_dim2, 'latent_dim': latent_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (56 towers) ===\n",
        "        trans_configs = []\n",
        "\n",
        "        # --- Geometric pos/neg pairs (10) ---\n",
        "        trans_configs.extend(preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']))\n",
        "\n",
        "        # --- Standard RoPE scales (14) ---\n",
        "        for scale in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 5.0, 10.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_s{int(scale*100):04d}',\n",
        "                rope='standard', address='standard', inverted=False,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=0.5 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_half_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 0.5},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_half_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 0.5},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=2.0 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_dbl_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 2.0},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_dbl_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 2.0},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=4.0 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_quad_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 4.0},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_quad_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 4.0},\n",
        "            ))\n",
        "\n",
        "        # --- Standard inverted (2) to reach 56 ---\n",
        "        for scale in [1.0, 2.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_inv_s{int(scale*100):04d}',\n",
        "                rope='standard', address='standard', inverted=True,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        # Total trans: 10 + 14 + 10 + 10 + 10 + 2 = 56\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count (WideRouter may discover more)\n",
        "        actual_trans_towers = len(trans_collective.tower_names)\n",
        "        if actual_trans_towers != len(trans_configs):\n",
        "            print(f\"WARNING: Trans tower count mismatch: {len(trans_configs)} configs -> {actual_trans_towers} towers\")\n",
        "        trans_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_trans_towers, fingerprint_dim)\n",
        "        trans_collective._num_towers = actual_trans_towers\n",
        "        # Rebuild fusion for correct input count\n",
        "        trans_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_trans_fusion', num_inputs=actual_trans_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (32 towers) ===\n",
        "        conv_configs = preset_conv_pos_neg()  # 8 base\n",
        "\n",
        "        # Add variants v2, v3, v4 (24 more)\n",
        "        for variant in ['v2', 'v3', 'v4']:\n",
        "            conv_configs.append(ConvTowerConfig(f'depth_{variant}_pos', tower_type='depth', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'depth_{variant}_neg', tower_type='depth', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'frequency_{variant}_pos', tower_type='frequency', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'frequency_{variant}_neg', tower_type='frequency', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'coarse_fine_{variant}_pos', tower_type='coarse_fine', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'coarse_fine_{variant}_neg', tower_type='coarse_fine', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'wide_resnet_{variant}_pos', tower_type='wide_resnet', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'wide_resnet_{variant}_neg', tower_type='wide_resnet', inverted=True))\n",
        "\n",
        "        # Total conv: 8 + 24 = 32\n",
        "        # GRAND TOTAL: 56 + 32 = 88 towers!\n",
        "\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count\n",
        "        actual_conv_towers = len(conv_collective.tower_names)\n",
        "        if actual_conv_towers != len(conv_configs):\n",
        "            print(f\"WARNING: Conv tower count mismatch: {len(conv_configs)} configs -> {actual_conv_towers} towers\")\n",
        "        conv_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_conv_towers, fingerprint_dim)\n",
        "        conv_collective._num_towers = actual_conv_towers\n",
        "        # Rebuild fusion for correct input count\n",
        "        conv_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_conv_fusion', num_inputs=actual_conv_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT PROJECTIONS ===\n",
        "        self.attach('expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(expert_dim),\n",
        "            nn.Linear(expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        if self.has_expert2:\n",
        "            self.attach('expert_proj2', nn.Sequential(\n",
        "                nn.LayerNorm(expert_dim2),\n",
        "                nn.Linear(expert_dim2, dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim, dim),\n",
        "            ))\n",
        "\n",
        "        # === GATED FUSION (3 inputs: expert, trans, conv) ===\n",
        "        self.attach('fusion', GatedFusion(f'{name}_fusion', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # === BOTTLENECK (VAE-style) ===\n",
        "        self.attach('mu_proj', nn.Linear(dim, latent_dim))\n",
        "        self.attach('logvar_proj', nn.Linear(dim, latent_dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.Linear(latent_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Scale buffers\n",
        "        self.register_buffer('expert_scale', torch.tensor(self.EXPERT_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"VAE reparameterization trick.\"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            return mu + torch.randn_like(std) * std\n",
        "        return mu\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor, expert_latents2: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale\n",
        "\n",
        "        # Conv collective\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused * self.conv_scale\n",
        "\n",
        "        # Expert(s)\n",
        "        expert1 = self['expert_proj'](expert_latents)\n",
        "        if self.has_expert2 and expert_latents2 is not None:\n",
        "            expert2 = self['expert_proj2'](expert_latents2)\n",
        "            expert = (expert1 + expert2) * 0.5 * self.expert_scale  # Average and scale\n",
        "        else:\n",
        "            expert = expert1 * self.expert_scale\n",
        "\n",
        "        # Gated fusion\n",
        "        combined = self['fusion'](expert, trans_fused, conv_fused)\n",
        "\n",
        "        # Bottleneck\n",
        "        mu = self['mu_proj'](combined)\n",
        "        logvar = self['logvar_proj'](combined)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self['classifier'](z)\n",
        "\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'UnifiedFusionRouter':\n",
        "        \"\"\"Prepare collectives and compile.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS - Output Norm Analysis\n",
        "# =============================================================================\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Measure tower output norms to assess relative contributions.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False, dual_expert=True):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.dual_expert = dual_expert\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def measure_output_norms(self) -> Dict[str, float]:\n",
        "        \"\"\"Measure mean L2 norm of each tower's output over test set.\"\"\"\n",
        "        self.router.eval()\n",
        "\n",
        "        # Accumulators\n",
        "        trans_norms = {name: [] for name in self.base['trans_collective'].tower_names}\n",
        "        conv_norms = {name: [] for name in self.base['conv_collective'].tower_names}\n",
        "\n",
        "        for batch in self.loader:\n",
        "            if self.dual_expert:\n",
        "                img, _, _, _ = batch  # img, exp1, exp2, lbl\n",
        "            else:\n",
        "                img, _, _ = batch  # img, exp, lbl\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            # Get patch embeddings\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            # Measure trans tower outputs\n",
        "            trans_collective = self.base['trans_collective']\n",
        "            for name in trans_collective.tower_names:\n",
        "                out = trans_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                # Mean L2 norm across batch\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                trans_norms[name].append(norm)\n",
        "\n",
        "            # Measure conv tower outputs\n",
        "            conv_collective = self.base['conv_collective']\n",
        "            for name in conv_collective.tower_names:\n",
        "                out = conv_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                conv_norms[name].append(norm)\n",
        "\n",
        "        # Average across batches\n",
        "        results = {}\n",
        "        for name, norms in trans_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'trans'}\n",
        "        for name, norms in conv_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'conv'}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print output norm analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOWER OUTPUT NORMS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        norms = self.measure_output_norms()\n",
        "\n",
        "        # Separate by collective\n",
        "        trans = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'trans']\n",
        "        conv = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'conv']\n",
        "\n",
        "        trans.sort(key=lambda x: x[1], reverse=True)\n",
        "        conv.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTrans towers (scale={self.base.TRANS_SCALE}):\")\n",
        "        for name, norm in trans:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        print(f\"\\nConv towers (scale={self.base.CONV_SCALE}):\")\n",
        "        for name, norm in conv:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        # Summary\n",
        "        trans_mean = sum(n for _, n in trans) / len(trans) if trans else 0\n",
        "        conv_mean = sum(n for _, n in conv) / len(conv) if conv else 0\n",
        "\n",
        "        print(f\"\\n\" + \"-\" * 40)\n",
        "        print(f\"Trans mean norm: {trans_mean:.3f} (×{self.base.TRANS_SCALE} = {trans_mean * self.base.TRANS_SCALE:.3f})\")\n",
        "        print(f\"Conv mean norm:  {conv_mean:.3f} (×{self.base.CONV_SCALE} = {conv_mean * self.base.CONV_SCALE:.3f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def kl_divergence(mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "    \"\"\"KL divergence from N(mu, sigma) to N(0, 1).\"\"\"\n",
        "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def train_epoch(router, loader, opt, device, is_compiled=False, kl_weight=0.001, dual_expert=False):\n",
        "    \"\"\"Train one epoch with CE + KL loss.\"\"\"\n",
        "    router.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ce_loss = F.cross_entropy(logits, lbl)\n",
        "        kl_loss = kl_divergence(mu, logvar)\n",
        "        loss = ce_loss + kl_weight * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        total_ce += ce_loss.item() * lbl.size(0)\n",
        "        total_kl += kl_loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            ce=f'{total_ce/total:.3f}',\n",
        "            kl=f'{total_kl/total:.4f}',\n",
        "            acc=f'{100*correct/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss/total, total_ce/total, total_kl/total, 100*correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, dual_expert=False):\n",
        "    \"\"\"Evaluate accuracy.\"\"\"\n",
        "    router.eval()\n",
        "    correct, total = 0, 0\n",
        "    for batch in loader:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Unified Fusion Router\")\n",
        "    print(\"GatedFusion + KL Bottleneck + Dual Experts + SGD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config - 88 TOWERS MODE\n",
        "    BATCH = 128\n",
        "    EPOCHS = 20\n",
        "    DIM = 256\n",
        "    LATENT_DIM = 256\n",
        "    KL_WEIGHT = 0.001\n",
        "    LR = 0.1\n",
        "    LR_DROP = 0.001  # Drop to this after LR_DROP_EPOCH\n",
        "    LR_DROP_EPOCH = 10\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitl16'  # Second expert\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoders: {VISION_ENCODER}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} → {LR_DROP} at epoch {LR_DROP_EPOCH}\")\n",
        "    print(f\"Scaling: Expert={UnifiedFusionRouter.EXPERT_SCALE}, Trans={UnifiedFusionRouter.TRANS_SCALE}, Conv={UnifiedFusionRouter.CONV_SCALE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features - Expert 1 (ConvNeXt)\n",
        "    cacher1 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat1 = cacher1.build_cache(train_base, 'train')\n",
        "    test_lat1 = cacher1.build_cache(test_base, 'test')\n",
        "    expert_dim = cacher1.dim\n",
        "\n",
        "    # Cache vision features - Expert 2 (ViT-L)\n",
        "    cacher2 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER_2,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat2 = cacher2.build_cache(train_base, 'train')\n",
        "    test_lat2 = cacher2.build_cache(test_base, 'test')\n",
        "    expert_dim2 = cacher2.dim\n",
        "\n",
        "    print(f\"Expert dims: {expert_dim} + {expert_dim2}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat1, train_lat2)\n",
        "    test_ds = CachedDataset(test_base, test_lat1, test_lat2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Build router - 88 TOWERS with fp_proj fix\n",
        "    router = UnifiedFusionRouter(\n",
        "        dim=DIM,\n",
        "        expert_dim=expert_dim,\n",
        "        expert_dim2=expert_dim2,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        patch_size=4,\n",
        "        trans_depth=1,\n",
        "        conv_depth=1,\n",
        "        num_heads=4,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if COMPILE:\n",
        "        # 'default' is safer, 'reduce-overhead' uses CUDA graphs (can conflict with TF)\n",
        "        COMPILE_MODE = 'default'  # Use 'default' for stability, 'reduce-overhead' for speed\n",
        "        print(f\"\\nCompiling with mode='{COMPILE_MODE}'...\")\n",
        "        sys.stdout.flush()\n",
        "        try:\n",
        "            router = router.prepare_and_compile(mode=COMPILE_MODE)\n",
        "            print(\"Compilation done\")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Compilation failed: {e}\")\n",
        "            print(\"Falling back to eager mode\")\n",
        "            COMPILE = False\n",
        "\n",
        "    # Optimizer - check if router is actually compiled\n",
        "    is_compiled = hasattr(router, '_orig_mod')\n",
        "    model_params = router._orig_mod.parameters() if is_compiled else router.parameters()\n",
        "    opt = torch.optim.SGD(model_params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={LR}, momentum=0.9)\")\n",
        "    print(f\"LR Schedule: {LR} for {LR_DROP_EPOCH} epochs → {LR_DROP} for {EPOCHS - LR_DROP_EPOCH} epochs\")\n",
        "    print(f\"Loss: CE + {KL_WEIGHT}×KL\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    best = 0\n",
        "    current_lr = LR\n",
        "    DUAL_EXPERT = True  # Using two experts\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # LR drop at specified epoch\n",
        "        if epoch == LR_DROP_EPOCH:\n",
        "            current_lr = LR_DROP\n",
        "            for param_group in opt.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "            print(f\"\\n>>> LR dropped to {current_lr} <<<\\n\")\n",
        "\n",
        "        loss, ce, kl, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled, kl_weight=KL_WEIGHT, dual_expert=DUAL_EXPERT\n",
        "        )\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, dual_expert=DUAL_EXPERT)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | lr={current_lr:.6f} | loss={loss:.3f} (ce={ce:.3f} kl={kl:.4f}) | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Diagnostics\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=is_compiled, dual_expert=DUAL_EXPERT)\n",
        "    diag.print_summary()\n",
        "\n",
        "    return router\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZ1yBmtpK3Af",
        "outputId": "960edba2-30af-4f12-c355-33763a685a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Unified Fusion Router\n",
            "GatedFusion + KL Bottleneck + Dual Experts + SGD\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoders: dinov3_convnext_large, dinov3_vitl16\n",
            "Compile: True\n",
            "Epochs: 20, Batch: 128\n",
            "LR: 0.1 → 0.001 at epoch 10\n",
            "Scaling: Expert=0.4, Trans=0.6, Conv=1.0\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_convnext_large_test.pt\n",
            "VisionCacher: dinov3_vitl16 (1024d)\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_train.pt\n",
            "Loading cache: encoder_cache/cifar100/dinov3_vitl16_test.pt\n",
            "Expert dims: 1536 + 1024\n",
            "\n",
            "Params: 62,759,585\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg', 'sinusoidal_pos', 'sinusoidal_neg', 'standard_s0010', 'standard_s0020', 'standard_s0030', 'standard_s0040', 'standard_s0050', 'standard_s0060', 'standard_s0075', 'standard_s0100', 'standard_s0125', 'standard_s0150', 'standard_s0200', 'standard_s0300', 'standard_s0500', 'standard_s1000', 'cantor_half_pos', 'cantor_half_neg', 'beatrix_half_pos', 'beatrix_half_neg', 'helix_half_pos', 'helix_half_neg', 'simplex_half_pos', 'simplex_half_neg', 'sinusoidal_half_pos', 'sinusoidal_half_neg', 'cantor_dbl_pos', 'cantor_dbl_neg', 'beatrix_dbl_pos', 'beatrix_dbl_neg', 'helix_dbl_pos', 'helix_dbl_neg', 'simplex_dbl_pos', 'simplex_dbl_neg', 'sinusoidal_dbl_pos', 'sinusoidal_dbl_neg', 'cantor_quad_pos', 'cantor_quad_neg', 'beatrix_quad_pos', 'beatrix_quad_neg', 'helix_quad_pos', 'helix_quad_neg', 'simplex_quad_pos', 'simplex_quad_neg', 'sinusoidal_quad_pos', 'sinusoidal_quad_neg', 'standard_inv_s0100', 'standard_inv_s0200']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg', 'depth_v2_pos', 'depth_v2_neg', 'frequency_v2_pos', 'frequency_v2_neg', 'coarse_fine_v2_pos', 'coarse_fine_v2_neg', 'wide_resnet_v2_pos', 'wide_resnet_v2_neg', 'depth_v3_pos', 'depth_v3_neg', 'frequency_v3_pos', 'frequency_v3_neg', 'coarse_fine_v3_pos', 'coarse_fine_v3_neg', 'wide_resnet_v3_pos', 'wide_resnet_v3_neg', 'depth_v4_pos', 'depth_v4_neg', 'frequency_v4_pos', 'frequency_v4_neg', 'coarse_fine_v4_pos', 'coarse_fine_v4_neg', 'wide_resnet_v4_pos', 'wide_resnet_v4_neg']\n",
            "\n",
            "Compiling with mode='default'...\n",
            "Compilation done\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "LR Schedule: 0.1 for 10 epochs → 0.001 for 10 epochs\n",
            "Loss: CE + 0.001×KL\n",
            "Compiled: True\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 390/391 [07:45<00:00,  3.05it/s, acc=47.5%, ce=2.281, kl=0.5224, loss=2.282]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.100000 | loss=2.278 (ce=2.278 kl=0.5234) | train=47.5% | test=87.8% * | 1064.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.100000 | loss=0.405 (ce=0.403 kl=1.2393) | train=89.2% | test=90.0% * | 136.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.100000 | loss=0.267 (ce=0.265 kl=1.4173) | train=92.6% | test=90.3% * | 133.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.100000 | loss=0.184 (ce=0.182 kl=1.5004) | train=94.7% | test=90.4% * | 132.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.100000 | loss=0.135 (ce=0.133 kl=1.5306) | train=96.0% | test=90.6% * | 133.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.100000 | loss=0.107 (ce=0.105 kl=1.5635) | train=96.7% | test=90.6% | 133.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.100000 | loss=0.088 (ce=0.087 kl=1.5528) | train=97.3% | test=90.5% | 134.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.100000 | loss=0.068 (ce=0.066 kl=1.5722) | train=97.9% | test=90.8% * | 130.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.100000 | loss=0.053 (ce=0.051 kl=1.5102) | train=98.4% | test=90.8% | 134.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.100000 | loss=0.050 (ce=0.049 kl=1.4915) | train=98.5% | test=91.0% * | 134.2s\n",
            "\n",
            ">>> LR dropped to 0.001 <<<\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.001000 | loss=0.032 (ce=0.031 kl=1.4788) | train=99.0% | test=91.5% * | 134.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.001000 | loss=0.022 (ce=0.021 kl=1.4747) | train=99.4% | test=91.7% * | 133.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.001000 | loss=0.019 (ce=0.017 kl=1.4728) | train=99.5% | test=91.9% * | 134.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.001000 | loss=0.017 (ce=0.015 kl=1.4717) | train=99.5% | test=91.9% * | 134.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.001000 | loss=0.015 (ce=0.014 kl=1.4704) | train=99.6% | test=92.0% * | 134.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.001000 | loss=0.014 (ce=0.013 kl=1.4692) | train=99.6% | test=92.0% * | 133.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.001000 | loss=0.014 (ce=0.012 kl=1.4684) | train=99.6% | test=92.0% * | 132.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.001000 | loss=0.013 (ce=0.011 kl=1.4673) | train=99.7% | test=92.0% | 132.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.001000 | loss=0.012 (ce=0.011 kl=1.4666) | train=99.7% | test=92.0% | 133.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.001000 | loss=0.012 (ce=0.010 kl=1.4655) | train=99.7% | test=92.0% * | 133.9s\n",
            "\n",
            "======================================================================\n",
            "Best: 92.01%\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "TOWER OUTPUT NORMS\n",
            "============================================================\n",
            "\n",
            "Trans towers (scale=0.6):\n",
            "  sinusoidal_dbl_pos      2.564\n",
            "  helix_dbl_neg           2.562\n",
            "  standard_s0100          2.547\n",
            "  standard_s0075          2.534\n",
            "  standard_s0050          2.530\n",
            "  standard_s0010          2.519\n",
            "  helix_dbl_pos           2.505\n",
            "  standard_s0500          2.497\n",
            "  beatrix_quad_pos        2.484\n",
            "  sinusoidal_quad_neg     2.481\n",
            "  cantor_half_neg         2.478\n",
            "  sinusoidal_quad_pos     2.473\n",
            "  beatrix_quad_neg        2.473\n",
            "  beatrix_dbl_pos         2.473\n",
            "  simplex_quad_pos        2.471\n",
            "  simplex_dbl_pos         2.471\n",
            "  simplex_quad_neg        2.467\n",
            "  cantor_quad_pos         2.464\n",
            "  simplex_half_neg        2.461\n",
            "  standard_inv_s0100      2.461\n",
            "  sinusoidal_half_neg     2.454\n",
            "  beatrix_dbl_neg         2.451\n",
            "  sinusoidal_pos          2.449\n",
            "  cantor_pos              2.447\n",
            "  helix_quad_pos          2.445\n",
            "  beatrix_pos             2.443\n",
            "  helix_quad_neg          2.443\n",
            "  standard_inv_s0200      2.442\n",
            "  beatrix_neg             2.434\n",
            "  standard_s0150          2.434\n",
            "  helix_half_pos          2.431\n",
            "  standard_s0200          2.424\n",
            "  helix_half_neg          2.424\n",
            "  helix_neg               2.420\n",
            "  cantor_half_pos         2.416\n",
            "  simplex_neg             2.415\n",
            "  standard_s0060          2.413\n",
            "  standard_s0020          2.412\n",
            "  standard_s0030          2.411\n",
            "  standard_s0040          2.411\n",
            "  simplex_pos             2.410\n",
            "  cantor_quad_neg         2.409\n",
            "  sinusoidal_dbl_neg      2.408\n",
            "  sinusoidal_neg          2.407\n",
            "  simplex_dbl_neg         2.398\n",
            "  simplex_half_pos        2.395\n",
            "  standard_s1000          2.394\n",
            "  cantor_dbl_pos          2.391\n",
            "  standard_s0125          2.389\n",
            "  helix_pos               2.384\n",
            "  beatrix_half_pos        2.384\n",
            "  sinusoidal_half_pos     2.367\n",
            "  beatrix_half_neg        2.352\n",
            "  cantor_dbl_neg          2.335\n",
            "  standard_s0300          2.327\n",
            "  cantor_neg              2.311\n",
            "\n",
            "Conv towers (scale=1.0):\n",
            "  wide_resnet_v2_neg      6.355\n",
            "  frequency_v3_pos        6.039\n",
            "  wide_resnet_v4_pos      5.965\n",
            "  wide_resnet_v4_neg      5.879\n",
            "  frequency_v2_neg        5.871\n",
            "  frequency_v4_neg        5.793\n",
            "  coarse_fine_pos         5.724\n",
            "  wide_resnet_v3_pos      5.629\n",
            "  frequency_v4_pos        5.542\n",
            "  depth_v2_neg            5.380\n",
            "  depth_v4_neg            5.364\n",
            "  frequency_v2_pos        5.357\n",
            "  wide_resnet_pos         5.351\n",
            "  coarse_fine_v4_pos      5.327\n",
            "  frequency_pos           5.320\n",
            "  wide_resnet_v3_neg      5.300\n",
            "  frequency_neg           5.300\n",
            "  coarse_fine_v2_neg      5.262\n",
            "  depth_v4_pos            5.248\n",
            "  coarse_fine_v3_neg      5.242\n",
            "  wide_resnet_v2_pos      5.234\n",
            "  depth_v2_pos            5.217\n",
            "  coarse_fine_v3_pos      5.214\n",
            "  depth_pos               5.083\n",
            "  coarse_fine_neg         5.037\n",
            "  depth_neg               5.033\n",
            "  wide_resnet_neg         5.011\n",
            "  depth_v3_pos            4.933\n",
            "  frequency_v3_neg        4.930\n",
            "  coarse_fine_v4_neg      4.875\n",
            "  coarse_fine_v2_pos      4.863\n",
            "  depth_v3_neg            4.844\n",
            "\n",
            "----------------------------------------\n",
            "Trans mean norm: 2.439 (×0.6 = 1.463)\n",
            "Conv mean norm:  5.360 (×1.0 = 5.360)\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# same patch 4 instead of 8"
      ],
      "metadata": {
        "id": "5ld3Lug9j4Eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Unified Fusion Router\n",
        "================================\n",
        "\n",
        "Dual expert architecture with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "Architecture:\n",
        "    Expert1 (ConvNeXt, ×0.4) ──┐\n",
        "    Expert2 (ViT-L, ×0.4)    ──┼─ averaged ─┐\n",
        "                                            ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "    Trans  (×0.6)            ──────────────┤\n",
        "    Conv   (×1.0)            ──────────────┘\n",
        "\n",
        "Loss: CrossEntropy + 0.001 × KL-divergence\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# === DISABLE TENSORFLOW GPU (conflicts with PyTorch in Colab) ===\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''  # No GPU for TF\n",
        "\n",
        "# Suppress all torch logging\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "        'dinov3_vitl16': {\n",
        "            'hf_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "    def load_or_create(self) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Load or create train/test caches.\"\"\"\n",
        "        norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "        tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "        train_ds = datasets.CIFAR100('./data', train=True, transform=tf, download=True)\n",
        "        test_ds = datasets.CIFAR100('./data', train=False, transform=tf, download=True)\n",
        "\n",
        "        return self.build_cache(train_ds, 'train'), self.build_cache(test_ds, 'test')\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents (one or more experts) + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor, latents2: Tensor = None):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "        self.latents2 = latents2  # Optional second expert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        if self.latents2 is not None:\n",
        "            return img, self.latents[i], self.latents2[i], label\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UNIFIED FUSION ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedFusionRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "    Architecture:\n",
        "        Expert1 (convnext) ──┐\n",
        "        Expert2 (vit)      ──┼─ combined ─┐\n",
        "                                          ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "        Trans  (×0.6)      ──────────────┤\n",
        "        Conv   (×1.0)      ──────────────┘\n",
        "\n",
        "    Single head learns shared fusion space. KL regularization encourages\n",
        "    structured latent representations.\n",
        "    \"\"\"\n",
        "\n",
        "    EXPERT_SCALE = 0.4\n",
        "    TRANS_SCALE = 0.6\n",
        "    CONV_SCALE = 1.0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'unified_fusion',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        expert_dim2: int = None,  # Second expert (optional)\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        latent_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "        self.has_expert2 = expert_dim2 is not None\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes, 'expert_dim': expert_dim,\n",
        "            'expert_dim2': expert_dim2, 'latent_dim': latent_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (56 towers) ===\n",
        "        trans_configs = []\n",
        "\n",
        "        # --- Geometric pos/neg pairs (10) ---\n",
        "        trans_configs.extend(preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']))\n",
        "\n",
        "        # --- Standard RoPE scales (14) ---\n",
        "        for scale in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.75, 1.0, 1.25, 1.5, 2.0, 3.0, 5.0, 10.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_s{int(scale*100):04d}',\n",
        "                rope='standard', address='standard', inverted=False,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=0.5 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_half_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 0.5},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_half_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 0.5},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=2.0 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_dbl_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 2.0},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_dbl_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 2.0},\n",
        "            ))\n",
        "\n",
        "        # --- Geometric @ θ=4.0 pos/neg (10) ---\n",
        "        for rope_type in ['cantor', 'beatrix', 'helix', 'simplex', 'sinusoidal']:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_quad_pos', rope=rope_type, address=rope_type, inverted=False,\n",
        "                rope_params={'theta_scale': 4.0},\n",
        "            ))\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'{rope_type}_quad_neg', rope=rope_type, address=rope_type, inverted=True,\n",
        "                rope_params={'theta_scale': 4.0},\n",
        "            ))\n",
        "\n",
        "        # --- Standard inverted (2) to reach 56 ---\n",
        "        for scale in [1.0, 2.0]:\n",
        "            trans_configs.append(TowerConfig(\n",
        "                f'standard_inv_s{int(scale*100):04d}',\n",
        "                rope='standard', address='standard', inverted=True,\n",
        "                rope_params={'theta_scale': scale},\n",
        "            ))\n",
        "\n",
        "        # Total trans: 10 + 14 + 10 + 10 + 10 + 2 = 56\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count (WideRouter may discover more)\n",
        "        actual_trans_towers = len(trans_collective.tower_names)\n",
        "        if actual_trans_towers != len(trans_configs):\n",
        "            print(f\"WARNING: Trans tower count mismatch: {len(trans_configs)} configs -> {actual_trans_towers} towers\")\n",
        "        trans_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_trans_towers, fingerprint_dim)\n",
        "        trans_collective._num_towers = actual_trans_towers\n",
        "        # Rebuild fusion for correct input count\n",
        "        trans_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_trans_fusion', num_inputs=actual_trans_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (32 towers) ===\n",
        "        conv_configs = preset_conv_pos_neg()  # 8 base\n",
        "\n",
        "        # Add variants v2, v3, v4 (24 more)\n",
        "        for variant in ['v2', 'v3', 'v4']:\n",
        "            conv_configs.append(ConvTowerConfig(f'depth_{variant}_pos', tower_type='depth', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'depth_{variant}_neg', tower_type='depth', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'frequency_{variant}_pos', tower_type='frequency', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'frequency_{variant}_neg', tower_type='frequency', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'coarse_fine_{variant}_pos', tower_type='coarse_fine', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'coarse_fine_{variant}_neg', tower_type='coarse_fine', inverted=True))\n",
        "            conv_configs.append(ConvTowerConfig(f'wide_resnet_{variant}_pos', tower_type='wide_resnet', inverted=False))\n",
        "            conv_configs.append(ConvTowerConfig(f'wide_resnet_{variant}_neg', tower_type='wide_resnet', inverted=True))\n",
        "\n",
        "        # Total conv: 8 + 24 = 32\n",
        "        # GRAND TOTAL: 56 + 32 = 88 towers!\n",
        "\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count\n",
        "        actual_conv_towers = len(conv_collective.tower_names)\n",
        "        if actual_conv_towers != len(conv_configs):\n",
        "            print(f\"WARNING: Conv tower count mismatch: {len(conv_configs)} configs -> {actual_conv_towers} towers\")\n",
        "        conv_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_conv_towers, fingerprint_dim)\n",
        "        conv_collective._num_towers = actual_conv_towers\n",
        "        # Rebuild fusion for correct input count\n",
        "        conv_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_conv_fusion', num_inputs=actual_conv_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT PROJECTIONS ===\n",
        "        self.attach('expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(expert_dim),\n",
        "            nn.Linear(expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        if self.has_expert2:\n",
        "            self.attach('expert_proj2', nn.Sequential(\n",
        "                nn.LayerNorm(expert_dim2),\n",
        "                nn.Linear(expert_dim2, dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim, dim),\n",
        "            ))\n",
        "\n",
        "        # === GATED FUSION (3 inputs: expert, trans, conv) ===\n",
        "        self.attach('fusion', GatedFusion(f'{name}_fusion', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # === BOTTLENECK (VAE-style) ===\n",
        "        self.attach('mu_proj', nn.Linear(dim, latent_dim))\n",
        "        self.attach('logvar_proj', nn.Linear(dim, latent_dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.Linear(latent_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Scale buffers\n",
        "        self.register_buffer('expert_scale', torch.tensor(self.EXPERT_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"VAE reparameterization trick.\"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            return mu + torch.randn_like(std) * std\n",
        "        return mu\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor, expert_latents2: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale\n",
        "\n",
        "        # Conv collective\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused * self.conv_scale\n",
        "\n",
        "        # Expert(s)\n",
        "        expert1 = self['expert_proj'](expert_latents)\n",
        "        if self.has_expert2 and expert_latents2 is not None:\n",
        "            expert2 = self['expert_proj2'](expert_latents2)\n",
        "            expert = (expert1 + expert2) * 0.5 * self.expert_scale  # Average and scale\n",
        "        else:\n",
        "            expert = expert1 * self.expert_scale\n",
        "\n",
        "        # Gated fusion\n",
        "        combined = self['fusion'](expert, trans_fused, conv_fused)\n",
        "\n",
        "        # Bottleneck\n",
        "        mu = self['mu_proj'](combined)\n",
        "        logvar = self['logvar_proj'](combined)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self['classifier'](z)\n",
        "\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'UnifiedFusionRouter':\n",
        "        \"\"\"Prepare collectives and compile.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS - Output Norm Analysis\n",
        "# =============================================================================\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Measure tower output norms to assess relative contributions.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False, dual_expert=True):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.dual_expert = dual_expert\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def measure_output_norms(self) -> Dict[str, float]:\n",
        "        \"\"\"Measure mean L2 norm of each tower's output over test set.\"\"\"\n",
        "        self.router.eval()\n",
        "\n",
        "        # Accumulators\n",
        "        trans_norms = {name: [] for name in self.base['trans_collective'].tower_names}\n",
        "        conv_norms = {name: [] for name in self.base['conv_collective'].tower_names}\n",
        "\n",
        "        for batch in self.loader:\n",
        "            if self.dual_expert:\n",
        "                img, _, _, _ = batch  # img, exp1, exp2, lbl\n",
        "            else:\n",
        "                img, _, _ = batch  # img, exp, lbl\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            # Get patch embeddings\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            # Measure trans tower outputs\n",
        "            trans_collective = self.base['trans_collective']\n",
        "            for name in trans_collective.tower_names:\n",
        "                out = trans_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                # Mean L2 norm across batch\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                trans_norms[name].append(norm)\n",
        "\n",
        "            # Measure conv tower outputs\n",
        "            conv_collective = self.base['conv_collective']\n",
        "            for name in conv_collective.tower_names:\n",
        "                out = conv_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                conv_norms[name].append(norm)\n",
        "\n",
        "        # Average across batches\n",
        "        results = {}\n",
        "        for name, norms in trans_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'trans'}\n",
        "        for name, norms in conv_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'conv'}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print output norm analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOWER OUTPUT NORMS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        norms = self.measure_output_norms()\n",
        "\n",
        "        # Separate by collective\n",
        "        trans = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'trans']\n",
        "        conv = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'conv']\n",
        "\n",
        "        trans.sort(key=lambda x: x[1], reverse=True)\n",
        "        conv.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTrans towers (scale={self.base.TRANS_SCALE}):\")\n",
        "        for name, norm in trans:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        print(f\"\\nConv towers (scale={self.base.CONV_SCALE}):\")\n",
        "        for name, norm in conv:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        # Summary\n",
        "        trans_mean = sum(n for _, n in trans) / len(trans) if trans else 0\n",
        "        conv_mean = sum(n for _, n in conv) / len(conv) if conv else 0\n",
        "\n",
        "        print(f\"\\n\" + \"-\" * 40)\n",
        "        print(f\"Trans mean norm: {trans_mean:.3f} (×{self.base.TRANS_SCALE} = {trans_mean * self.base.TRANS_SCALE:.3f})\")\n",
        "        print(f\"Conv mean norm:  {conv_mean:.3f} (×{self.base.CONV_SCALE} = {conv_mean * self.base.CONV_SCALE:.3f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def kl_divergence(mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "    \"\"\"KL divergence from N(mu, sigma) to N(0, 1).\"\"\"\n",
        "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def train_epoch(router, loader, opt, device, is_compiled=False, kl_weight=0.001, dual_expert=False):\n",
        "    \"\"\"Train one epoch with CE + KL loss.\"\"\"\n",
        "    router.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ce_loss = F.cross_entropy(logits, lbl)\n",
        "        kl_loss = kl_divergence(mu, logvar)\n",
        "        loss = ce_loss + kl_weight * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        total_ce += ce_loss.item() * lbl.size(0)\n",
        "        total_kl += kl_loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            ce=f'{total_ce/total:.3f}',\n",
        "            kl=f'{total_kl/total:.4f}',\n",
        "            acc=f'{100*correct/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss/total, total_ce/total, total_kl/total, 100*correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, dual_expert=False):\n",
        "    \"\"\"Evaluate accuracy.\"\"\"\n",
        "    router.eval()\n",
        "    correct, total = 0, 0\n",
        "    for batch in loader:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Unified Fusion Router\")\n",
        "    print(\"GatedFusion + KL Bottleneck + Dual Experts + SGD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config - 88 TOWERS MODE\n",
        "    BATCH = 128\n",
        "    EPOCHS = 20\n",
        "    DIM = 256\n",
        "    LATENT_DIM = 256\n",
        "    KL_WEIGHT = 0.001\n",
        "    LR = 0.1\n",
        "    LR_DROP = 0.001  # Drop to this after LR_DROP_EPOCH\n",
        "    LR_DROP_EPOCH = 10\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitl16'  # Second expert\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoders: {VISION_ENCODER}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} → {LR_DROP} at epoch {LR_DROP_EPOCH}\")\n",
        "    print(f\"Scaling: Expert={UnifiedFusionRouter.EXPERT_SCALE}, Trans={UnifiedFusionRouter.TRANS_SCALE}, Conv={UnifiedFusionRouter.CONV_SCALE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features - Expert 1 (ConvNeXt)\n",
        "    cacher1 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat1 = cacher1.build_cache(train_base, 'train')\n",
        "    test_lat1 = cacher1.build_cache(test_base, 'test')\n",
        "    expert_dim = cacher1.dim\n",
        "\n",
        "    # Cache vision features - Expert 2 (ViT-L)\n",
        "    cacher2 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER_2,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat2 = cacher2.build_cache(train_base, 'train')\n",
        "    test_lat2 = cacher2.build_cache(test_base, 'test')\n",
        "    expert_dim2 = cacher2.dim\n",
        "\n",
        "    print(f\"Expert dims: {expert_dim} + {expert_dim2}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat1, train_lat2)\n",
        "    test_ds = CachedDataset(test_base, test_lat1, test_lat2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Build router - 88 TOWERS with fp_proj fix\n",
        "    router = UnifiedFusionRouter(\n",
        "        dim=DIM,\n",
        "        expert_dim=expert_dim,\n",
        "        expert_dim2=expert_dim2,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        patch_size=4,\n",
        "        trans_depth=1,\n",
        "        conv_depth=1,\n",
        "        num_heads=4,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if COMPILE:\n",
        "        # 'default' is safer, 'reduce-overhead' uses CUDA graphs (can conflict with TF)\n",
        "        COMPILE_MODE = 'default'  # Use 'default' for stability, 'reduce-overhead' for speed\n",
        "        print(f\"\\nCompiling with mode='{COMPILE_MODE}'...\")\n",
        "        sys.stdout.flush()\n",
        "        try:\n",
        "            router = router.prepare_and_compile(mode=COMPILE_MODE)\n",
        "            print(\"Compilation done\")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Compilation failed: {e}\")\n",
        "            print(\"Falling back to eager mode\")\n",
        "            COMPILE = False\n",
        "\n",
        "    # Optimizer - check if router is actually compiled\n",
        "    is_compiled = hasattr(router, '_orig_mod')\n",
        "    model_params = router._orig_mod.parameters() if is_compiled else router.parameters()\n",
        "    opt = torch.optim.SGD(model_params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={LR}, momentum=0.9)\")\n",
        "    print(f\"LR Schedule: {LR} for {LR_DROP_EPOCH} epochs → {LR_DROP} for {EPOCHS - LR_DROP_EPOCH} epochs\")\n",
        "    print(f\"Loss: CE + {KL_WEIGHT}×KL\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    best = 0\n",
        "    current_lr = LR\n",
        "    DUAL_EXPERT = True  # Using two experts\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # LR drop at specified epoch\n",
        "        if epoch == LR_DROP_EPOCH:\n",
        "            current_lr = LR_DROP\n",
        "            for param_group in opt.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "            print(f\"\\n>>> LR dropped to {current_lr} <<<\\n\")\n",
        "\n",
        "        loss, ce, kl, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled, kl_weight=KL_WEIGHT, dual_expert=DUAL_EXPERT\n",
        "        )\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, dual_expert=DUAL_EXPERT)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | lr={current_lr:.6f} | loss={loss:.3f} (ce={ce:.3f} kl={kl:.4f}) | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Diagnostics\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=is_compiled, dual_expert=DUAL_EXPERT)\n",
        "    diag.print_summary()\n",
        "\n",
        "    return router\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z1ppy87Xj5wB",
        "outputId": "45aea2ab-8cbe-41b4-ac4d-f09d21b97535"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Unified Fusion Router\n",
            "GatedFusion + KL Bottleneck + Dual Experts + SGD\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision encoders: dinov3_convnext_large, dinov3_vitl16\n",
            "Compile: True\n",
            "Epochs: 20, Batch: 128\n",
            "LR: 0.1 → 0.001 at epoch 10\n",
            "Scaling: Expert=0.4, Trans=0.6, Conv=1.0\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Building dinov3_convnext_large cache for train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching train: 100%|██████████| 782/782 [01:13<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cached 50000 samples, unloaded encoder\n",
            "Building dinov3_convnext_large cache for test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching test: 100%|██████████| 157/157 [00:14<00:00, 10.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cached 10000 samples, unloaded encoder\n",
            "VisionCacher: dinov3_vitl16 (1024d)\n",
            "Building dinov3_vitl16 cache for train...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching train: 100%|██████████| 782/782 [01:54<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cached 50000 samples, unloaded encoder\n",
            "Building dinov3_vitl16 cache for test...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Caching test: 100%|██████████| 157/157 [00:22<00:00,  6.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Cached 10000 samples, unloaded encoder\n",
            "Expert dims: 1536 + 1024\n",
            "\n",
            "Params: 87,999,137\n",
            "Trans towers: ['cantor_pos', 'cantor_neg', 'beatrix_pos', 'beatrix_neg', 'helix_pos', 'helix_neg', 'simplex_pos', 'simplex_neg', 'sinusoidal_pos', 'sinusoidal_neg', 'standard_s0010', 'standard_s0020', 'standard_s0030', 'standard_s0040', 'standard_s0050', 'standard_s0060', 'standard_s0075', 'standard_s0100', 'standard_s0125', 'standard_s0150', 'standard_s0200', 'standard_s0300', 'standard_s0500', 'standard_s1000', 'cantor_half_pos', 'cantor_half_neg', 'beatrix_half_pos', 'beatrix_half_neg', 'helix_half_pos', 'helix_half_neg', 'simplex_half_pos', 'simplex_half_neg', 'sinusoidal_half_pos', 'sinusoidal_half_neg', 'cantor_dbl_pos', 'cantor_dbl_neg', 'beatrix_dbl_pos', 'beatrix_dbl_neg', 'helix_dbl_pos', 'helix_dbl_neg', 'simplex_dbl_pos', 'simplex_dbl_neg', 'sinusoidal_dbl_pos', 'sinusoidal_dbl_neg', 'cantor_quad_pos', 'cantor_quad_neg', 'beatrix_quad_pos', 'beatrix_quad_neg', 'helix_quad_pos', 'helix_quad_neg', 'simplex_quad_pos', 'simplex_quad_neg', 'sinusoidal_quad_pos', 'sinusoidal_quad_neg', 'standard_inv_s0100', 'standard_inv_s0200']\n",
            "Conv towers: ['depth_pos', 'depth_neg', 'frequency_pos', 'frequency_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'wide_resnet_pos', 'wide_resnet_neg', 'depth_v2_pos', 'depth_v2_neg', 'frequency_v2_pos', 'frequency_v2_neg', 'coarse_fine_v2_pos', 'coarse_fine_v2_neg', 'wide_resnet_v2_pos', 'wide_resnet_v2_neg', 'depth_v3_pos', 'depth_v3_neg', 'frequency_v3_pos', 'frequency_v3_neg', 'coarse_fine_v3_pos', 'coarse_fine_v3_neg', 'wide_resnet_v3_pos', 'wide_resnet_v3_neg', 'depth_v4_pos', 'depth_v4_neg', 'frequency_v4_pos', 'frequency_v4_neg', 'coarse_fine_v4_pos', 'coarse_fine_v4_neg', 'wide_resnet_v4_pos', 'wide_resnet_v4_neg']\n",
            "\n",
            "Compiling with mode='default'...\n",
            "Compilation done\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: SGD(lr=0.1, momentum=0.9)\n",
            "LR Schedule: 0.1 for 10 epochs → 0.001 for 10 epochs\n",
            "Loss: CE + 0.001×KL\n",
            "Compiled: True\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2545611417.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2545611417.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n>>> LR dropped to {current_lr} <<<\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m         loss, ce, kl, train_acc = train_epoch(\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0mis_compiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_compiled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mKL_WEIGHT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual_expert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDUAL_EXPERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2545611417.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(router, loader, opt, device, is_compiled, kl_weight, dual_expert)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdual_expert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2545611417.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatents2\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatents2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msigned\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msigns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                     \u001b[0mmagnitude\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_apply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagnitude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/autoaugment.py\u001b[0m in \u001b[0;36m_apply_op\u001b[0;34m(img, op_name, magnitude, interpolation, fill)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocontrast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Equalize\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mop_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Invert\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m   1511\u001b[0m         \u001b[0m_log_api_usage_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1512\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1513\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"img should be PIL Image. Got {type(img)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mImageOps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/ImageOps.py\u001b[0m in \u001b[0;36mequalize\u001b[0;34m(image, mask)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                     \u001b[0mlut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_lut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2048 wide resnet, 2048 helix tower pair"
      ],
      "metadata": {
        "id": "GwncS-5Kxpau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Unified Fusion Router\n",
        "================================\n",
        "\n",
        "Dual expert architecture with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "Architecture:\n",
        "    Expert1 (ConvNeXt, ×0.4) ──┐\n",
        "    Expert2 (ViT-L, ×0.4)    ──┼─ averaged ─┐\n",
        "                                            ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "    Trans  (×0.6)            ──────────────┤\n",
        "    Conv   (×1.0)            ──────────────┘\n",
        "\n",
        "Loss: CrossEntropy + 0.001 × KL-divergence\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "# === DISABLE TENSORFLOW GPU (conflicts with PyTorch in Colab) ===\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # Suppress TF logging\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''  # No GPU for TF\n",
        "\n",
        "# Suppress all torch logging\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Optional: encoder caching (may not be present)\n",
        "try:\n",
        "    from geofractal.router.components.encoder_data_component import (\n",
        "        MultiVisionEncode, MODEL_REGISTRY,\n",
        "    )\n",
        "    HAS_ENCODER_COMPONENT = True\n",
        "except ImportError:\n",
        "    HAS_ENCODER_COMPONENT = False\n",
        "    MODEL_REGISTRY = {}\n",
        "\n",
        "# Tower builders - WideRouter-based collectives\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATA CACHING (Using EncoderDataComponent)\n",
        "# =============================================================================\n",
        "\n",
        "class VisionCacher:\n",
        "    \"\"\"\n",
        "    Vision encoder caching with automatic VRAM management.\n",
        "\n",
        "    Supports any vision encoder in MODEL_REGISTRY plus custom models.\n",
        "    Automatically unloads model after caching to free VRAM.\n",
        "    \"\"\"\n",
        "\n",
        "    PRESETS = {\n",
        "        'dino_small': 'dinov2_small',\n",
        "        'dino_base': 'dinov2_base',\n",
        "        'dino_large': 'dinov2_large',\n",
        "        'dino_giant': 'dinov2_giant',\n",
        "        'convnext_large': 'convnext_large',\n",
        "    }\n",
        "\n",
        "    # Custom models not in registry\n",
        "    CUSTOM_MODELS = {\n",
        "        'dinov3_convnext_large': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "            'dim': 1536,\n",
        "        },\n",
        "        'dinov3_convnext_small': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-small-pretrain-lvd1689m',\n",
        "            'dim': 768,\n",
        "        },\n",
        "        'dinov3_convnext_base': {\n",
        "            'hf_path': 'facebook/dinov3-convnext-base-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "        'dinov3_vitl16': {\n",
        "            'hf_path': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "            'dim': 1024,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder_name: str = 'dinov2_base',\n",
        "        dataset_name: str = 'cifar100',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "    ):\n",
        "        self.encoder_name = self.PRESETS.get(encoder_name, encoder_name)\n",
        "        self.dataset_name = dataset_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.use_multi_vision = HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY\n",
        "\n",
        "        # Get encoder config\n",
        "        if HAS_ENCODER_COMPONENT and self.encoder_name in MODEL_REGISTRY:\n",
        "            config = MODEL_REGISTRY[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        elif self.encoder_name in self.CUSTOM_MODELS:\n",
        "            config = self.CUSTOM_MODELS[self.encoder_name]\n",
        "            self.dim = config['dim']\n",
        "            self.hf_path = config['hf_path']\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown encoder: {self.encoder_name}\")\n",
        "\n",
        "        print(f\"VisionCacher: {self.encoder_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, split: str) -> Path:\n",
        "        return self.cache_dir / self.dataset_name / f\"{self.encoder_name}_{split}.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(\n",
        "        self,\n",
        "        dataset: Dataset,\n",
        "        split: str,\n",
        "        batch_size: int = 64,\n",
        "        force_rebuild: bool = False,\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build or load cached vision embeddings.\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(split)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building {self.encoder_name} cache for {split}...\")\n",
        "\n",
        "        if self.use_multi_vision:\n",
        "            encoder = MultiVisionEncode(\n",
        "                encoders=[self.encoder_name],\n",
        "                dataset_name=f\"{self.dataset_name}_{split}\",\n",
        "                device=self.device,\n",
        "                cache_enabled=False,\n",
        "                concatenate=True,\n",
        "                pool_output=True,\n",
        "            )\n",
        "\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                features = encoder.encode(images)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            encoder.unload_all()\n",
        "            del encoder\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "        else:\n",
        "            from transformers import AutoModel\n",
        "\n",
        "            model = AutoModel.from_pretrained(self.hf_path).to(self.device).eval()\n",
        "            loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "            latents = []\n",
        "            for batch in tqdm(loader, desc=f\"Caching {split}\"):\n",
        "                images = batch[0] if isinstance(batch, (tuple, list)) else batch\n",
        "                images = images.to(self.device)\n",
        "\n",
        "                if images.shape[-1] != 224:\n",
        "                    images = F.interpolate(images, 224, mode='bilinear', align_corners=False)\n",
        "\n",
        "                out = model(pixel_values=images)\n",
        "                features = out.pooler_output if hasattr(out, 'pooler_output') else out.last_hidden_state.mean(1)\n",
        "                latents.append(features.cpu())\n",
        "\n",
        "            latents = torch.cat(latents, dim=0)\n",
        "\n",
        "            cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "            torch.save(latents, cache_path)\n",
        "\n",
        "            del model\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"✓ Cached {len(latents)} samples, unloaded encoder\")\n",
        "\n",
        "            return latents\n",
        "\n",
        "    def load_or_create(self) -> Tuple[Tensor, Tensor]:\n",
        "        \"\"\"Load or create train/test caches.\"\"\"\n",
        "        norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "        tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "        train_ds = datasets.CIFAR100('./data', train=True, transform=tf, download=True)\n",
        "        test_ds = datasets.CIFAR100('./data', train=False, transform=tf, download=True)\n",
        "\n",
        "        return self.build_cache(train_ds, 'train'), self.build_cache(test_ds, 'test')\n",
        "\n",
        "\n",
        "# Legacy alias\n",
        "DinoCacher = VisionCacher\n",
        "\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset wrapper: images + cached latents (one or more experts) + labels.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, latents: Tensor, latents2: Tensor = None):\n",
        "        self.base = base\n",
        "        self.latents = latents\n",
        "        self.latents2 = latents2  # Optional second expert\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        img, label = self.base[i]\n",
        "        if self.latents2 is not None:\n",
        "            return img, self.latents[i], self.latents2[i], label\n",
        "        return img, self.latents[i], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(TorchComponent):\n",
        "    \"\"\"Patch embedding for CIFAR images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8, img_size: int = 32):\n",
        "        super().__init__(name)\n",
        "        n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch_size, patch_size)\n",
        "        self.pos = nn.Parameter(torch.randn(1, n_patches, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# UNIFIED FUSION ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class UnifiedFusionRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    CIFAR-100 classifier with unified GatedFusion and KL bottleneck.\n",
        "\n",
        "    Architecture:\n",
        "        Expert1 (convnext) ──┐\n",
        "        Expert2 (vit)      ──┼─ combined ─┐\n",
        "                                          ├── GatedFusion ── Bottleneck(μ,σ) ── Classifier\n",
        "        Trans  (×0.6)      ──────────────┤\n",
        "        Conv   (×1.0)      ──────────────┘\n",
        "\n",
        "    Single head learns shared fusion space. KL regularization encourages\n",
        "    structured latent representations.\n",
        "    \"\"\"\n",
        "\n",
        "    EXPERT_SCALE = 0.4\n",
        "    TRANS_SCALE = 0.6\n",
        "    CONV_SCALE = 1.0\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'unified_fusion',\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        expert_dim: int = 1536,\n",
        "        expert_dim2: int = None,  # Second expert (optional)\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        latent_dim: int = 256,\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "        self.has_expert2 = expert_dim2 is not None\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes, 'expert_dim': expert_dim,\n",
        "            'expert_dim2': expert_dim2, 'latent_dim': latent_dim,\n",
        "        }\n",
        "\n",
        "        # === PATCH EMBEDDING ===\n",
        "        self.attach('patch_embed', PatchEmbed(f'{name}_patch', dim, patch_size))\n",
        "\n",
        "        # === TRANSFORMER COLLECTIVE (2 towers - helix pair) ===\n",
        "        trans_configs = [\n",
        "            TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "            TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "        ]\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count\n",
        "        actual_trans_towers = len(trans_collective.tower_names)\n",
        "        if actual_trans_towers != len(trans_configs):\n",
        "            print(f\"WARNING: Trans tower count mismatch: {len(trans_configs)} configs -> {actual_trans_towers} towers\")\n",
        "        trans_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_trans_towers, fingerprint_dim)\n",
        "        trans_collective._num_towers = actual_trans_towers\n",
        "        trans_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_trans_fusion', num_inputs=actual_trans_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV COLLECTIVE (2 towers - wide_resnet pair) ===\n",
        "        conv_configs = [\n",
        "            ConvTowerConfig('wide_resnet_pos', tower_type='wide_resnet', inverted=False),\n",
        "            ConvTowerConfig('wide_resnet_neg', tower_type='wide_resnet', inverted=True),\n",
        "        ]\n",
        "\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "\n",
        "        # FIX: Resize fp_proj to match actual tower count\n",
        "        actual_conv_towers = len(conv_collective.tower_names)\n",
        "        if actual_conv_towers != len(conv_configs):\n",
        "            print(f\"WARNING: Conv tower count mismatch: {len(conv_configs)} configs -> {actual_conv_towers} towers\")\n",
        "        conv_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_conv_towers, fingerprint_dim)\n",
        "        conv_collective._num_towers = actual_conv_towers\n",
        "        conv_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_conv_fusion', num_inputs=actual_conv_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === EXPERT PROJECTIONS ===\n",
        "        self.attach('expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(expert_dim),\n",
        "            nn.Linear(expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        if self.has_expert2:\n",
        "            self.attach('expert_proj2', nn.Sequential(\n",
        "                nn.LayerNorm(expert_dim2),\n",
        "                nn.Linear(expert_dim2, dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(dim, dim),\n",
        "            ))\n",
        "\n",
        "        # === GATED FUSION (3 inputs: expert, trans, conv) ===\n",
        "        self.attach('fusion', GatedFusion(f'{name}_fusion', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # === BOTTLENECK (VAE-style) ===\n",
        "        self.attach('mu_proj', nn.Linear(dim, latent_dim))\n",
        "        self.attach('logvar_proj', nn.Linear(dim, latent_dim))\n",
        "\n",
        "        # === CLASSIFIER ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.Linear(latent_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # Scale buffers\n",
        "        self.register_buffer('expert_scale', torch.tensor(self.EXPERT_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "    @property\n",
        "    def trans_collective(self) -> ConfigurableCollective:\n",
        "        return self['trans_collective']\n",
        "\n",
        "    @property\n",
        "    def conv_collective(self) -> ConvTowerCollective:\n",
        "        return self['conv_collective']\n",
        "\n",
        "    @torch.compiler.disable\n",
        "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "        \"\"\"VAE reparameterization trick.\"\"\"\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            return mu + torch.randn_like(std) * std\n",
        "        return mu\n",
        "\n",
        "    def forward(self, images: Tensor, expert_latents: Tensor, expert_latents2: Tensor = None) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        # Patch embed\n",
        "        x = self['patch_embed'](images)\n",
        "\n",
        "        # Transformer collective\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale\n",
        "\n",
        "        # Conv collective\n",
        "        conv_fused, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused * self.conv_scale\n",
        "\n",
        "        # Expert(s)\n",
        "        expert1 = self['expert_proj'](expert_latents)\n",
        "        if self.has_expert2 and expert_latents2 is not None:\n",
        "            expert2 = self['expert_proj2'](expert_latents2)\n",
        "            expert = (expert1 + expert2) * 0.5 * self.expert_scale  # Average and scale\n",
        "        else:\n",
        "            expert = expert1 * self.expert_scale\n",
        "\n",
        "        # Gated fusion\n",
        "        combined = self['fusion'](expert, trans_fused, conv_fused)\n",
        "\n",
        "        # Bottleneck\n",
        "        mu = self['mu_proj'](combined)\n",
        "        logvar = self['logvar_proj'](combined)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self['classifier'](z)\n",
        "\n",
        "        return logits, mu, logvar\n",
        "\n",
        "    def prepare_and_compile(self, **compile_kwargs) -> 'UnifiedFusionRouter':\n",
        "        \"\"\"Prepare collectives and compile.\"\"\"\n",
        "        self.trans_collective.analyze_structure()\n",
        "        self.conv_collective.analyze_structure()\n",
        "        return torch.compile(self, **compile_kwargs)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TOWER DIAGNOSTICS - Output Norm Analysis\n",
        "# =============================================================================\n",
        "\n",
        "class TowerDiagnostics:\n",
        "    \"\"\"Measure tower output norms to assess relative contributions.\"\"\"\n",
        "\n",
        "    def __init__(self, router, loader, device, is_compiled=False, dual_expert=True):\n",
        "        self.router = router\n",
        "        self.loader = loader\n",
        "        self.device = device\n",
        "        self.is_compiled = is_compiled\n",
        "        self.dual_expert = dual_expert\n",
        "        self.base = router._orig_mod if is_compiled else router\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def measure_output_norms(self) -> Dict[str, float]:\n",
        "        \"\"\"Measure mean L2 norm of each tower's output over test set.\"\"\"\n",
        "        self.router.eval()\n",
        "\n",
        "        # Accumulators\n",
        "        trans_norms = {name: [] for name in self.base['trans_collective'].tower_names}\n",
        "        conv_norms = {name: [] for name in self.base['conv_collective'].tower_names}\n",
        "\n",
        "        for batch in self.loader:\n",
        "            if self.dual_expert:\n",
        "                img, _, _, _ = batch  # img, exp1, exp2, lbl\n",
        "            else:\n",
        "                img, _, _ = batch  # img, exp, lbl\n",
        "            img = img.to(self.device)\n",
        "\n",
        "            # Get patch embeddings\n",
        "            x = self.base['patch_embed'](img)\n",
        "\n",
        "            # Measure trans tower outputs\n",
        "            trans_collective = self.base['trans_collective']\n",
        "            for name in trans_collective.tower_names:\n",
        "                out = trans_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                # Mean L2 norm across batch\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                trans_norms[name].append(norm)\n",
        "\n",
        "            # Measure conv tower outputs\n",
        "            conv_collective = self.base['conv_collective']\n",
        "            for name in conv_collective.tower_names:\n",
        "                out = conv_collective[name](x)\n",
        "                if isinstance(out, tuple):\n",
        "                    out = out[0]\n",
        "                norm = out.norm(dim=-1).mean().item()\n",
        "                conv_norms[name].append(norm)\n",
        "\n",
        "        # Average across batches\n",
        "        results = {}\n",
        "        for name, norms in trans_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'trans'}\n",
        "        for name, norms in conv_norms.items():\n",
        "            results[name] = {'norm': sum(norms)/len(norms), 'collective': 'conv'}\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_summary(self):\n",
        "        \"\"\"Print output norm analysis.\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TOWER OUTPUT NORMS\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        norms = self.measure_output_norms()\n",
        "\n",
        "        # Separate by collective\n",
        "        trans = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'trans']\n",
        "        conv = [(n, d['norm']) for n, d in norms.items() if d['collective'] == 'conv']\n",
        "\n",
        "        trans.sort(key=lambda x: x[1], reverse=True)\n",
        "        conv.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"\\nTrans towers (scale={self.base.TRANS_SCALE}):\")\n",
        "        for name, norm in trans:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        print(f\"\\nConv towers (scale={self.base.CONV_SCALE}):\")\n",
        "        for name, norm in conv:\n",
        "            print(f\"  {name:<20} {norm:>8.3f}\")\n",
        "\n",
        "        # Summary\n",
        "        trans_mean = sum(n for _, n in trans) / len(trans) if trans else 0\n",
        "        conv_mean = sum(n for _, n in conv) / len(conv) if conv else 0\n",
        "\n",
        "        print(f\"\\n\" + \"-\" * 40)\n",
        "        print(f\"Trans mean norm: {trans_mean:.3f} (×{self.base.TRANS_SCALE} = {trans_mean * self.base.TRANS_SCALE:.3f})\")\n",
        "        print(f\"Conv mean norm:  {conv_mean:.3f} (×{self.base.CONV_SCALE} = {conv_mean * self.base.CONV_SCALE:.3f})\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def kl_divergence(mu: Tensor, logvar: Tensor) -> Tensor:\n",
        "    \"\"\"KL divergence from N(mu, sigma) to N(0, 1).\"\"\"\n",
        "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "\n",
        "\n",
        "def train_epoch(router, loader, opt, device, is_compiled=False, kl_weight=0.001, dual_expert=False):\n",
        "    \"\"\"Train one epoch with CE + KL loss.\"\"\"\n",
        "    router.train()\n",
        "\n",
        "    total_loss, total_ce, total_kl = 0, 0, 0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, mu, logvar = router(img, exp)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        ce_loss = F.cross_entropy(logits, lbl)\n",
        "        kl_loss = kl_divergence(mu, logvar)\n",
        "        loss = ce_loss + kl_weight * kl_loss\n",
        "\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item() * lbl.size(0)\n",
        "        total_ce += ce_loss.item() * lbl.size(0)\n",
        "        total_kl += kl_loss.item() * lbl.size(0)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "\n",
        "        pbar.set_postfix(\n",
        "            loss=f'{total_loss/total:.3f}',\n",
        "            ce=f'{total_ce/total:.3f}',\n",
        "            kl=f'{total_kl/total:.4f}',\n",
        "            acc=f'{100*correct/total:.1f}%'\n",
        "        )\n",
        "\n",
        "    return total_loss/total, total_ce/total, total_kl/total, 100*correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device, dual_expert=False):\n",
        "    \"\"\"Evaluate accuracy.\"\"\"\n",
        "    router.eval()\n",
        "    correct, total = 0, 0\n",
        "    for batch in loader:\n",
        "        if dual_expert:\n",
        "            img, exp1, exp2, lbl = batch\n",
        "            img, exp1, exp2, lbl = img.to(device), exp1.to(device), exp2.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp1, exp2)\n",
        "        else:\n",
        "            img, exp, lbl = batch\n",
        "            img, exp, lbl = img.to(device), exp.to(device), lbl.to(device)\n",
        "            logits, _, _ = router(img, exp)\n",
        "        correct += logits.argmax(1).eq(lbl).sum().item()\n",
        "        total += lbl.size(0)\n",
        "    return 100 * correct / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Unified Fusion Router\")\n",
        "    print(\"GatedFusion + KL Bottleneck + Dual Experts + SGD\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Config - 4 MASSIVE TOWERS (2 helix + 2 wide_resnet)\n",
        "    BATCH = 128\n",
        "    EPOCHS = 20\n",
        "    DIM = 2048\n",
        "    LATENT_DIM = 2048\n",
        "    KL_WEIGHT = 0.001\n",
        "    LR = 0.1\n",
        "    LR_DROP = 0.001  # Drop to this after LR_DROP_EPOCH\n",
        "    LR_DROP_EPOCH = 10\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    VISION_ENCODER = 'dinov3_convnext_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitl16'  # Second expert\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision encoders: {VISION_ENCODER}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} → {LR_DROP} at epoch {LR_DROP_EPOCH}\")\n",
        "    print(f\"Scaling: Expert={UnifiedFusionRouter.EXPERT_SCALE}, Trans={UnifiedFusionRouter.TRANS_SCALE}, Conv={UnifiedFusionRouter.CONV_SCALE}\")\n",
        "\n",
        "    # Transforms\n",
        "    norm = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.262))\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "        transforms.ToTensor(), norm,\n",
        "    ])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), norm])\n",
        "\n",
        "    # Datasets\n",
        "    train_base = datasets.CIFAR100('./data', train=True, download=True, transform=test_tf)\n",
        "    test_base = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision features - Expert 1 (ConvNeXt)\n",
        "    cacher1 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat1 = cacher1.build_cache(train_base, 'train')\n",
        "    test_lat1 = cacher1.build_cache(test_base, 'test')\n",
        "    expert_dim = cacher1.dim\n",
        "\n",
        "    # Cache vision features - Expert 2 (ViT-L)\n",
        "    cacher2 = VisionCacher(\n",
        "        encoder_name=VISION_ENCODER_2,\n",
        "        dataset_name='cifar100',\n",
        "        cache_dir='./encoder_cache',\n",
        "        device=DEVICE,\n",
        "    )\n",
        "    train_lat2 = cacher2.build_cache(train_base, 'train')\n",
        "    test_lat2 = cacher2.build_cache(test_base, 'test')\n",
        "    expert_dim2 = cacher2.dim\n",
        "\n",
        "    print(f\"Expert dims: {expert_dim} + {expert_dim2}\")\n",
        "\n",
        "    train_aug = datasets.CIFAR100('./data', train=True, transform=train_tf)\n",
        "    train_ds = CachedDataset(train_aug, train_lat1, train_lat2)\n",
        "    test_ds = CachedDataset(test_base, test_lat1, test_lat2)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, BATCH, shuffle=True, num_workers=0)\n",
        "    test_loader = DataLoader(test_ds, BATCH, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Build router - 4 MASSIVE towers (helix + wide_resnet), depth=2\n",
        "    router = UnifiedFusionRouter(\n",
        "        dim=DIM,\n",
        "        expert_dim=expert_dim,\n",
        "        expert_dim2=expert_dim2,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        patch_size=8,\n",
        "        trans_depth=2,     # 2 layers per tower\n",
        "        conv_depth=2,      # 2 layers per tower\n",
        "        num_heads=16,      # 2048/16 = 128 head_dim\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters())\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Trans towers: {router.trans_collective.tower_names}\")\n",
        "    print(f\"Conv towers: {router.conv_collective.tower_names}\")\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    if COMPILE:\n",
        "        # 'default' is safer, 'reduce-overhead' uses CUDA graphs (can conflict with TF)\n",
        "        COMPILE_MODE = 'default'  # Use 'default' for stability, 'reduce-overhead' for speed\n",
        "        print(f\"\\nCompiling with mode='{COMPILE_MODE}'...\")\n",
        "        sys.stdout.flush()\n",
        "        try:\n",
        "            router = router.prepare_and_compile(mode=COMPILE_MODE)\n",
        "            print(\"Compilation done\")\n",
        "            sys.stdout.flush()\n",
        "        except Exception as e:\n",
        "            print(f\"Compilation failed: {e}\")\n",
        "            print(\"Falling back to eager mode\")\n",
        "            COMPILE = False\n",
        "\n",
        "    # Optimizer - check if router is actually compiled\n",
        "    is_compiled = hasattr(router, '_orig_mod')\n",
        "    model_params = router._orig_mod.parameters() if is_compiled else router.parameters()\n",
        "    opt = torch.optim.SGD(model_params, lr=LR, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    # Train\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: SGD(lr={LR}, momentum=0.9)\")\n",
        "    print(f\"LR Schedule: {LR} for {LR_DROP_EPOCH} epochs → {LR_DROP} for {EPOCHS - LR_DROP_EPOCH} epochs\")\n",
        "    print(f\"Loss: CE + {KL_WEIGHT}×KL\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "    sys.stdout.flush()\n",
        "\n",
        "    best = 0\n",
        "    current_lr = LR\n",
        "    DUAL_EXPERT = True  # Using two experts\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # LR drop at specified epoch\n",
        "        if epoch == LR_DROP_EPOCH:\n",
        "            current_lr = LR_DROP\n",
        "            for param_group in opt.param_groups:\n",
        "                param_group['lr'] = current_lr\n",
        "            print(f\"\\n>>> LR dropped to {current_lr} <<<\\n\")\n",
        "\n",
        "        loss, ce, kl, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled, kl_weight=KL_WEIGHT, dual_expert=DUAL_EXPERT\n",
        "        )\n",
        "        test_acc = evaluate(router, test_loader, DEVICE, dual_expert=DUAL_EXPERT)\n",
        "\n",
        "        marker = \" *\" if test_acc > best else \"\"\n",
        "        best = max(best, test_acc)\n",
        "\n",
        "        print(f\"E{epoch+1:02d} | lr={current_lr:.6f} | loss={loss:.3f} (ce={ce:.3f} kl={kl:.4f}) | \"\n",
        "              f\"train={train_acc:.1f}% | test={test_acc:.1f}%{marker} | {time.time()-t0:.1f}s\")\n",
        "        sys.stdout.flush()\n",
        "\n",
        "    # Final\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best: {best:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Diagnostics\n",
        "    diag = TowerDiagnostics(router, test_loader, DEVICE, is_compiled=is_compiled, dual_expert=DUAL_EXPERT)\n",
        "    diag.print_summary()\n",
        "\n",
        "    return router\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "X-aaA6IBxs-1",
        "outputId": "e273b19e-60ea-4dab-a1bf-c16f8e2eea4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Multi-Modal Router\n",
            "Vision (ConvNeXt + ViT) + Text (Qwen) + Simplex Cross-Modal\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision: dinov3_convnext_large, dinov3_vitl16\n",
            "Text: qwen2.5-1.5b\n",
            "Compile: True\n",
            "Epochs: 20, Batch: 128\n",
            "LR: 0.1 → 0.001 at epoch 10\n",
            "VisionCacher: dinov3_convnext_large (1536d)\n",
            "Building cache: encoder_cache/cifar100/dinov3_convnext_large_train.pt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "property 'device' of 'MultiVisionEncode' object has no setter",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1417791071.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1417791071.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;31m# === Vision Caching ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m     \u001b[0mcacher1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVisionCacher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVISION_ENCODER_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m     \u001b[0mtrain_latents1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcacher1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m     \u001b[0mtest_latents1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcacher1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0mvision_dim1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcacher1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1417791071.py\u001b[0m in \u001b[0;36mbuild_cache\u001b[0;34m(self, dataset, split)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Use MultiVisionEncode for proper encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         encoder = MultiVisionEncode(\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0mencoders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistry_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/components/encoder_data_component.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, encoders, dataset_name, device, dtype, cache_enabled, cache_dir, concatenate, image_size, pool_output)\u001b[0m\n\u001b[1;32m   1781\u001b[0m         \u001b[0mpool_output\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Pool to [B, D] or keep [B, N, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m     ):\n\u001b[0;32m-> 1783\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m   1784\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multi_vision_encode'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m             \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/components/encoder_data_component.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, dataset_name, device, dtype, cache_enabled, cache_dir, cache_max_memory_mb)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: property 'device' of 'MultiVisionEncode' object has no setter"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multimodal"
      ],
      "metadata": {
        "id": "RQh8fA9DBgK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Multi-Modal Router\n",
        "=============================\n",
        "\n",
        "Architecture integrating:\n",
        "- Vision Experts: DINOv3-ViT-B/16 (768d) + DINOv3-ConvNeXt-Small (768d) - frozen, cached\n",
        "- Text Expert: Qwen2.5-1.5B-Instruct (frozen, cached class embeddings)\n",
        "- Simplex Towers: Cross-modal attention between vision patches and text\n",
        "- Multi-layer Fusion: Hierarchical opinion aggregation\n",
        "\n",
        "The text encoder provides semantic grounding for each class via instruct-style\n",
        "prompts, enabling the simplex towers to learn vision-language associations.\n",
        "\n",
        "Loss: CrossEntropy + KL-divergence regularization\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['CUDA_VISIBLE_DEVICES_FOR_TF'] = ''\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple, Union\n",
        "from collections import defaultdict\n",
        "from pathlib import Path\n",
        "import time\n",
        "import math\n",
        "import logging\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion, GatedFusion\n",
        "\n",
        "# Tower builders\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableTower, ConfigurableCollective,\n",
        "    build_tower_collective, RoPEType, AddressType, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    ConvTowerConfig, ConfigurableConvTower, ConvTowerCollective,\n",
        "    build_conv_collective, ConvTowerType, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEXT SLOT PROJECTION\n",
        "# =============================================================================\n",
        "\n",
        "class TextSlotProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-slot text projection - each class gets multiple prototype embeddings.\n",
        "\n",
        "    [num_classes, text_dim] → [num_classes, num_slots, out_dim]\n",
        "\n",
        "    Instead of compressing 1536d → 512d with one vector per class,\n",
        "    we expand to num_slots learnable prototypes per class.\n",
        "    Vision matches against slots via max-pooling (best prototype match).\n",
        "\n",
        "    This preserves the rich information from 512-token text embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str,\n",
        "        text_dim: int,\n",
        "        out_dim: int,\n",
        "        num_slots: int = 8,\n",
        "        num_classes: int = 100,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.text_dim = text_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.num_slots = num_slots\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Per-class slot embeddings (learnable prototypes)\n",
        "        self.slot_embed = nn.Parameter(\n",
        "            torch.randn(num_classes, num_slots, out_dim) * 0.02\n",
        "        )\n",
        "\n",
        "        # Project text to slot-compatible dim (EXPAND, don't compress!)\n",
        "        self.text_proj = nn.Sequential(\n",
        "            nn.LayerNorm(text_dim),\n",
        "            nn.Linear(text_dim, out_dim * num_slots),  # 1536 → 512*8 = 4096\n",
        "            nn.GELU(),\n",
        "            nn.LayerNorm(out_dim * num_slots),\n",
        "        )\n",
        "\n",
        "        # Per-slot refinement with residual\n",
        "        self.slot_refine = nn.Sequential(\n",
        "            nn.Linear(out_dim, out_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim * 2, out_dim),\n",
        "        )\n",
        "\n",
        "        # Slot-wise attention for weighted combination\n",
        "        self.slot_attention = nn.Sequential(\n",
        "            nn.Linear(out_dim, out_dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(out_dim // 4, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, text_latents: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Project text embeddings to multi-slot prototypes.\n",
        "\n",
        "        Args:\n",
        "            text_latents: [num_classes, text_dim]\n",
        "\n",
        "        Returns:\n",
        "            [num_classes, num_slots, out_dim] slot prototypes\n",
        "        \"\"\"\n",
        "        # Project and reshape to slots\n",
        "        x = self.text_proj(text_latents)  # [100, out_dim * num_slots]\n",
        "        x = x.view(-1, self.num_slots, self.out_dim)  # [100, num_slots, out_dim]\n",
        "\n",
        "        # Add learnable slot prototypes\n",
        "        x = x + self.slot_embed\n",
        "\n",
        "        # Refine each slot with residual\n",
        "        x = x + self.slot_refine(x)\n",
        "\n",
        "        return x  # [100, num_slots, out_dim]\n",
        "\n",
        "    def compute_logits(\n",
        "        self,\n",
        "        vision_embed: Tensor,\n",
        "        text_slots: Tensor,\n",
        "        temperature: float = 1.0,\n",
        "        mode: str = 'max',\n",
        "    ) -> Tensor:\n",
        "        \"\"\"\n",
        "        Compute vision-text similarity logits.\n",
        "\n",
        "        Args:\n",
        "            vision_embed: [B, dim] normalized vision embeddings\n",
        "            text_slots: [num_classes, num_slots, dim] normalized slot prototypes\n",
        "            temperature: softmax temperature\n",
        "            mode: 'max' (best slot), 'mean' (average), 'attention' (learned weights)\n",
        "\n",
        "        Returns:\n",
        "            [B, num_classes] logits\n",
        "        \"\"\"\n",
        "        # Similarity: [B, num_classes, num_slots]\n",
        "        # vision_embed: [B, dim] → [B, 1, 1, dim]\n",
        "        # text_slots: [C, S, dim]\n",
        "        sim = torch.einsum('bd,csd->bcs', vision_embed, text_slots) / temperature\n",
        "\n",
        "        if mode == 'max':\n",
        "            # Best prototype match per class\n",
        "            logits = sim.max(dim=-1).values  # [B, num_classes]\n",
        "        elif mode == 'mean':\n",
        "            # Average over slots\n",
        "            logits = sim.mean(dim=-1)  # [B, num_classes]\n",
        "        elif mode == 'attention':\n",
        "            # Learned attention weights over slots\n",
        "            # Use slot_attention on text_slots\n",
        "            attn_logits = self.slot_attention(text_slots).squeeze(-1)  # [C, S]\n",
        "            attn_weights = F.softmax(attn_logits, dim=-1)  # [C, S]\n",
        "            logits = (sim * attn_weights.unsqueeze(0)).sum(dim=-1)  # [B, C]\n",
        "        elif mode == 'softmax':\n",
        "            # Soft attention based on similarity\n",
        "            slot_weights = F.softmax(sim / 0.1, dim=-1)  # [B, C, S]\n",
        "            logits = (sim * slot_weights).sum(dim=-1)  # [B, C]\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown mode: {mode}\")\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def get_best_slot_embed(self, vision_embed: Tensor, text_slots: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Get the best matching slot embedding for each vision sample.\n",
        "\n",
        "        Args:\n",
        "            vision_embed: [B, dim]\n",
        "            text_slots: [num_classes, num_slots, dim]\n",
        "\n",
        "        Returns:\n",
        "            [B, dim] best matching text embedding per sample\n",
        "        \"\"\"\n",
        "        # [B, C, S]\n",
        "        sim = torch.einsum('bd,csd->bcs', vision_embed, text_slots)\n",
        "\n",
        "        # Best slot per class: [B, C]\n",
        "        best_slot_idx = sim.argmax(dim=-1)\n",
        "\n",
        "        # Best class per sample (based on max similarity)\n",
        "        best_class_sim, best_class_idx = sim.max(dim=-1).values.max(dim=-1)  # [B]\n",
        "\n",
        "        # Gather best slot for best class\n",
        "        B = vision_embed.shape[0]\n",
        "        best_slots = best_slot_idx[torch.arange(B), best_class_idx]  # [B]\n",
        "\n",
        "        # Get the actual embeddings\n",
        "        best_text = text_slots[best_class_idx, best_slots]  # [B, dim]\n",
        "\n",
        "        return best_text\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR-100 CLASS PROMPTS\n",
        "# =============================================================================\n",
        "\n",
        "CIFAR100_CLASSES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "def generate_class_prompts(style: str = 'instruct', max_length: int = 512) -> List[str]:\n",
        "    \"\"\"Generate instruct-style prompts for each CIFAR-100 class.\"\"\"\n",
        "    prompts = []\n",
        "\n",
        "    for cls in CIFAR100_CLASSES:\n",
        "        # Convert underscores to spaces\n",
        "        cls_name = cls.replace('_', ' ')\n",
        "\n",
        "        if style == 'instruct':\n",
        "            # Instruct-style with description request\n",
        "            prompt = f\"Describe the visual characteristics of a {cls_name}. What does it look like in a photograph?\"\n",
        "        elif style == 'simple':\n",
        "            # Simple description\n",
        "            prompt = f\"A photograph of a {cls_name}.\"\n",
        "        elif style == 'detailed':\n",
        "            # More detailed instruct\n",
        "            prompt = f\"You are analyzing an image. Describe the key visual features that would help identify a {cls_name}: its shape, color, texture, and typical context.\"\n",
        "        elif style == 'repeated':\n",
        "            # Repeat class name to fill context - creates more distinctive embeddings\n",
        "            base = f\"{cls_name} \"\n",
        "            # Estimate ~4 chars per token, fill most of max_length\n",
        "            target_chars = max_length * 3  # ~3 chars per token average\n",
        "            repeats = target_chars // len(base)\n",
        "            prompt = (base * repeats).strip()\n",
        "        elif style == 'repeated_phrase':\n",
        "            # Repeat descriptive phrase\n",
        "            base = f\"This is a {cls_name}. \"\n",
        "            target_chars = max_length * 3\n",
        "            repeats = target_chars // len(base)\n",
        "            prompt = (base * repeats).strip()\n",
        "        else:\n",
        "            prompt = f\"This is an image of a {cls_name}.\"\n",
        "\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    return prompts\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEXT ENCODER CACHING (Qwen)\n",
        "# =============================================================================\n",
        "\n",
        "class QwenTextCacher:\n",
        "    \"\"\"Cache Qwen hidden states for class prompts.\"\"\"\n",
        "\n",
        "    # Use same naming convention as MODEL_REGISTRY\n",
        "    QWEN_MODELS = {\n",
        "        'qwen2.5_0.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-0.5B-Instruct', 'dim': 896},\n",
        "        'qwen2.5_1.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-1.5B-Instruct', 'dim': 1536},\n",
        "        'qwen2.5_3b_instruct': {'hf_path': 'Qwen/Qwen2.5-3B-Instruct', 'dim': 2048},\n",
        "        'qwen2.5_7b_instruct': {'hf_path': 'Qwen/Qwen2.5-7B-Instruct', 'dim': 3584},\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = 'qwen2.5_1.5b_instruct',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "        max_length: int = 512,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        config = self.QWEN_MODELS.get(model_name)\n",
        "        if config is None:\n",
        "            raise ValueError(f\"Unknown model: {model_name}. Available: {list(self.QWEN_MODELS.keys())}\")\n",
        "\n",
        "        self.hf_path = config['hf_path']\n",
        "        self.dim = config['dim']\n",
        "\n",
        "        print(f\"QwenTextCacher: {model_name} ({self.dim}d, max_length={max_length})\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, style: str = 'instruct') -> Path:\n",
        "        return self.cache_dir / 'cifar100' / f\"{self.model_name}_{style}_class_embeddings.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, prompt_style: str = 'instruct') -> Tensor:\n",
        "        \"\"\"Build and cache embeddings for all 100 classes.\"\"\"\n",
        "        cache_path = self._cache_path(prompt_style)\n",
        "\n",
        "        if cache_path.exists():\n",
        "            print(f\"Loading cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building cache: {cache_path}\")\n",
        "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Load Qwen model\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "        print(f\"Loading {self.hf_path}...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.hf_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.hf_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=self.device,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        model.eval()\n",
        "\n",
        "        # Generate prompts (pass max_length for repeated styles)\n",
        "        prompts = generate_class_prompts(style=prompt_style, max_length=self.max_length)\n",
        "\n",
        "        # Encode each prompt\n",
        "        embeddings = []\n",
        "        for i, prompt in enumerate(tqdm(prompts, desc=f\"Encoding class prompts ({prompt_style})\")):\n",
        "            # For repeated styles, skip chat template - just tokenize directly\n",
        "            if prompt_style.startswith('repeated'):\n",
        "                inputs = tokenizer(\n",
        "                    prompt,\n",
        "                    return_tensors=\"pt\",\n",
        "                    max_length=self.max_length,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                ).to(self.device)\n",
        "            else:\n",
        "                # Format as instruct message\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at visual recognition and image classification.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ]\n",
        "\n",
        "                text = tokenizer.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "\n",
        "                inputs = tokenizer(\n",
        "                    text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    max_length=self.max_length,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                ).to(self.device)\n",
        "\n",
        "            # Get hidden states\n",
        "            outputs = model(\n",
        "                **inputs,\n",
        "                output_hidden_states=True,\n",
        "                return_dict=True,\n",
        "            )\n",
        "\n",
        "            # Use last hidden state, mean pooled over sequence\n",
        "            last_hidden = outputs.hidden_states[-1]  # [1, seq_len, dim]\n",
        "\n",
        "            # Attention mask for proper pooling\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1)  # [1, seq_len, 1]\n",
        "            pooled = (last_hidden * mask).sum(dim=1) / mask.sum(dim=1)  # [1, dim]\n",
        "\n",
        "            embeddings.append(pooled.cpu().float())\n",
        "\n",
        "        # Stack all embeddings [100, dim]\n",
        "        embeddings = torch.cat(embeddings, dim=0)\n",
        "\n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Save cache\n",
        "        torch.save(embeddings, cache_path)\n",
        "        print(f\"Saved: {cache_path} ({embeddings.shape})\")\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    def load_cache(self) -> Tensor:\n",
        "        \"\"\"Load cached embeddings.\"\"\"\n",
        "        cache_path = self._cache_path()\n",
        "        if not cache_path.exists():\n",
        "            return self.build_cache()\n",
        "        return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# VISION ENCODING - Using fixed MultiVisionEncode\n",
        "# =============================================================================\n",
        "\n",
        "# The fixed encoder_data_component.py should be placed in your working directory\n",
        "# or patch the installed version. To patch at runtime:\n",
        "#\n",
        "#   import sys\n",
        "#   sys.path.insert(0, '/path/to/fixed/')\n",
        "#   from encoder_data_component import MultiVisionEncode\n",
        "#\n",
        "# Or simply overwrite the installed file.\n",
        "\n",
        "from geofractal.router.components.encoder_data_component import MultiVisionEncode\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_vision_cache(\n",
        "    encoder_name: str,\n",
        "    split: str,\n",
        "    dataset_name: str = 'cifar100',\n",
        "    cache_dir: str = './encoder_cache',\n",
        "    device: str = 'cuda',\n",
        ") -> Tuple[Tensor, int]:\n",
        "    \"\"\"\n",
        "    Build or load vision encoder cache using MultiVisionEncode.\n",
        "\n",
        "    Args:\n",
        "        encoder_name: Registry name (e.g., 'convnext_large', 'dinov2_large')\n",
        "        split: 'train' or 'test'\n",
        "        dataset_name: Dataset namespace\n",
        "        cache_dir: Where to store caches\n",
        "        device: Encoding device\n",
        "\n",
        "    Returns:\n",
        "        (latents, dim): Cached embeddings and dimension\n",
        "    \"\"\"\n",
        "    cache_path = Path(cache_dir) / dataset_name / f\"{encoder_name}_{split}.pt\"\n",
        "\n",
        "    # Load if cached\n",
        "    if cache_path.exists():\n",
        "        print(f\"Loading cache: {cache_path}\")\n",
        "        latents = torch.load(cache_path, weights_only=True)\n",
        "        # Infer dim from cache\n",
        "        return latents, latents.shape[-1]\n",
        "\n",
        "    print(f\"Building cache: {cache_path}\")\n",
        "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create encoder\n",
        "    encoder = MultiVisionEncode(\n",
        "        encoders=[encoder_name],\n",
        "        dataset_name=dataset_name,\n",
        "        device=device,\n",
        "        cache_enabled=False,  # We do our own caching\n",
        "        concatenate=True,\n",
        "        pool_output=True,\n",
        "    )\n",
        "\n",
        "    dim = encoder.combined_dim\n",
        "\n",
        "    # Load raw CIFAR100\n",
        "    raw_dataset = datasets.CIFAR100(\n",
        "        './data', train=(split == 'train'), download=True,\n",
        "        transform=transforms.ToTensor(),\n",
        "    )\n",
        "\n",
        "    latents = []\n",
        "    batch_size = 64\n",
        "\n",
        "    for i in tqdm(range(0, len(raw_dataset), batch_size), desc=f\"Encoding {split}\"):\n",
        "        batch_images = []\n",
        "        for j in range(i, min(i + batch_size, len(raw_dataset))):\n",
        "            img, _ = raw_dataset[j]\n",
        "            batch_images.append(img)\n",
        "\n",
        "        batch_tensor = torch.stack(batch_images)\n",
        "        z = encoder.encode(batch_tensor)\n",
        "        latents.append(z.cpu().float())\n",
        "\n",
        "    latents = torch.cat(latents, dim=0)\n",
        "    torch.save(latents, cache_path)\n",
        "    print(f\"Saved: {cache_path} shape={latents.shape}\")\n",
        "\n",
        "    # Cleanup\n",
        "    encoder.unload_all()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return latents, dim\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CACHED DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset with pre-computed vision latents only (CLIP-style).\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        base: Dataset,\n",
        "        vision_latents1: Tensor,\n",
        "        vision_latents2: Tensor = None,\n",
        "    ):\n",
        "        self.base = base\n",
        "        self.vision_latents1 = vision_latents1\n",
        "        self.vision_latents2 = vision_latents2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base[idx]\n",
        "        v1 = self.vision_latents1[idx]\n",
        "\n",
        "        if self.vision_latents2 is not None:\n",
        "            v2 = self.vision_latents2[idx]\n",
        "            return img, v1, v2, label\n",
        "\n",
        "        return img, v1, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class SafeAttachMixin:\n",
        "    \"\"\"Mixin for safe component attachment.\"\"\"\n",
        "    def attach(self, name: str, module: nn.Module):\n",
        "        if hasattr(self, 'components'):\n",
        "            self.components[name] = module\n",
        "        if isinstance(module, nn.Module):\n",
        "            setattr(self, f'_mod_{name}', module)\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Patch embedding for images.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, dim: int, patch_size: int = 8):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x)  # [B, dim, H/p, W/p]\n",
        "        x = x.flatten(2).transpose(1, 2)  # [B, num_patches, dim]\n",
        "        x = self.norm(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    \"\"\"VAE-style bottleneck with KL regularization.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, in_dim: int, latent_dim: int):\n",
        "        super().__init__()\n",
        "        self.to_mu = nn.Linear(in_dim, latent_dim)\n",
        "        self.to_logvar = nn.Linear(in_dim, latent_dim)\n",
        "        self.from_latent = nn.Linear(latent_dim, in_dim)\n",
        "\n",
        "        # Initialize logvar bias to small negative value for stable start\n",
        "        nn.init.constant_(self.to_logvar.bias, -2.0)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
        "        mu = self.to_mu(x)\n",
        "        logvar = self.to_logvar(x)\n",
        "\n",
        "        # Clamp logvar for numerical stability\n",
        "        logvar = torch.clamp(logvar, min=-20.0, max=2.0)\n",
        "\n",
        "        if self.training:\n",
        "            std = torch.exp(0.5 * logvar)\n",
        "            eps = torch.randn_like(std)\n",
        "            z = mu + eps * std\n",
        "        else:\n",
        "            z = mu\n",
        "\n",
        "        out = self.from_latent(z)\n",
        "        return out, mu, logvar\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MULTI-MODAL FUSION ROUTER (CLIP-style Contrastive)\n",
        "# =============================================================================\n",
        "\n",
        "class MultiModalRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    Multi-modal CIFAR-100 classifier with slot-based text matching.\n",
        "\n",
        "    Architecture:\n",
        "        Vision Expert1 (DINOv3) ──┐\n",
        "        Vision Expert2 (DINOv2) ──┼─ vision_fused ─┐\n",
        "                                                    │\n",
        "        Vision patches           ── Helix towers ──┼── GatedFusion ── vision_embed\n",
        "                                                    │\n",
        "        Conv towers              ──────────────────┘\n",
        "\n",
        "        Text embeddings [100, text_dim] ←── pre-computed, one per class\n",
        "            ↓\n",
        "        TextSlotProjection → [100, num_slots, dim] ←── multiple prototypes per class\n",
        "\n",
        "        Hybrid Loss:\n",
        "        1. Direct CE: classifier(vision_combined) → logits (proven to work)\n",
        "        2. Slot Match: cosine(vision_embed, text_slots) → alignment logits\n",
        "        3. MSE: vision_embed ↔ target_text_slot (auxiliary alignment)\n",
        "    \"\"\"\n",
        "\n",
        "    VISION_SCALE = 1.0\n",
        "    TRANS_SCALE = 1.0\n",
        "    CONV_SCALE = 1.0\n",
        "    TEMPERATURE = 0.5\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        name: str = 'multimodal_router',\n",
        "        dim: int = 512,\n",
        "        patch_size: int = 8,\n",
        "        num_classes: int = 100,\n",
        "        vision_dim1: int = 1536,   # DINOv3\n",
        "        vision_dim2: int = 1024,   # DINOv2\n",
        "        text_dim: int = 1536,      # Qwen\n",
        "        text_latents: Tensor = None,  # [100, text_dim] - REQUIRED\n",
        "        trans_depth: int = 2,\n",
        "        conv_depth: int = 2,\n",
        "        num_heads: int = 8,\n",
        "        fingerprint_dim: int = 64,\n",
        "        num_text_slots: int = 8,   # Slots per class\n",
        "    ):\n",
        "        super().__init__(name, strict=False)\n",
        "\n",
        "        if text_latents is None:\n",
        "            raise ValueError(\"text_latents required\")\n",
        "\n",
        "        spatial = 32 // patch_size\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'spatial': spatial,\n",
        "            'num_classes': num_classes,\n",
        "            'vision_dim1': vision_dim1, 'vision_dim2': vision_dim2,\n",
        "            'text_dim': text_dim, 'num_text_slots': num_text_slots,\n",
        "        }\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_text_slots = num_text_slots\n",
        "\n",
        "        # Store scales as buffers\n",
        "        self.register_buffer('vision_scale', torch.tensor(self.VISION_SCALE))\n",
        "        self.register_buffer('trans_scale', torch.tensor(self.TRANS_SCALE))\n",
        "        self.register_buffer('conv_scale', torch.tensor(self.CONV_SCALE))\n",
        "\n",
        "        # Learnable temperature (log scale for numerical stability)\n",
        "        self.log_temperature = nn.Parameter(torch.tensor(math.log(self.TEMPERATURE)))\n",
        "\n",
        "        # === PATCH EMBEDDING (plain nn.Module) ===\n",
        "        self.patch_embed = PatchEmbed(f'{name}_patch', dim, patch_size)\n",
        "\n",
        "        # === VISION EXPERT PROJECTIONS ===\n",
        "        self.attach('vision_proj1', nn.Sequential(\n",
        "            nn.LayerNorm(vision_dim1),\n",
        "            nn.Linear(vision_dim1, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        self.attach('vision_proj2', nn.Sequential(\n",
        "            nn.LayerNorm(vision_dim2),\n",
        "            nn.Linear(vision_dim2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        # === TEXT SLOT PROJECTION (multi-prototype per class) ===\n",
        "        self.text_slot_proj = TextSlotProjection(\n",
        "            f'{name}_text_slots',\n",
        "            text_dim=text_dim,\n",
        "            out_dim=dim,\n",
        "            num_slots=num_text_slots,\n",
        "            num_classes=num_classes,\n",
        "        )\n",
        "\n",
        "        # Store RAW text latents\n",
        "        self.register_buffer('text_latents_raw', text_latents.float())  # [100, text_dim]\n",
        "\n",
        "        # === TRANSFORMER TOWERS (Dual Stack - many rope geometries) ===\n",
        "        trans_configs = [\n",
        "            # Helix pair\n",
        "            TowerConfig('helix_pos', rope='helix', address='helix', inverted=False),\n",
        "            TowerConfig('helix_neg', rope='helix', address='helix', inverted=True),\n",
        "            # Cantor pair\n",
        "            TowerConfig('cantor_pos', rope='cantor', address='cantor', inverted=False),\n",
        "            TowerConfig('cantor_neg', rope='cantor', address='cantor', inverted=True),\n",
        "            # Simplex pair\n",
        "            TowerConfig('simplex_pos', rope='simplex', address='simplex', inverted=False),\n",
        "            TowerConfig('simplex_neg', rope='simplex', address='simplex', inverted=True),\n",
        "            # Golden pair\n",
        "            TowerConfig('golden_pos', rope='golden', address='golden', inverted=False),\n",
        "            TowerConfig('golden_neg', rope='golden', address='golden', inverted=True),\n",
        "            # Fibonacci pair\n",
        "            TowerConfig('fibonacci_pos', rope='fibonacci', address='fibonacci', inverted=False),\n",
        "            TowerConfig('fibonacci_neg', rope='fibonacci', address='fibonacci', inverted=True),\n",
        "            # Beatrix pair\n",
        "            TowerConfig('beatrix_pos', rope='beatrix', address='beatrix', inverted=False),\n",
        "            TowerConfig('beatrix_neg', rope='beatrix', address='beatrix', inverted=True),\n",
        "        ]\n",
        "\n",
        "        trans_collective = build_tower_collective(\n",
        "            configs=trans_configs, dim=dim, default_depth=trans_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name=f'{name}_trans',\n",
        "        )\n",
        "\n",
        "        # Fix fp_proj\n",
        "        actual_trans = len(trans_collective.tower_names)\n",
        "        trans_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_trans, fingerprint_dim)\n",
        "        trans_collective._num_towers = actual_trans\n",
        "        trans_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_trans_fusion', num_inputs=actual_trans, in_features=dim\n",
        "        )\n",
        "        self.attach('trans_collective', trans_collective)\n",
        "\n",
        "        # === CONV TOWERS (Dual Stack - multiple spatial patterns) ===\n",
        "        conv_configs = [\n",
        "            # Wide ResNet pair\n",
        "            ConvTowerConfig('wide_resnet_pos', tower_type='wide_resnet', inverted=False),\n",
        "            ConvTowerConfig('wide_resnet_neg', tower_type='wide_resnet', inverted=True),\n",
        "            # Frequency pair\n",
        "            ConvTowerConfig('frequency_pos', tower_type='frequency', inverted=False),\n",
        "            ConvTowerConfig('frequency_neg', tower_type='frequency', inverted=True),\n",
        "            # Bottleneck pair\n",
        "            ConvTowerConfig('bottleneck_pos', tower_type='bottleneck', inverted=False),\n",
        "            ConvTowerConfig('bottleneck_neg', tower_type='bottleneck', inverted=True),\n",
        "            # Squeeze-Excite pair\n",
        "            ConvTowerConfig('squeeze_excite_pos', tower_type='squeeze_excite', inverted=False),\n",
        "            ConvTowerConfig('squeeze_excite_neg', tower_type='squeeze_excite', inverted=True),\n",
        "            # Dilated pair\n",
        "            ConvTowerConfig('dilated_pos', tower_type='dilated', inverted=False),\n",
        "            ConvTowerConfig('dilated_neg', tower_type='dilated', inverted=True),\n",
        "            # Spatial Attention pair\n",
        "            ConvTowerConfig('spatial_attention_pos', tower_type='spatial_attention', inverted=False),\n",
        "            ConvTowerConfig('spatial_attention_neg', tower_type='spatial_attention', inverted=True),\n",
        "        ]\n",
        "\n",
        "        conv_collective = build_conv_collective(\n",
        "            configs=conv_configs, dim=dim, default_depth=conv_depth,\n",
        "            fingerprint_dim=fingerprint_dim, spatial_size=spatial, name=f'{name}_conv',\n",
        "        )\n",
        "\n",
        "        # Fix fp_proj\n",
        "        actual_conv = len(conv_collective.tower_names)\n",
        "        conv_collective.components['fp_proj'] = nn.Linear(fingerprint_dim * actual_conv, fingerprint_dim)\n",
        "        conv_collective._num_towers = actual_conv\n",
        "        conv_collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{name}_conv_fusion', num_inputs=actual_conv, in_features=dim\n",
        "        )\n",
        "        self.attach('conv_collective', conv_collective)\n",
        "\n",
        "        # === GATED FUSION (3 streams: vision_combined, trans, conv) ===\n",
        "        self.attach('gated_fusion', GatedFusion(\n",
        "            f'{name}_gated', num_inputs=3, in_features=dim\n",
        "        ))\n",
        "\n",
        "        # === DIRECT CLASSIFIER HEAD (what worked before - CE on fused features) ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # === VISION EMBEDDING HEAD (for text alignment) ===\n",
        "        self.attach('vision_embed_proj', nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: Tensor,\n",
        "        vision_latent1: Tensor,\n",
        "        vision_latent2: Tensor,\n",
        "        labels: Optional[Tensor] = None,\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Hybrid forward pass with slot-based text matching.\n",
        "\n",
        "        Args:\n",
        "            images: [B, 3, 32, 32] raw images\n",
        "            vision_latent1: [B, vision_dim1] DINOv3 features\n",
        "            vision_latent2: [B, vision_dim2] DINOv2 features\n",
        "            labels: [B] optional ground truth labels for MSE alignment\n",
        "\n",
        "        Returns:\n",
        "            Dict with:\n",
        "                - logits: [B, num_classes] direct classification logits\n",
        "                - slot_logits: [B, num_classes] slot-based similarity logits\n",
        "                - vision_embed: [B, dim] normalized vision embedding\n",
        "                - text_slots: [num_classes, num_slots, dim] text slot prototypes\n",
        "                - mse_target: [B, dim] target text embedding (if labels provided)\n",
        "        \"\"\"\n",
        "        # === Vision Expert Processing ===\n",
        "        v1 = self['vision_proj1'](vision_latent1)  # [B, dim]\n",
        "        v2 = self['vision_proj2'](vision_latent2)  # [B, dim]\n",
        "        vision_combined = (v1 + v2) * 0.5 * self.vision_scale\n",
        "\n",
        "        # === Patch Processing ===\n",
        "        x = self.patch_embed(images)  # [B, num_patches, dim]\n",
        "\n",
        "        # === Transformer Towers (vision patches) ===\n",
        "        trans_out = self['trans_collective'](x)\n",
        "        trans_fused = trans_out.fused * self.trans_scale  # [B, dim]\n",
        "\n",
        "        # === Conv Towers (spatial patterns) ===\n",
        "        conv_fused_raw, _ = self['conv_collective'](x)\n",
        "        conv_fused = conv_fused_raw * self.conv_scale  # [B, dim]\n",
        "\n",
        "        # === Gated Fusion (3 streams) ===\n",
        "        fused = self['gated_fusion'](vision_combined, trans_fused, conv_fused)  # [B, dim]\n",
        "\n",
        "        # === Direct Classification (proven to work) ===\n",
        "        logits = self['classifier'](fused)  # [B, num_classes]\n",
        "\n",
        "        # === Vision Embedding for Text Alignment ===\n",
        "        vision_embed = self['vision_embed_proj'](fused)  # [B, dim]\n",
        "        vision_embed = F.normalize(vision_embed, dim=-1)\n",
        "\n",
        "        # === Text Slot Projection ===\n",
        "        text_latents = self.text_latents_raw.to(vision_embed.device)\n",
        "        text_slots = self.text_slot_proj(text_latents)  # [100, num_slots, dim]\n",
        "        text_slots = F.normalize(text_slots, dim=-1)\n",
        "\n",
        "        # === Slot-based Logits ===\n",
        "        temperature = torch.exp(self.log_temperature).clamp(min=0.01, max=2.0)\n",
        "        slot_logits = self.text_slot_proj.compute_logits(\n",
        "            vision_embed, text_slots, temperature=temperature, mode='max'\n",
        "        )  # [B, num_classes]\n",
        "\n",
        "        result = {\n",
        "            'logits': logits,\n",
        "            'slot_logits': slot_logits,\n",
        "            'vision_embed': vision_embed,\n",
        "            'text_slots': text_slots,\n",
        "        }\n",
        "\n",
        "        # === MSE Target (ground truth class slot) ===\n",
        "        if labels is not None:\n",
        "            # Get best matching slot for ground truth class\n",
        "            # text_slots: [C, S, dim], labels: [B]\n",
        "            B = labels.shape[0]\n",
        "            gt_slots = text_slots[labels]  # [B, S, dim]\n",
        "\n",
        "            # Find best slot per sample via similarity\n",
        "            sim_to_gt = torch.einsum('bd,bsd->bs', vision_embed, gt_slots)  # [B, S]\n",
        "            best_slot_idx = sim_to_gt.argmax(dim=-1)  # [B]\n",
        "\n",
        "            # Gather the best slot embedding\n",
        "            mse_target = gt_slots[torch.arange(B, device=labels.device), best_slot_idx]  # [B, dim]\n",
        "            result['mse_target'] = mse_target\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING UTILITIES\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(\n",
        "    router: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    is_compiled: bool = False,\n",
        "    mse_weight: float = 0.1,\n",
        "    slot_weight: float = 0.3,\n",
        "    max_grad_norm: float = 1.0,\n",
        ") -> Tuple[float, float, float, float, float]:\n",
        "    \"\"\"\n",
        "    Train one epoch with hybrid loss.\n",
        "\n",
        "    Loss = CE(logits) + slot_weight * CE(slot_logits) + mse_weight * MSE(vision, target_text)\n",
        "\n",
        "    - CE on direct logits: proven to work\n",
        "    - CE on slot logits: encourages text alignment\n",
        "    - MSE: vision embedding matches best text slot\n",
        "    \"\"\"\n",
        "    router.train()\n",
        "    total_loss = 0.0\n",
        "    total_ce, total_slot_ce, total_mse = 0.0, 0.0, 0.0\n",
        "    correct, correct_slot, total = 0, 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for batch in pbar:\n",
        "        img, v1, v2, label = batch\n",
        "\n",
        "        img = img.to(device, dtype=torch.float32)\n",
        "        v1 = v1.to(device, dtype=torch.float32)\n",
        "        v2 = v2.to(device, dtype=torch.float32)\n",
        "        label = label.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        # Forward with labels for MSE target\n",
        "        out = router(img, v1, v2, labels=label)\n",
        "\n",
        "        logits = out['logits']\n",
        "        slot_logits = out['slot_logits']\n",
        "        vision_embed = out['vision_embed']\n",
        "        mse_target = out['mse_target']\n",
        "\n",
        "        # NaN detection\n",
        "        if torch.isnan(logits).any() or torch.isnan(slot_logits).any():\n",
        "            print(f\"\\n⚠️ NaN detected!\")\n",
        "            continue\n",
        "\n",
        "        # Direct CE (main driver)\n",
        "        ce = F.cross_entropy(logits, label)\n",
        "\n",
        "        # Slot-based CE (alignment driver)\n",
        "        slot_ce = F.cross_entropy(slot_logits, label)\n",
        "\n",
        "        # MSE alignment (auxiliary)\n",
        "        mse = F.mse_loss(vision_embed, mse_target)\n",
        "\n",
        "        # Combined loss\n",
        "        loss = ce + slot_weight * slot_ce + mse_weight * mse\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(router.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_ce += ce.item()\n",
        "        total_slot_ce += slot_ce.item()\n",
        "        total_mse += mse.item()\n",
        "        correct += (logits.argmax(dim=-1) == label).sum().item()\n",
        "        correct_slot += (slot_logits.argmax(dim=-1) == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'acc': f'{100*correct/total:.1f}%',\n",
        "            'slot': f'{100*correct_slot/total:.1f}%',\n",
        "            'ce': f'{ce.item():.3f}',\n",
        "            'mse': f'{mse.item():.4f}',\n",
        "        })\n",
        "\n",
        "    n = len(loader)\n",
        "    return total_loss/n, total_ce/n, total_slot_ce/n, total_mse/n, correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    router: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: torch.device,\n",
        "    is_compiled: bool = False,\n",
        "    use_slot_logits: bool = False,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Evaluate accuracy.\n",
        "\n",
        "    Returns:\n",
        "        (direct_acc, slot_acc) - accuracy from both heads\n",
        "    \"\"\"\n",
        "    router.eval()\n",
        "    correct, correct_slot, total = 0, 0, 0\n",
        "\n",
        "    for batch in loader:\n",
        "        img, v1, v2, label = batch\n",
        "        img = img.to(device, dtype=torch.float32)\n",
        "        v1 = v1.to(device, dtype=torch.float32)\n",
        "        v2 = v2.to(device, dtype=torch.float32)\n",
        "        label = label.to(device)\n",
        "\n",
        "        out = router(img, v1, v2)\n",
        "        logits = out['logits']\n",
        "        slot_logits = out['slot_logits']\n",
        "\n",
        "        correct += (logits.argmax(dim=-1) == label).sum().item()\n",
        "        correct_slot += (slot_logits.argmax(dim=-1) == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "    return correct / total, correct_slot / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Multi-Modal Router (Slot-Based Text Matching)\")\n",
        "    print(\"Dual Tower Stack: 12 Trans + 12 Conv towers\")\n",
        "    print(\"Vision (DINOv3-ViT-B/16 + DINOv3-ConvNeXt-Small) → Text Slots (Qwen)\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 100\n",
        "    DIM = 512\n",
        "    NUM_TEXT_SLOTS = 8  # Prototypes per class\n",
        "    MSE_WEIGHT = 0.1    # Vision-text alignment\n",
        "    SLOT_WEIGHT = 0.3   # Slot-based CE weight\n",
        "    LR = 1e-3\n",
        "    LR_MIN = 1e-6       # Cosine annealing min\n",
        "    WARMUP_EPOCHS = 5   # Linear warmup\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Models - using smaller DINOv3 variants for per-epoch runtime\n",
        "    VISION_ENCODER_1 = 'dinov3_vitb16'    # 768d (ViT-B/16)\n",
        "    VISION_ENCODER_2 = 'dinov3_small'     # 768d (ConvNeXt-Small)\n",
        "    TEXT_ENCODER = 'qwen2.5_1.5b_instruct'  # 1536d\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision: {VISION_ENCODER_1}, {VISION_ENCODER_2}\")\n",
        "    print(f\"Text: {TEXT_ENCODER} ({NUM_TEXT_SLOTS} slots/class)\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH}\")\n",
        "    print(f\"LR: {LR} (cosine → {LR_MIN}, warmup={WARMUP_EPOCHS})\")\n",
        "\n",
        "    # Setup TF32\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Data transforms\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_ds = datasets.CIFAR100('./data', train=True, download=True, transform=train_transform)\n",
        "    test_ds = datasets.CIFAR100('./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    # === Vision Caching (using MultiVisionEncode) ===\n",
        "    train_latents1, vision_dim1 = build_vision_cache(VISION_ENCODER_1, 'train', device=str(DEVICE))\n",
        "    test_latents1, _ = build_vision_cache(VISION_ENCODER_1, 'test', device=str(DEVICE))\n",
        "\n",
        "    train_latents2, vision_dim2 = build_vision_cache(VISION_ENCODER_2, 'train', device=str(DEVICE))\n",
        "    test_latents2, _ = build_vision_cache(VISION_ENCODER_2, 'test', device=str(DEVICE))\n",
        "\n",
        "    # Ensure fp32\n",
        "    train_latents1 = train_latents1.float()\n",
        "    test_latents1 = test_latents1.float()\n",
        "    train_latents2 = train_latents2.float()\n",
        "    test_latents2 = test_latents2.float()\n",
        "\n",
        "    # === Text Caching ===\n",
        "    PROMPT_STYLE = 'repeated'\n",
        "    text_cacher = QwenTextCacher(TEXT_ENCODER, device=str(DEVICE), max_length=512)\n",
        "    text_latents = text_cacher.build_cache(prompt_style=PROMPT_STYLE).float()\n",
        "    text_dim = text_cacher.embed_dim\n",
        "\n",
        "    # Show sample prompts\n",
        "    sample_prompts = generate_class_prompts(style=PROMPT_STYLE, max_length=512)\n",
        "    print(f\"Text prompt style: {PROMPT_STYLE}\")\n",
        "    print(f\"Sample prompt (apple): '{sample_prompts[0][:80]}...' ({len(sample_prompts[0])} chars)\")\n",
        "\n",
        "    print(f\"Vision dims: {vision_dim1} + {vision_dim2}\")\n",
        "    print(f\"Text dim: {text_dim} → {NUM_TEXT_SLOTS} slots × {DIM}d = {NUM_TEXT_SLOTS * DIM}d effective\")\n",
        "\n",
        "    # Cached datasets\n",
        "    train_cached = CachedDataset(train_ds, train_latents1, train_latents2)\n",
        "    test_cached = CachedDataset(test_ds, test_latents1, test_latents2)\n",
        "\n",
        "    train_loader = DataLoader(train_cached, batch_size=BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_cached, batch_size=BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Build router with text slots\n",
        "    router = MultiModalRouter(\n",
        "        dim=DIM,\n",
        "        vision_dim1=vision_dim1,\n",
        "        vision_dim2=vision_dim2,\n",
        "        text_dim=text_dim,\n",
        "        text_latents=text_latents,\n",
        "        trans_depth=2,\n",
        "        conv_depth=2,\n",
        "        num_heads=8,\n",
        "        num_text_slots=NUM_TEXT_SLOTS,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    # Count params\n",
        "    total_params = sum(p.numel() for p in router.parameters() if p.requires_grad)\n",
        "    print(f\"\\nParams: {total_params:,}\")\n",
        "\n",
        "    # Print tower info\n",
        "    print(f\"Trans towers: {list(router['trans_collective'].towers.keys())}\")\n",
        "    print(f\"Conv towers: {list(router['conv_collective'].towers.keys())}\")\n",
        "\n",
        "    # Compile\n",
        "    is_compiled = False\n",
        "    if COMPILE and hasattr(torch, 'compile'):\n",
        "        print(f\"\\nCompiling with mode='default'...\")\n",
        "        router = torch.compile(router, mode='default')\n",
        "        is_compiled = True\n",
        "        print(\"Compilation done\")\n",
        "\n",
        "    # Optimizer\n",
        "    opt = torch.optim.AdamW(router.parameters(), lr=LR, weight_decay=0.05, betas=(0.9, 0.98))\n",
        "\n",
        "    # Cosine annealing with warmup\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < WARMUP_EPOCHS:\n",
        "            # Linear warmup\n",
        "            return (epoch + 1) / WARMUP_EPOCHS\n",
        "        else:\n",
        "            # Cosine annealing\n",
        "            progress = (epoch - WARMUP_EPOCHS) / (EPOCHS - WARMUP_EPOCHS)\n",
        "            return LR_MIN/LR + (1 - LR_MIN/LR) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Optimizer: AdamW(lr={LR}, wd=0.05)\")\n",
        "    print(f\"Scheduler: CosineAnnealing(warmup={WARMUP_EPOCHS}, min_lr={LR_MIN})\")\n",
        "    print(f\"Loss: CE + {SLOT_WEIGHT}×SlotCE + {MSE_WEIGHT}×MSE\")\n",
        "    print(f\"Scales: vision=1.0, trans=1.0, conv=1.0\")\n",
        "    print(f\"Compiled: {is_compiled}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    # Training\n",
        "    best_acc = 0.0\n",
        "    best_slot = 0.0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        start = time.time()\n",
        "\n",
        "        current_lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        loss, ce, slot_ce, mse, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE,\n",
        "            is_compiled=is_compiled,\n",
        "            mse_weight=MSE_WEIGHT,\n",
        "            slot_weight=SLOT_WEIGHT,\n",
        "        )\n",
        "\n",
        "        # Step scheduler after epoch\n",
        "        scheduler.step()\n",
        "\n",
        "        test_acc, test_slot_acc = evaluate(router, test_loader, DEVICE, is_compiled=is_compiled)\n",
        "\n",
        "        elapsed = time.time() - start\n",
        "        is_best = test_acc > best_acc\n",
        "        is_best_slot = test_slot_acc > best_slot\n",
        "        if is_best:\n",
        "            best_acc = test_acc\n",
        "        if is_best_slot:\n",
        "            best_slot = test_slot_acc\n",
        "\n",
        "        # Get learned temperature\n",
        "        if hasattr(router, '_orig_mod'):\n",
        "            temp = torch.exp(router._orig_mod.log_temperature).item()\n",
        "        else:\n",
        "            temp = torch.exp(router.log_temperature).item()\n",
        "\n",
        "        mark = '*' if is_best else ''\n",
        "        slot_mark = '^' if is_best_slot else ''\n",
        "\n",
        "        print(f\"E{epoch:02d} | lr={current_lr:.6f} | τ={temp:.3f} | \"\n",
        "              f\"loss={loss:.3f} ce={ce:.3f} slot={slot_ce:.3f} mse={mse:.4f} | \"\n",
        "              f\"train={100*train_acc:.1f}% | test={100*test_acc:.1f}%{mark} slot={100*test_slot_acc:.1f}%{slot_mark} | {elapsed:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best Direct: {100*best_acc:.2f}%\")\n",
        "    print(f\"Best Slot:   {100*best_slot:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "54b93ce6dea141199ab7d9c71cc17bd3",
            "a0b14c9237e145f194d3c7d15a01eb84",
            "ba6e9c92994244479e809415efa56900",
            "504ab435d23048d99660f382e3502a48",
            "d8c012e0c7014b3591021e43bf66e45c",
            "fa5d30cc0cf64deb8eb77ab6d73bda76",
            "06a2432f88c448f88fb86375c4610580",
            "add5e130ff04473491695b38239ae6e1",
            "3afc3db163024cf5a3e9942caf2b30d6",
            "e086a5f71bce469a94313c3e381a3600",
            "c4589dddcf1c442d921882f3c30f692b",
            "6276e61f080c49cfa19510549e4c0b62",
            "6af2639416314bfc95a94d2efb1cedf5",
            "2d3e445f77814a9799fd7f844418516d",
            "72e5d06d8aa144ac9e9b853299c4abb8",
            "b3a9de287ff8424e8345bba52e972c59",
            "3b018f1187a14e36a6b24a3702be25c4",
            "7895574cf16d43c5ae8c41f27560d818",
            "60ae15ff42714319b1343ef1c1761e8d",
            "0bf29967041f44c8b4c58b392b9cd069",
            "ea3ac6ceb9d2414692055422541c381e",
            "d6873a0816854ea4a1122615b3f43119",
            "2df6f47005ea429f93e6432215a3c3bf",
            "48acde64835a495f8aecb1ad3f0db273",
            "c1f3166c160b40c190f5f9b55fe1a9f2",
            "4d2bc4e79dc64f20aeba2e1b6a92bdf3",
            "f6d263ce88124fb5af54fe317f3912c6",
            "09be89632479470bb0b6a70f657fae62",
            "ebe661bb07ee47ce8b8cf8522ac469c4",
            "e0fb0d23de434088848a7f8f0a28c311",
            "def4f0babf4c4855be9e77ffecba561b",
            "e23c99903b2a44be8f1aaf3114fa3513",
            "996f8520f55b4ef6ba8327eed4ae36da",
            "3be73c8da38b4e03ab4949459daff05e",
            "415aba6e6cfc4a649c3f703bbfb1b91c",
            "7767048b604a4197bf1a7ba58e5eee0b",
            "6834971356284080a44804d98189ee02",
            "83ae42bc76f64956bff5d52cfaa7d1ab",
            "c546441a5613405abf88f3c513a4a604",
            "b8b03564053743a897930e7a76a0d341",
            "86eaa0c834284e7b8a8137325ea557d3",
            "90085490af674433a68a2322b560d0c4",
            "6e1bfb0fc5c04d95aca84ef23306603b",
            "671ec51b1b874b5392ee554d6a05f66d",
            "d075d671557045fd81466639b5c69e62",
            "cc77902e874948df90f7e52741ba8395",
            "73371ab01fb5496abb5f4943e7767f4b",
            "9d681e78661e437687f9773c2aa46f81",
            "56152b5411934b4aa4fdfafea5595fc9",
            "1d1c4242ebb141c0b6633502de402d46",
            "249bb040cf474ebabfe8661bfc6c460c",
            "4b1a4e6673214bf5bfa58a66496ca9ed",
            "97041718eeb04037b85226691afed906",
            "a0045524095e4229bbf820413ae88fcf",
            "0bada257179c47c9abec90a9e3b476c9",
            "1b3de4823bf14d4f809a0edbf590d500",
            "0a0b9132499a4b8186c83b49b20c73f1",
            "30048f6492504356b4996d41620419f0",
            "71ebffa1055948788bccec051d78b2b9",
            "095ec648a0c14c8d9664e69324dfbe99",
            "85ed96f169c94bdabb6c3c43d3798db0",
            "85d68d33994640509982cc6779e30bb6",
            "865dd78ad416436aa62567b90f5e9d98",
            "070aa84cb68c439382e888696b7c2daa",
            "9c61b2b7899c4eebad7684899abdedcd",
            "67daf02d32954e829b51b4ea15a11e55"
          ]
        },
        "id": "TwtwzTufBjMv",
        "outputId": "5a23cd1f-3aa7-4cb8-8d6f-586ee097de72"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Multi-Modal Router (Slot-Based Text Matching)\n",
            "Dual Tower Stack: 12 Trans + 12 Conv towers\n",
            "Vision (DINOv3-ViT-B/16 + DINOv3-ConvNeXt-Small) → Text Slots (Qwen)\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision: dinov3_vitb16, dinov3_small\n",
            "Text: qwen2.5_1.5b_instruct (8 slots/class)\n",
            "Compile: True\n",
            "Epochs: 100, Batch: 128\n",
            "LR: 0.001 (cosine → 1e-06, warmup=5)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building cache: encoder_cache/cifar100/dinov3_vitb16_train.pt\n",
            "\n",
            "============================================================\n",
            "Initializing MultiVisionEncode\n",
            "============================================================\n",
            "Dataset: cifar100\n",
            "Encoders: ['dinov3_vitb16']\n",
            "  Loading dinov3_vitb16...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/744 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54b93ce6dea141199ab7d9c71cc17bd3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/343M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6276e61f080c49cfa19510549e4c0b62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2df6f47005ea429f93e6432215a3c3bf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ dinov3_vitb16: 85,660,416 params, dim=768\n",
            "\n",
            "Dimensions: {'dinov3_vitb16': 768}\n",
            "Combined: 768\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding train: 100%|██████████| 782/782 [00:57<00:00, 13.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: encoder_cache/cifar100/dinov3_vitb16_train.pt shape=torch.Size([50000, 768])\n",
            "Building cache: encoder_cache/cifar100/dinov3_vitb16_test.pt\n",
            "\n",
            "============================================================\n",
            "Initializing MultiVisionEncode\n",
            "============================================================\n",
            "Dataset: cifar100\n",
            "Encoders: ['dinov3_vitb16']\n",
            "  Loading dinov3_vitb16...\n",
            "  ✓ dinov3_vitb16: 85,660,416 params, dim=768\n",
            "\n",
            "Dimensions: {'dinov3_vitb16': 768}\n",
            "Combined: 768\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding test: 100%|██████████| 157/157 [00:11<00:00, 13.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: encoder_cache/cifar100/dinov3_vitb16_test.pt shape=torch.Size([10000, 768])\n",
            "Building cache: encoder_cache/cifar100/dinov3_small_train.pt\n",
            "\n",
            "============================================================\n",
            "Initializing MultiVisionEncode\n",
            "============================================================\n",
            "Dataset: cifar100\n",
            "Encoders: ['dinov3_small']\n",
            "  Loading dinov3_small...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/447 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3be73c8da38b4e03ab4949459daff05e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/198M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d075d671557045fd81466639b5c69e62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/585 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b3de4823bf14d4f809a0edbf590d500"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  ✓ dinov3_small: 49,454,688 params, dim=768\n",
            "\n",
            "Dimensions: {'dinov3_small': 768}\n",
            "Combined: 768\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding train: 100%|██████████| 782/782 [00:53<00:00, 14.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: encoder_cache/cifar100/dinov3_small_train.pt shape=torch.Size([50000, 768])\n",
            "Building cache: encoder_cache/cifar100/dinov3_small_test.pt\n",
            "\n",
            "============================================================\n",
            "Initializing MultiVisionEncode\n",
            "============================================================\n",
            "Dataset: cifar100\n",
            "Encoders: ['dinov3_small']\n",
            "  Loading dinov3_small...\n",
            "  ✓ dinov3_small: 49,454,688 params, dim=768\n",
            "\n",
            "Dimensions: {'dinov3_small': 768}\n",
            "Combined: 768\n",
            "============================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding test: 100%|██████████| 157/157 [00:09<00:00, 16.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: encoder_cache/cifar100/dinov3_small_test.pt shape=torch.Size([10000, 768])\n",
            "QwenTextCacher: qwen2.5_1.5b_instruct (1536d, max_length=512)\n",
            "Loading cache: encoder_cache/cifar100/qwen2.5_1.5b_instruct_repeated_class_embeddings.pt\n",
            "Text prompt style: repeated\n",
            "Sample prompt (apple): 'apple apple apple apple apple apple apple apple apple apple apple apple apple ap...' (1535 chars)\n",
            "Vision dims: 768 + 768\n",
            "Text dim: 1536 → 8 slots × 512d = 4096d effective\n",
            "\n",
            "Params: 100,270,165\n",
            "Trans towers: ['helix_pos', 'helix_neg', 'cantor_pos', 'cantor_neg', 'simplex_pos', 'simplex_neg', 'golden_pos', 'golden_neg', 'fibonacci_pos', 'fibonacci_neg', 'beatrix_pos', 'beatrix_neg']\n",
            "Conv towers: ['wide_resnet_pos', 'wide_resnet_neg', 'frequency_pos', 'frequency_neg', 'bottleneck_pos', 'bottleneck_neg', 'squeeze_excite_pos', 'squeeze_excite_neg', 'dilated_pos', 'dilated_neg', 'spatial_attention_pos', 'spatial_attention_neg']\n",
            "\n",
            "Compiling with mode='default'...\n",
            "Compilation done\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Optimizer: AdamW(lr=0.001, wd=0.05)\n",
            "Scheduler: CosineAnnealing(warmup=5, min_lr=1e-06)\n",
            "Loss: CE + 0.3×SlotCE + 0.1×MSE\n",
            "Scales: vision=1.0, trans=1.0, conv=1.0\n",
            "Compiled: True\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 390/391 [01:45<00:00,  5.37it/s, acc=12.8%, slot=9.7%, ce=3.215, mse=0.0016]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.000200 | τ=0.462 | loss=4.964 ce=3.742 slot=4.075 mse=0.0021 | train=12.8% | test=20.1%* slot=14.0%^ | 315.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.000400 | τ=0.399 | loss=4.281 ce=3.156 slot=3.750 mse=0.0018 | train=22.2% | test=25.0%* slot=17.7%^ | 77.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.000600 | τ=0.321 | loss=4.033 ce=2.963 slot=3.563 mse=0.0017 | train=25.5% | test=27.6%* slot=21.1%^ | 77.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.000800 | τ=0.242 | loss=3.875 ce=2.869 slot=3.352 mse=0.0018 | train=27.3% | test=28.4%* slot=23.5%^ | 77.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.001000 | τ=0.175 | loss=3.771 ce=2.829 slot=3.140 mse=0.0018 | train=28.2% | test=28.6%* slot=25.0%^ | 79.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.001000 | τ=0.136 | loss=3.599 ce=2.719 slot=2.934 mse=0.0018 | train=30.5% | test=29.8%* slot=26.5%^ | 79.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.001000 | τ=0.118 | loss=3.462 ce=2.618 slot=2.812 mse=0.0018 | train=32.4% | test=30.8%* slot=28.2%^ | 78.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.000999 | τ=0.109 | loss=3.358 ce=2.542 slot=2.718 mse=0.0018 | train=34.0% | test=32.1%* slot=29.8%^ | 79.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.000998 | τ=0.104 | loss=3.255 ce=2.463 slot=2.638 mse=0.0018 | train=35.6% | test=33.9%* slot=31.4%^ | 80.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.000996 | τ=0.099 | loss=3.176 ce=2.402 slot=2.579 mse=0.0018 | train=37.0% | test=33.9%* slot=32.3%^ | 80.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.000993 | τ=0.095 | loss=3.079 ce=2.328 slot=2.503 mse=0.0019 | train=38.4% | test=36.4%* slot=33.7%^ | 78.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.000990 | τ=0.091 | loss=2.997 ce=2.265 slot=2.441 mse=0.0019 | train=39.9% | test=35.8% slot=34.2%^ | 80.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.000987 | τ=0.087 | loss=2.915 ce=2.201 slot=2.378 mse=0.0019 | train=41.2% | test=36.0% slot=34.4%^ | 79.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.000983 | τ=0.085 | loss=2.851 ce=2.151 slot=2.332 mse=0.0019 | train=42.0% | test=37.8%* slot=35.3%^ | 78.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.000978 | τ=0.082 | loss=2.764 ce=2.084 slot=2.263 mse=0.0020 | train=43.6% | test=38.0%* slot=35.9%^ | 79.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.000973 | τ=0.079 | loss=2.699 ce=2.035 slot=2.212 mse=0.0020 | train=44.9% | test=37.3% slot=35.6% | 81.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.000967 | τ=0.077 | loss=2.634 ce=1.986 slot=2.162 mse=0.0020 | train=46.0% | test=38.5%* slot=37.1%^ | 78.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.000961 | τ=0.074 | loss=2.563 ce=1.930 slot=2.111 mse=0.0020 | train=47.2% | test=38.4% slot=37.1% | 80.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.000955 | τ=0.072 | loss=2.494 ce=1.877 slot=2.056 mse=0.0020 | train=48.3% | test=38.8%* slot=37.7%^ | 81.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.000947 | τ=0.069 | loss=2.424 ce=1.823 slot=2.003 mse=0.0020 | train=49.6% | test=39.0%* slot=37.8%^ | 78.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | lr=0.000940 | τ=0.068 | loss=2.379 ce=1.789 slot=1.965 mse=0.0021 | train=50.3% | test=39.3%* slot=37.7% | 81.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | lr=0.000932 | τ=0.065 | loss=2.308 ce=1.734 slot=1.911 mse=0.0021 | train=51.6% | test=39.7%* slot=38.4%^ | 80.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | lr=0.000923 | τ=0.063 | loss=2.246 ce=1.687 slot=1.862 mse=0.0021 | train=52.9% | test=39.5% slot=38.2% | 80.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2066344602.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2066344602.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0mcurrent_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m         loss, ce, slot_ce, mse, train_acc = train_epoch(\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0mis_compiled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2066344602.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(router, loader, opt, device, is_compiled, mse_weight, slot_weight, max_grad_norm)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;31m# Forward with labels for MSE target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             )\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m     def __reduce__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36mcompile_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 832\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    833\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mUnsupported\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2066344602.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, vision_latent1, vision_latent2, labels)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;31m# === Transformer Towers (vision patches) ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0mtrans_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans_collective'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0mtrans_fused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfused\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrans_scale\u001b[0m  \u001b[0;31m# [B, dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/prefab/geometric_tower_builder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;31m# Use WideRouter's optimized execution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;31m# Returns Dict[tower_name, opinion_tensor]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m         \u001b[0mopinions_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwide_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;31m# Build TowerOpinion objects with cached features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/geofractal/router/wide_router.py\u001b[0m in \u001b[0;36mwide_forward\u001b[0;34m(self, x, tower_names, mask)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;31m# =========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m     def wide_forward(\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_callback_from_stance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m                     \u001b[0mset_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/aot_autograd.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*runtime_args)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams_buffers_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mfull_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;31m# Just for convenience\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mruntime_wrapper\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    337\u001b[0m             ):\n\u001b[1;32m    338\u001b[0m                 \u001b[0mrecord_runtime_wrapper_prologue_exit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 339\u001b[0;31m                 all_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m    340\u001b[0m                     \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable_amp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable_amp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteal_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mg\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_boxed_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boxed_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, *deduped_flat_tensor_args)\u001b[0m\n\u001b[1;32m   2116\u001b[0m                 \u001b[0;31m# - Note that donated buffer logic requires (*saved_tensors, *saved_symints) showing up last\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2117\u001b[0m                 \u001b[0;31m#   in the fw output order.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2118\u001b[0;31m                 fw_outs = call_func_at_runtime_with_args(\n\u001b[0m\u001b[1;32m   2119\u001b[0m                     \u001b[0mCompiledFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_fw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2120\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/utils.py\u001b[0m in \u001b[0;36mcall_func_at_runtime_with_args\u001b[0;34m(f, args, steal_args, disable_amp)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_boxed_call\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;31m# TODO: Please remove soon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_functorch/_aot_autograd/runtime_wrappers.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(runtime_args)\u001b[0m\n\u001b[1;32m    524\u001b[0m                 )\n\u001b[1;32m    525\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcompiled_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruntime_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/output_code.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    611\u001b[0m                 \u001b[0;34mf\"## Call CompiledFxGraph {self._fx_graph_cache_key} ##\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             ):\n\u001b[0;32m--> 613\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m             \u001b[0mget_runtime_metrics_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_inductor/utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(new_inputs)\u001b[0m\n\u001b[1;32m   2960\u001b[0m             \u001b[0mnew_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_to_check\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmutated_input_idxs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2961\u001b[0m         )\n\u001b[0;32m-> 2962\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m         \u001b[0;31m# If a mutated tensor was cloned to be aligned, we need to reflect back the mutation to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/torchinductor_root/wx/cwx2ncfa456kck3pwtwz3bbrolnf3enhesf556kvzffn6kzera47.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   2014\u001b[0m             \u001b[0mbuf218\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_strided_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0;31m# Topologically Sorted Source Nodes: [layer_norm, layer_norm_15, linear_39, linear_41], Original ATen: [aten.native_layer_norm, aten.view, aten.t, aten.mm]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m             \u001b[0mextern_kernels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf215\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreinterpret_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprimals_96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuf218\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m             \u001b[0mbuf219\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty_strided_cuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2018\u001b[0m             \u001b[0;31m# Topologically Sorted Source Nodes: [linear_39, view_24, q_6, matmul_12], Original ATen: [aten._unsafe_view, aten.view, aten.transpose, aten.clone]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tri tower experiment"
      ],
      "metadata": {
        "id": "THPvFXDekY_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Tri-Collective Router\n",
        "================================\n",
        "\n",
        "Architecture:\n",
        "    Vision Specialist ──── vision_collective(patches) ──→ v_opinion ──┐\n",
        "                                                                       │\n",
        "    Shared Bridge ──┬── shared_collective(patches) ───→ v_shared ─────┼──→ CooperativeFusion ──→ classifier\n",
        "                    │                                                  │\n",
        "                    └── shared_collective(text_seq) ──→ t_shared ─────┤\n",
        "                                                                       │\n",
        "    Text Specialist ───── text_collective(text_seq) ──→ t_opinion ────┘\n",
        "\n",
        "Three collectives, same geometric configs (cantor, beatrix, helix, simplex ±).\n",
        "Cooperative fusion: sigmoid weights (all contribute) not softmax (competition).\n",
        "Shared collective forces cross-modal alignment through identical geometry.\n",
        "\n",
        "Text Encoding: Two-shot generation for semantic signal diversity.\n",
        "\n",
        "Loss: CrossEntropy (classification to prove concept)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from pathlib import Path\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableCollective,\n",
        "    build_tower_collective, preset_pos_neg_pairs,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR-100 CLASS INFO\n",
        "# =============================================================================\n",
        "\n",
        "CIFAR100_CLASSES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEXT ENCODER CACHING (Two-Shot Generation)\n",
        "# =============================================================================\n",
        "\n",
        "class QwenTextCacher:\n",
        "    \"\"\"\n",
        "    Cache Qwen hidden states for class prompts.\n",
        "\n",
        "    Modes:\n",
        "    - 'repeated': Old method, just repeat class name (poor semantic signal)\n",
        "    - 'last_token': Chat template, take last token position (better)\n",
        "    - 'twoshot': Generate description, encode the output (BEST)\n",
        "    \"\"\"\n",
        "\n",
        "    QWEN_MODELS = {\n",
        "        'qwen2.5_0.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-0.5B-Instruct', 'dim': 896},\n",
        "        'qwen2.5_1.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-1.5B-Instruct', 'dim': 1536},\n",
        "        'qwen2.5_3b_instruct': {'hf_path': 'Qwen/Qwen2.5-3B-Instruct', 'dim': 2048},\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str = 'qwen2.5_1.5b_instruct',\n",
        "        cache_dir: str = './encoder_cache',\n",
        "        device: str = 'cuda',\n",
        "        max_length: int = 512,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        config = self.QWEN_MODELS.get(model_name)\n",
        "        if config is None:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "\n",
        "        self.hf_path = config['hf_path']\n",
        "        self.dim = config['dim']\n",
        "        print(f\"QwenTextCacher: {model_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, style: str = 'twoshot') -> Path:\n",
        "        return self.cache_dir / 'cifar100' / f\"{self.model_name}_{style}_class_embeddings.pt\"\n",
        "\n",
        "    def _build_twoshot_prompt(self, tokenizer, class_name: str) -> str:\n",
        "        \"\"\"Build two-shot prompt with examples.\"\"\"\n",
        "        cls_name = class_name.replace('_', ' ')\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You describe objects in exactly one sentence. Be specific about visual features.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Describe: car\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"A four-wheeled motor vehicle with windows, doors, headlights, and a metal body used for transportation on roads.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": \"Describe: sunflower\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"A tall plant with a large circular flower head containing yellow petals surrounding a brown seed-filled center.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Describe: {cls_name}\"\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, prompt_style: str = 'twoshot', force_rebuild: bool = False) -> Tensor:\n",
        "        \"\"\"\n",
        "        Build and cache embeddings for all 100 classes.\n",
        "\n",
        "        Args:\n",
        "            prompt_style: 'twoshot' (best), 'last_token', or 'repeated' (legacy)\n",
        "            force_rebuild: If True, rebuild even if cache exists\n",
        "        \"\"\"\n",
        "        cache_path = self._cache_path(prompt_style)\n",
        "\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading text cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building text cache: {cache_path} (style={prompt_style})\")\n",
        "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.hf_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.hf_path,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=self.device,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "        model.eval()\n",
        "\n",
        "        embeddings = []\n",
        "        descriptions = {}  # Store for inspection\n",
        "\n",
        "        for cls in tqdm(CIFAR100_CLASSES, desc=f\"Encoding ({prompt_style})\"):\n",
        "            cls_name = cls.replace('_', ' ')\n",
        "\n",
        "            if prompt_style == 'twoshot':\n",
        "                # === TWO-SHOT GENERATION ===\n",
        "                # Step 1: Generate description\n",
        "                prompt = self._build_twoshot_prompt(tokenizer, cls)\n",
        "                inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=50,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=tokenizer.eos_token_id,\n",
        "                )\n",
        "\n",
        "                # Decode generated text\n",
        "                generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
        "                description = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "                descriptions[cls] = description\n",
        "\n",
        "                # Step 2: Encode the generated description\n",
        "                desc_inputs = tokenizer(description, return_tensors=\"pt\").to(self.device)\n",
        "                desc_outputs = model(\n",
        "                    **desc_inputs,\n",
        "                    output_hidden_states=True,\n",
        "                    return_dict=True,\n",
        "                )\n",
        "\n",
        "                # Last token of generated description\n",
        "                last_hidden = desc_outputs.hidden_states[-1]\n",
        "                emb = last_hidden[0, -1, :].cpu().float()\n",
        "\n",
        "            elif prompt_style == 'last_token':\n",
        "                # === LAST TOKEN (generation position) ===\n",
        "                messages = [\n",
        "                    {\"role\": \"system\", \"content\": \"You are an expert at visual recognition.\"},\n",
        "                    {\"role\": \"user\", \"content\": f\"Describe the visual appearance of a {cls_name}.\"}\n",
        "                ]\n",
        "\n",
        "                text = tokenizer.apply_chat_template(\n",
        "                    messages,\n",
        "                    tokenize=False,\n",
        "                    add_generation_prompt=True\n",
        "                )\n",
        "\n",
        "                inputs = tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
        "                outputs = model(\n",
        "                    **inputs,\n",
        "                    output_hidden_states=True,\n",
        "                    return_dict=True,\n",
        "                )\n",
        "\n",
        "                last_hidden = outputs.hidden_states[-1]\n",
        "                emb = last_hidden[0, -1, :].cpu().float()\n",
        "\n",
        "            else:  # 'repeated' (legacy)\n",
        "                # === REPEATED CLASS NAME ===\n",
        "                base = f\"{cls_name} \"\n",
        "                target_chars = self.max_length * 3\n",
        "                repeats = target_chars // len(base)\n",
        "                prompt = (base * repeats).strip()\n",
        "\n",
        "                inputs = tokenizer(\n",
        "                    prompt,\n",
        "                    return_tensors=\"pt\",\n",
        "                    max_length=self.max_length,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                ).to(self.device)\n",
        "\n",
        "                outputs = model(\n",
        "                    **inputs,\n",
        "                    output_hidden_states=True,\n",
        "                    return_dict=True,\n",
        "                )\n",
        "\n",
        "                last_hidden = outputs.hidden_states[-1]\n",
        "                mask = inputs['attention_mask'].unsqueeze(-1)\n",
        "                emb = ((last_hidden * mask).sum(dim=1) / mask.sum(dim=1)).squeeze(0).cpu().float()\n",
        "\n",
        "            embeddings.append(emb)\n",
        "\n",
        "        embeddings = torch.stack(embeddings, dim=0)  # [100, dim]\n",
        "\n",
        "        # Cleanup\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Save\n",
        "        torch.save(embeddings, cache_path)\n",
        "        print(f\"Saved: {cache_path} ({embeddings.shape})\")\n",
        "\n",
        "        # Save descriptions for inspection (twoshot only)\n",
        "        if prompt_style == 'twoshot' and descriptions:\n",
        "            desc_path = cache_path.with_suffix('.descriptions.txt')\n",
        "            with open(desc_path, 'w') as f:\n",
        "                for cls, desc in descriptions.items():\n",
        "                    f.write(f\"{cls}: {desc}\\n\")\n",
        "            print(f\"Saved descriptions: {desc_path}\")\n",
        "\n",
        "        # Quick stats\n",
        "        E_norm = F.normalize(embeddings, dim=-1)\n",
        "        sim = torch.mm(E_norm, E_norm.T)\n",
        "        mask = ~torch.eye(100, dtype=torch.bool)\n",
        "        off_diag = sim[mask]\n",
        "        print(f\"  Mean similarity: {off_diag.mean():.4f} ± {off_diag.std():.4f}\")\n",
        "        print(f\"  Range: [{off_diag.min():.4f}, {off_diag.max():.4f}]\")\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# VISION CACHING\n",
        "# =============================================================================\n",
        "\n",
        "from geofractal.router.components.encoder_data_component import MultiVisionEncode\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_vision_cache(\n",
        "    encoder_name: str,\n",
        "    split: str,\n",
        "    dataset_name: str = 'cifar100',\n",
        "    cache_dir: str = './encoder_cache',\n",
        "    device: str = 'cuda',\n",
        ") -> Tuple[Tensor, int]:\n",
        "    \"\"\"Build or load vision encoder cache.\"\"\"\n",
        "    cache_path = Path(cache_dir) / dataset_name / f\"{encoder_name}_{split}.pt\"\n",
        "\n",
        "    if cache_path.exists():\n",
        "        print(f\"Loading vision cache: {cache_path}\")\n",
        "        latents = torch.load(cache_path, weights_only=True)\n",
        "        return latents, latents.shape[-1]\n",
        "\n",
        "    print(f\"Building vision cache: {cache_path}\")\n",
        "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    encoder = MultiVisionEncode(\n",
        "        encoders=[encoder_name],\n",
        "        dataset_name=dataset_name,\n",
        "        device=device,\n",
        "        cache_enabled=False,\n",
        "        concatenate=True,\n",
        "        pool_output=True,\n",
        "    )\n",
        "\n",
        "    dim = encoder.combined_dim\n",
        "\n",
        "    raw_dataset = datasets.CIFAR100(\n",
        "        './data', train=(split == 'train'), download=True,\n",
        "        transform=transforms.ToTensor(),\n",
        "    )\n",
        "\n",
        "    latents = []\n",
        "    batch_size = 64\n",
        "\n",
        "    for i in tqdm(range(0, len(raw_dataset), batch_size), desc=f\"Encoding {split}\"):\n",
        "        batch_images = []\n",
        "        for j in range(i, min(i + batch_size, len(raw_dataset))):\n",
        "            img, _ = raw_dataset[j]\n",
        "            batch_images.append(img)\n",
        "\n",
        "        batch_tensor = torch.stack(batch_images)\n",
        "        z = encoder.encode(batch_tensor)\n",
        "        latents.append(z.cpu().float())\n",
        "\n",
        "    latents = torch.cat(latents, dim=0)\n",
        "    torch.save(latents, cache_path)\n",
        "\n",
        "    encoder.unload_all()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return latents, dim\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class CachedDataset(Dataset):\n",
        "    \"\"\"Dataset with pre-computed vision latents.\"\"\"\n",
        "\n",
        "    def __init__(self, base: Dataset, vision_latents: Tensor):\n",
        "        self.base = base\n",
        "        self.vision_latents = vision_latents\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base[idx]\n",
        "        v = self.vision_latents[idx]\n",
        "        return img, v, label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\"Patch embedding for images.\"\"\"\n",
        "\n",
        "    def __init__(self, dim: int, patch_size: int = 4, img_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, dim) * 0.02)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)  # [B, num_patches, dim]\n",
        "        x = x + self.pos_embed\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class TextSequenceExpander(nn.Module):\n",
        "    \"\"\"\n",
        "    Expand text class embeddings to pseudo-sequences for tower processing.\n",
        "\n",
        "    [num_classes, text_dim] → [num_classes, seq_len, dim]\n",
        "\n",
        "    Creates learnable \"token\" positions from each class embedding,\n",
        "    allowing geometric towers to process text with RoPE/attention.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, text_dim: int, dim: int, seq_len: int = 8, num_classes: int = 100):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Project text embedding to sequence\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(text_dim),\n",
        "            nn.Linear(text_dim, dim * seq_len),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        # Learnable position embeddings for text sequence\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, dim) * 0.02)\n",
        "\n",
        "        # Per-position refinement\n",
        "        self.refine = nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, text_latents: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_latents: [num_classes, text_dim]\n",
        "        Returns:\n",
        "            [num_classes, seq_len, dim] text sequences\n",
        "        \"\"\"\n",
        "        x = self.proj(text_latents)  # [C, dim * seq_len]\n",
        "        x = x.view(-1, self.seq_len, x.shape[-1] // self.seq_len)  # [C, seq_len, dim]\n",
        "        x = x + self.pos_embed\n",
        "        return self.refine(x)\n",
        "\n",
        "\n",
        "class CooperativeFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Cooperative fusion: all opinions contribute independently.\n",
        "\n",
        "    Unlike AdaptiveFusion (softmax → competition), uses sigmoid\n",
        "    so each opinion's weight is independent of others.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name: str, num_inputs: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.name = name\n",
        "        self.num_inputs = num_inputs\n",
        "\n",
        "        # Per-input weight predictor\n",
        "        self.weight_net = nn.Sequential(\n",
        "            nn.Linear(dim, dim // 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim // 4, 1),\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, *opinions: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            opinions: num_inputs tensors of shape [B, dim]\n",
        "        Returns:\n",
        "            [B, dim] cooperatively fused output\n",
        "        \"\"\"\n",
        "        stacked = torch.stack(opinions, dim=1)  # [B, num_inputs, dim]\n",
        "\n",
        "        # Independent weights via sigmoid (not softmax)\n",
        "        weights = torch.sigmoid(self.weight_net(stacked))  # [B, num_inputs, 1]\n",
        "\n",
        "        # Weighted sum normalized by total participation\n",
        "        weighted = stacked * weights  # [B, num_inputs, dim]\n",
        "        fused = weighted.sum(dim=1) / (weights.sum(dim=1) + 1e-8)  # [B, dim]\n",
        "\n",
        "        return self.out_proj(fused)\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRI-COLLECTIVE ROUTER\n",
        "# =============================================================================\n",
        "\n",
        "class TriCollectiveRouter(BaseRouter):\n",
        "    \"\"\"\n",
        "    Tri-Collective architecture with cooperative fusion.\n",
        "\n",
        "    Three collectives with identical geometric configs:\n",
        "    1. Vision Specialist: processes image patches only\n",
        "    2. Text Specialist: processes text sequences only\n",
        "    3. Shared Bridge: processes both modalities (same weights)\n",
        "\n",
        "    Produces 4 opinions:\n",
        "    - v_opinion: vision specialist on patches\n",
        "    - t_opinion: text specialist on text sequence\n",
        "    - v_shared: shared on patches\n",
        "    - t_shared: shared on text sequence\n",
        "\n",
        "    Cooperative fusion combines all 4, then classifies.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 100,\n",
        "        vision_expert_dim: int = 768,\n",
        "        text_dim: int = 1536,\n",
        "        text_seq_len: int = 8,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        text_latents: Tensor = None,\n",
        "    ):\n",
        "        super().__init__('tri_collective', strict=False)\n",
        "\n",
        "        if text_latents is None:\n",
        "            raise ValueError(\"text_latents required\")\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.text_seq_len = text_seq_len\n",
        "\n",
        "        # Store config\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'num_classes': num_classes,\n",
        "            'vision_expert_dim': vision_expert_dim, 'text_dim': text_dim,\n",
        "        }\n",
        "\n",
        "        # === EMBEDDINGS ===\n",
        "        self.patch_embed = PatchEmbed(dim, patch_size)\n",
        "        self.text_expander = TextSequenceExpander(text_dim, dim, text_seq_len, num_classes)\n",
        "\n",
        "        # Store raw text latents\n",
        "        self.register_buffer('text_latents_raw', text_latents.float())  # [100, text_dim]\n",
        "\n",
        "        # Vision expert projection\n",
        "        self.attach('vision_expert_proj', nn.Sequential(\n",
        "            nn.LayerNorm(vision_expert_dim),\n",
        "            nn.Linear(vision_expert_dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim, dim),\n",
        "        ))\n",
        "\n",
        "        # === TOWER CONFIGS (same for all three collectives) ===\n",
        "        tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # === VISION SPECIALIST ===\n",
        "        vision_collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name='vision_specialist',\n",
        "        )\n",
        "        self._fix_collective(vision_collective, len(tower_configs), dim, fingerprint_dim)\n",
        "        self.attach('vision_collective', vision_collective)\n",
        "\n",
        "        # === TEXT SPECIALIST ===\n",
        "        text_collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name='text_specialist',\n",
        "        )\n",
        "        self._fix_collective(text_collective, len(tower_configs), dim, fingerprint_dim)\n",
        "        self.attach('text_collective', text_collective)\n",
        "\n",
        "        # === SHARED BRIDGE (processes both modalities) ===\n",
        "        shared_collective = build_tower_collective(\n",
        "            configs=tower_configs,\n",
        "            dim=dim,\n",
        "            default_depth=tower_depth,\n",
        "            num_heads=num_heads,\n",
        "            fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive',\n",
        "            name='shared_bridge',\n",
        "        )\n",
        "        self._fix_collective(shared_collective, len(tower_configs), dim, fingerprint_dim)\n",
        "        self.attach('shared_collective', shared_collective)\n",
        "\n",
        "        # === COOPERATIVE FUSION (4 opinions → combined) ===\n",
        "        self.coop_fusion = CooperativeFusion('cooperative', num_inputs=4, dim=dim)\n",
        "\n",
        "        # === VISION EXPERT FUSION (add expert signal) ===\n",
        "        self.expert_gate = nn.Sequential(\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        # === CLASSIFICATION HEAD ===\n",
        "        self.attach('classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim),\n",
        "            nn.Linear(dim, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "    def _fix_collective(self, collective, num_towers, dim, fp_dim):\n",
        "        \"\"\"Fix fp_proj and fusion for actual tower count.\"\"\"\n",
        "        collective.components['fp_proj'] = nn.Linear(fp_dim * num_towers, fp_dim)\n",
        "        collective._num_towers = num_towers\n",
        "        collective.components['fusion'] = AdaptiveFusion(\n",
        "            f'{collective.name}_fusion', num_inputs=num_towers, in_features=dim\n",
        "        )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: Tensor,\n",
        "        vision_latents: Tensor,\n",
        "        labels: Optional[Tensor] = None,\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        \"\"\"\n",
        "        Forward pass through tri-collective architecture.\n",
        "\n",
        "        Args:\n",
        "            images: [B, 3, 32, 32] raw images\n",
        "            vision_latents: [B, vision_expert_dim] frozen expert features\n",
        "            labels: [B] optional labels (for training, selects text class)\n",
        "\n",
        "        Returns:\n",
        "            Dict with logits and intermediate outputs\n",
        "        \"\"\"\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "\n",
        "        # === Vision Expert Processing ===\n",
        "        expert_feat = self['vision_expert_proj'](vision_latents)  # [B, dim]\n",
        "\n",
        "        # === Patch Embedding ===\n",
        "        v_patches = self.patch_embed(images)  # [B, num_patches, dim]\n",
        "\n",
        "        # === Text Sequence Expansion ===\n",
        "        # Expand ALL class embeddings to sequences\n",
        "        text_latents = self.text_latents_raw.to(device)\n",
        "        all_text_seq = self.text_expander(text_latents)  # [100, text_seq_len, dim]\n",
        "\n",
        "        # === VISION SPECIALIST ===\n",
        "        v_opinion = self['vision_collective'](v_patches).fused  # [B, dim]\n",
        "\n",
        "        # === SHARED COLLECTIVE (vision path) ===\n",
        "        v_shared = self['shared_collective'](v_patches).fused  # [B, dim]\n",
        "\n",
        "        # === TEXT PROCESSING ===\n",
        "        # Process ALL 100 class text sequences through collectives\n",
        "        # Then use similarity for classification\n",
        "\n",
        "        # Text specialist on all classes\n",
        "        t_opinions = []\n",
        "        t_shared_list = []\n",
        "\n",
        "        # Batch process all 100 classes\n",
        "        t_opinion_all = self['text_collective'](all_text_seq).fused  # [100, dim]\n",
        "        t_shared_all = self['shared_collective'](all_text_seq).fused  # [100, dim]\n",
        "\n",
        "        # === COOPERATIVE FUSION (vision side) ===\n",
        "        # Combine vision opinions: v_opinion, v_shared\n",
        "        # We need text opinions too - use mean of all classes as context\n",
        "        t_opinion_ctx = t_opinion_all.mean(dim=0, keepdim=True).expand(B, -1)  # [B, dim]\n",
        "        t_shared_ctx = t_shared_all.mean(dim=0, keepdim=True).expand(B, -1)    # [B, dim]\n",
        "\n",
        "        # 4 opinions into cooperative fusion\n",
        "        fused = self.coop_fusion(v_opinion, v_shared, t_opinion_ctx, t_shared_ctx)  # [B, dim]\n",
        "\n",
        "        # === EXPERT GATING ===\n",
        "        gate = self.expert_gate(torch.cat([fused, expert_feat], dim=-1))  # [B, dim]\n",
        "        combined = fused + gate * expert_feat  # Gated residual from expert\n",
        "\n",
        "        # === CLASSIFICATION ===\n",
        "        logits = self['classifier'](combined)  # [B, num_classes]\n",
        "\n",
        "        # === SIMILARITY-BASED LOGITS (auxiliary) ===\n",
        "        # Compare vision fused to text class representations\n",
        "        combined_norm = F.normalize(combined, dim=-1)\n",
        "        text_combined = (t_opinion_all + t_shared_all) * 0.5  # [100, dim]\n",
        "        text_norm = F.normalize(text_combined, dim=-1)\n",
        "        sim_logits = torch.matmul(combined_norm, text_norm.T) * 10.0  # [B, 100]\n",
        "\n",
        "        return {\n",
        "            'logits': logits,\n",
        "            'sim_logits': sim_logits,\n",
        "            'v_opinion': v_opinion,\n",
        "            'v_shared': v_shared,\n",
        "            't_opinion_all': t_opinion_all,\n",
        "            't_shared_all': t_shared_all,\n",
        "            'fused': fused,\n",
        "            'combined': combined,\n",
        "        }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(\n",
        "    router: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    opt: torch.optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    sim_weight: float = 0.3,\n",
        "    max_grad_norm: float = 1.0,\n",
        ") -> Tuple[float, float, float, float]:\n",
        "    \"\"\"\n",
        "    Train one epoch with hybrid loss.\n",
        "\n",
        "    Loss = CE(logits) + sim_weight * CE(sim_logits)\n",
        "    \"\"\"\n",
        "    router.train()\n",
        "    total_loss, total_ce, total_sim = 0.0, 0.0, 0.0\n",
        "    correct, correct_sim, total = 0, 0, 0\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for img, v, label in pbar:\n",
        "        img = img.to(device, dtype=torch.float32)\n",
        "        v = v.to(device, dtype=torch.float32)\n",
        "        label = label.to(device)\n",
        "\n",
        "        opt.zero_grad()\n",
        "\n",
        "        out = router(img, v, labels=label)\n",
        "        logits = out['logits']\n",
        "        sim_logits = out['sim_logits']\n",
        "\n",
        "        # Hybrid loss\n",
        "        ce = F.cross_entropy(logits, label)\n",
        "        sim_ce = F.cross_entropy(sim_logits, label)\n",
        "        loss = ce + sim_weight * sim_ce\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(router.parameters(), max_grad_norm)\n",
        "        opt.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_ce += ce.item()\n",
        "        total_sim += sim_ce.item()\n",
        "        correct += (logits.argmax(dim=-1) == label).sum().item()\n",
        "        correct_sim += (sim_logits.argmax(dim=-1) == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'acc': f'{100*correct/total:.1f}%',\n",
        "            'sim': f'{100*correct_sim/total:.1f}%',\n",
        "            'ce': f'{ce.item():.3f}',\n",
        "        })\n",
        "\n",
        "    n = len(loader)\n",
        "    return total_loss/n, total_ce/n, total_sim/n, correct/total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(\n",
        "    router: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    device: torch.device,\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"Evaluate accuracy for both heads.\"\"\"\n",
        "    router.eval()\n",
        "    correct, correct_sim, total = 0, 0, 0\n",
        "\n",
        "    for img, v, label in loader:\n",
        "        img = img.to(device, dtype=torch.float32)\n",
        "        v = v.to(device, dtype=torch.float32)\n",
        "        label = label.to(device)\n",
        "\n",
        "        out = router(img, v)\n",
        "        correct += (out['logits'].argmax(dim=-1) == label).sum().item()\n",
        "        correct_sim += (out['sim_logits'].argmax(dim=-1) == label).sum().item()\n",
        "        total += label.size(0)\n",
        "\n",
        "    return correct / total, correct_sim / total\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 70)\n",
        "    print(\"CIFAR-100 Tri-Collective Router\")\n",
        "    print(\"Vision Specialist + Text Specialist + Shared Bridge\")\n",
        "    print(\"Cooperative Fusion (sigmoid, not softmax)\")\n",
        "    print(\"Text: Two-Shot Generation for Semantic Diversity\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Config\n",
        "    BATCH = 128\n",
        "    EPOCHS = 60\n",
        "    DIM = 256\n",
        "    PATCH_SIZE = 4\n",
        "    TEXT_SEQ_LEN = 8\n",
        "    SIM_WEIGHT = 0.3\n",
        "    LR = 1e-3\n",
        "    LR_MIN = 1e-5\n",
        "    WARMUP = 5\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Encoders\n",
        "    VISION_ENCODER = 'dinov3_small'  # 768d\n",
        "    TEXT_ENCODER = 'qwen2.5_1.5b_instruct'  # 1536d\n",
        "    TEXT_STYLE = 'twoshot'  # 'twoshot', 'last_token', or 'repeated'\n",
        "\n",
        "    print(f\"Device: {DEVICE}\")\n",
        "    print(f\"Vision: {VISION_ENCODER}\")\n",
        "    print(f\"Text: {TEXT_ENCODER} ({TEXT_STYLE})\")\n",
        "    print(f\"Compile: {COMPILE}\")\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    # Transforms (augmented images, clean expert)\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    # Load datasets\n",
        "    train_ds = datasets.CIFAR100('./data', train=True, download=True, transform=train_tf)\n",
        "    test_ds = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # Cache vision (clean images)\n",
        "    train_v, vision_dim = build_vision_cache(VISION_ENCODER, 'train', device=str(DEVICE))\n",
        "    test_v, _ = build_vision_cache(VISION_ENCODER, 'test', device=str(DEVICE))\n",
        "    train_v, test_v = train_v.float(), test_v.float()\n",
        "\n",
        "    # Cache text (TWO-SHOT for semantic diversity)\n",
        "    text_cacher = QwenTextCacher(TEXT_ENCODER, device=str(DEVICE))\n",
        "    text_latents = text_cacher.build_cache(prompt_style=TEXT_STYLE).float()\n",
        "    text_dim = text_cacher.embed_dim\n",
        "\n",
        "    print(f\"Vision dim: {vision_dim}\")\n",
        "    print(f\"Text dim: {text_dim}\")\n",
        "\n",
        "    # Datasets\n",
        "    train_cached = CachedDataset(train_ds, train_v)\n",
        "    test_cached = CachedDataset(test_ds, test_v)\n",
        "\n",
        "    train_loader = DataLoader(train_cached, BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_cached, BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # Build router\n",
        "    router = TriCollectiveRouter(\n",
        "        dim=DIM,\n",
        "        patch_size=PATCH_SIZE,\n",
        "        vision_expert_dim=vision_dim,\n",
        "        text_dim=text_dim,\n",
        "        text_seq_len=TEXT_SEQ_LEN,\n",
        "        text_latents=text_latents,\n",
        "        tower_depth=2,\n",
        "        num_heads=4,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters() if p.requires_grad)\n",
        "    print(f\"\\nParams: {params:,}\")\n",
        "    print(f\"Tower configs: cantor±, beatrix±, helix±, simplex± (8 towers × 3 collectives = 24 total)\")\n",
        "\n",
        "    # Compile\n",
        "    if COMPILE:\n",
        "        print(\"Compiling...\")\n",
        "        router = torch.compile(router, mode='reduce-overhead')\n",
        "\n",
        "    # Optimizer + scheduler\n",
        "    opt = torch.optim.AdamW(router.parameters(), lr=LR, weight_decay=0.05)\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < WARMUP:\n",
        "            return (epoch + 1) / WARMUP\n",
        "        progress = (epoch - WARMUP) / (EPOCHS - WARMUP)\n",
        "        return LR_MIN/LR + (1 - LR_MIN/LR) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 70)\n",
        "    print(f\"Loss: CE + {SIM_WEIGHT}×SimCE\")\n",
        "    print(f\"LR: {LR} → {LR_MIN} (cosine, warmup={WARMUP})\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    best, best_sim = 0.0, 0.0\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        loss, ce, sim_ce, train_acc = train_epoch(\n",
        "            router, train_loader, opt, DEVICE, sim_weight=SIM_WEIGHT\n",
        "        )\n",
        "        scheduler.step()\n",
        "\n",
        "        test_acc, test_sim = evaluate(router, test_loader, DEVICE)\n",
        "\n",
        "        is_best = test_acc > best\n",
        "        is_best_sim = test_sim > best_sim\n",
        "        best = max(best, test_acc)\n",
        "        best_sim = max(best_sim, test_sim)\n",
        "\n",
        "        mark = '*' if is_best else ''\n",
        "        sim_mark = '^' if is_best_sim else ''\n",
        "\n",
        "        print(f\"E{epoch:02d} | lr={lr:.6f} | loss={loss:.3f} ce={ce:.3f} sim={sim_ce:.3f} | \"\n",
        "              f\"train={100*train_acc:.1f}% | test={100*test_acc:.1f}%{mark} sim={100*test_sim:.1f}%{sim_mark} | \"\n",
        "              f\"{time.time()-t0:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(f\"Best Direct:     {100*best:.2f}%\")\n",
        "    print(f\"Best Similarity: {100*best_sim:.2f}%\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7r9bbhDkaTd",
        "outputId": "1764eb16-a25e-4f83-de8b-3cd9ce4da996"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "CIFAR-100 Tri-Collective Router\n",
            "Vision Specialist + Text Specialist + Shared Bridge\n",
            "Cooperative Fusion (sigmoid, not softmax)\n",
            "Text: Two-Shot Generation for Semantic Diversity\n",
            "======================================================================\n",
            "Device: cuda\n",
            "Vision: dinov3_small\n",
            "Text: qwen2.5_1.5b_instruct (twoshot)\n",
            "Compile: True\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_small_train.pt\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_small_test.pt\n",
            "QwenTextCacher: qwen2.5_1.5b_instruct (1536d)\n",
            "Building text cache: encoder_cache/cifar100/qwen2.5_1.5b_instruct_twoshot_class_embeddings.pt (style=twoshot)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding (twoshot): 100%|██████████| 100/100 [01:41<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: encoder_cache/cifar100/qwen2.5_1.5b_instruct_twoshot_class_embeddings.pt (torch.Size([100, 1536]))\n",
            "Saved descriptions: encoder_cache/cifar100/qwen2.5_1.5b_instruct_twoshot_class_embeddings.descriptions.txt\n",
            "  Mean similarity: 0.8267 ± 0.1567\n",
            "  Range: [-0.0009, 0.9947]\n",
            "Vision dim: 768\n",
            "Text dim: 1536\n",
            "\n",
            "Params: 43,629,984\n",
            "Tower configs: cantor±, beatrix±, helix±, simplex± (8 towers × 3 collectives = 24 total)\n",
            "Compiling...\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Loss: CE + 0.3×SimCE\n",
            "LR: 0.001 → 1e-05 (cosine, warmup=5)\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.000200 | loss=5.246 ce=4.032 sim=4.047 | train=7.8% | test=13.4%* sim=11.5%^ | 78.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.000400 | loss=4.567 ce=3.487 sim=3.600 | train=15.9% | test=20.1%* sim=18.3%^ | 32.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.000600 | loss=4.191 ce=3.192 sim=3.328 | train=20.8% | test=23.9%* sim=20.8%^ | 29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.000800 | loss=3.997 ce=3.041 sim=3.188 | train=23.5% | test=27.2%* sim=24.2%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.001000 | loss=3.907 ce=2.971 sim=3.121 | train=24.6% | test=26.9% sim=23.3% | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.001000 | loss=3.760 ce=2.859 sim=3.005 | train=27.1% | test=29.4%* sim=26.7%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.000999 | loss=3.632 ce=2.759 sim=2.910 | train=29.2% | test=30.1%* sim=27.9%^ | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.000997 | loss=3.521 ce=2.673 sim=2.829 | train=31.0% | test=32.6%* sim=29.5%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.000993 | loss=3.415 ce=2.589 sim=2.752 | train=32.3% | test=32.1% sim=28.8% | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.000987 | loss=3.335 ce=2.526 sim=2.696 | train=33.6% | test=33.8%* sim=31.0%^ | 29.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.000980 | loss=3.250 ce=2.459 sim=2.635 | train=34.9% | test=34.2%* sim=31.8%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.000971 | loss=3.179 ce=2.403 sim=2.585 | train=36.3% | test=36.8%* sim=33.9%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.000961 | loss=3.107 ce=2.346 sim=2.537 | train=37.4% | test=37.6%* sim=35.2%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.000949 | loss=3.045 ce=2.298 sim=2.492 | train=38.3% | test=39.2%* sim=36.1%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.000936 | loss=2.988 ce=2.252 sim=2.455 | train=39.8% | test=39.3%* sim=37.1%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.000921 | loss=2.929 ce=2.205 sim=2.415 | train=40.3% | test=41.5%* sim=38.9%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.000905 | loss=2.874 ce=2.161 sim=2.377 | train=41.5% | test=40.6% sim=38.5% | 29.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.000888 | loss=2.819 ce=2.116 sim=2.345 | train=42.5% | test=41.5%* sim=39.0%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.000870 | loss=2.763 ce=2.073 sim=2.301 | train=43.2% | test=42.7%* sim=39.7%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.000850 | loss=2.717 ce=2.036 sim=2.269 | train=44.3% | test=43.5%* sim=40.6%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | lr=0.000829 | loss=2.667 ce=1.997 sim=2.234 | train=45.1% | test=44.1%* sim=41.9%^ | 29.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | lr=0.000807 | loss=2.623 ce=1.961 sim=2.209 | train=46.1% | test=44.4%* sim=42.7%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | lr=0.000784 | loss=2.573 ce=1.921 sim=2.175 | train=46.8% | test=45.3%* sim=42.7%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | lr=0.000761 | loss=2.527 ce=1.884 sim=2.144 | train=48.1% | test=46.1%* sim=43.5%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E25 | lr=0.000736 | loss=2.486 ce=1.852 sim=2.113 | train=48.8% | test=45.2% sim=42.9% | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E26 | lr=0.000711 | loss=2.448 ce=1.821 sim=2.088 | train=49.4% | test=47.4%* sim=45.0%^ | 29.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E27 | lr=0.000685 | loss=2.392 ce=1.776 sim=2.053 | train=50.3% | test=47.4%* sim=45.2%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E28 | lr=0.000658 | loss=2.341 ce=1.735 sim=2.020 | train=51.1% | test=46.8% sim=45.1% | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E29 | lr=0.000631 | loss=2.309 ce=1.710 sim=1.997 | train=52.1% | test=47.5%* sim=45.7%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E30 | lr=0.000603 | loss=2.247 ce=1.659 sim=1.961 | train=53.2% | test=48.4%* sim=46.6%^ | 29.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E31 | lr=0.000575 | loss=2.203 ce=1.624 sim=1.931 | train=54.3% | test=49.1%* sim=47.1%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E32 | lr=0.000547 | loss=2.166 ce=1.594 sim=1.904 | train=54.8% | test=50.3%* sim=47.7%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E33 | lr=0.000519 | loss=2.113 ce=1.551 sim=1.872 | train=55.7% | test=50.8%* sim=48.8%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E34 | lr=0.000491 | loss=2.061 ce=1.509 sim=1.840 | train=56.9% | test=50.9%* sim=49.1%^ | 29.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E35 | lr=0.000463 | loss=2.025 ce=1.481 sim=1.815 | train=57.5% | test=50.9%* sim=48.7% | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E36 | lr=0.000435 | loss=1.976 ce=1.440 sim=1.788 | train=58.6% | test=51.6%* sim=49.5%^ | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E37 | lr=0.000407 | loss=1.927 ce=1.400 sim=1.757 | train=59.7% | test=51.8%* sim=50.0%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E38 | lr=0.000379 | loss=1.885 ce=1.366 sim=1.729 | train=60.5% | test=52.7%* sim=50.8%^ | 29.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E39 | lr=0.000352 | loss=1.836 ce=1.326 sim=1.698 | train=61.5% | test=52.9%* sim=51.0%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E40 | lr=0.000325 | loss=1.791 ce=1.288 sim=1.674 | train=62.5% | test=52.8% sim=51.2%^ | 26.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E41 | lr=0.000299 | loss=1.748 ce=1.254 sim=1.646 | train=63.4% | test=53.4%* sim=51.6%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E42 | lr=0.000274 | loss=1.704 ce=1.219 sim=1.619 | train=64.2% | test=53.4%* sim=51.6%^ | 29.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E43 | lr=0.000249 | loss=1.661 ce=1.183 sim=1.594 | train=65.0% | test=53.3% sim=51.9%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E44 | lr=0.000226 | loss=1.621 ce=1.151 sim=1.567 | train=66.0% | test=53.7%* sim=52.1%^ | 26.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E45 | lr=0.000203 | loss=1.573 ce=1.111 sim=1.541 | train=67.2% | test=54.2%* sim=52.1%^ | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E46 | lr=0.000181 | loss=1.541 ce=1.085 sim=1.520 | train=67.8% | test=54.1% sim=52.8%^ | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E47 | lr=0.000160 | loss=1.492 ce=1.043 sim=1.494 | train=68.7% | test=53.5% sim=52.2% | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E48 | lr=0.000140 | loss=1.464 ce=1.021 sim=1.476 | train=69.6% | test=54.0% sim=53.1%^ | 29.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E49 | lr=0.000122 | loss=1.428 ce=0.992 sim=1.453 | train=70.3% | test=54.5%* sim=53.1%^ | 26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E50 | lr=0.000105 | loss=1.399 ce=0.968 sim=1.437 | train=71.0% | test=54.3% sim=52.9% | 26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E51 | lr=0.000089 | loss=1.368 ce=0.943 sim=1.416 | train=71.6% | test=54.6%* sim=53.3%^ | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E52 | lr=0.000074 | loss=1.346 ce=0.925 sim=1.405 | train=72.2% | test=54.5% sim=53.3%^ | 26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E53 | lr=0.000061 | loss=1.322 ce=0.905 sim=1.390 | train=72.8% | test=54.9%* sim=53.6%^ | 26.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E54 | lr=0.000049 | loss=1.294 ce=0.881 sim=1.377 | train=73.4% | test=54.8% sim=53.5% | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E55 | lr=0.000039 | loss=1.283 ce=0.873 sim=1.366 | train=73.6% | test=54.5% sim=53.3% | 26.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E56 | lr=0.000030 | loss=1.270 ce=0.862 sim=1.361 | train=73.8% | test=55.0%* sim=53.5% | 29.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E57 | lr=0.000023 | loss=1.259 ce=0.852 sim=1.355 | train=74.3% | test=54.8% sim=53.3% | 26.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E58 | lr=0.000017 | loss=1.249 ce=0.844 sim=1.349 | train=74.3% | test=54.8% sim=53.4% | 26.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E59 | lr=0.000013 | loss=1.245 ce=0.841 sim=1.345 | train=74.4% | test=55.0%* sim=53.6%^ | 26.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E60 | lr=0.000011 | loss=1.242 ce=0.839 sim=1.343 | train=74.5% | test=55.0% sim=53.4% | 26.7s\n",
            "\n",
            "======================================================================\n",
            "Best Direct:     55.05%\n",
            "Best Similarity: 53.65%\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# qwen + t5 base, dino3 convnext-large + vit-b-16"
      ],
      "metadata": {
        "id": "GU3VlZs6Az9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Tri-Collective Router (Dual-Signal, Adjacent Training)\n",
        "================================================================\n",
        "\n",
        "Training Strategy:\n",
        "    Vision Pathway ──→ vision_opt ──→ CE_vision\n",
        "    Text Pathway   ──→ text_opt   ──→ CE_text\n",
        "    Fusion Layer   ──→ fusion_opt ──→ MSE_align + CE_final\n",
        "\n",
        "Multi-Opinion Concatenation:\n",
        "    [v_opinion | v_shared | t_opinion | t_shared | expert_fused] → classifier\n",
        "\n",
        "Text Encoders:\n",
        "    Qwen 1.5B Instruct (1536d) ──┬── AdaptiveFusion ──→ text_fused\n",
        "    T5-Base (768d) ──────────────┘\n",
        "\n",
        "Vision Encoders:\n",
        "    DINOv3-Large (1536d) ──┬── AdaptiveFusion ──→ vision_fused\n",
        "    DINOv3-ViT-B-16 (768d) ┘\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from pathlib import Path\n",
        "import time\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "# === GEOFRACTAL IMPORTS ===\n",
        "from geofractal.router.base_router import BaseRouter\n",
        "from geofractal.router.components.torch_component import TorchComponent\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, ConfigurableCollective,\n",
        "    build_tower_collective, preset_pos_neg_pairs,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR-100 CLASS INFO\n",
        "# =============================================================================\n",
        "\n",
        "CIFAR100_CLASSES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# QWEN TEXT CACHER\n",
        "# =============================================================================\n",
        "\n",
        "class QwenTextCacher:\n",
        "    \"\"\"Cache Qwen hidden states with two-shot generation.\"\"\"\n",
        "\n",
        "    QWEN_MODELS = {\n",
        "        'qwen2.5_0.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-0.5B-Instruct', 'dim': 896},\n",
        "        'qwen2.5_1.5b_instruct': {'hf_path': 'Qwen/Qwen2.5-1.5B-Instruct', 'dim': 1536},\n",
        "        'qwen2.5_3b_instruct': {'hf_path': 'Qwen/Qwen2.5-3B-Instruct', 'dim': 2048},\n",
        "    }\n",
        "\n",
        "    def __init__(self, model_name: str = 'qwen2.5_1.5b_instruct', cache_dir: str = './encoder_cache', device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        config = self.QWEN_MODELS.get(model_name)\n",
        "        if config is None:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "        self.hf_path = config['hf_path']\n",
        "        self.dim = config['dim']\n",
        "        print(f\"QwenTextCacher: {model_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self, style: str = 'twoshot') -> Path:\n",
        "        return self.cache_dir / 'cifar100' / f\"{self.model_name}_{style}_class_embeddings.pt\"\n",
        "\n",
        "    def _descriptions_path(self) -> Path:\n",
        "        return self.cache_dir / 'cifar100' / f\"{self.model_name}_twoshot_descriptions.pt\"\n",
        "\n",
        "    def _build_twoshot_prompt(self, tokenizer, class_name: str) -> str:\n",
        "        cls_name = class_name.replace('_', ' ')\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You describe objects in exactly one sentence. Be specific about visual features.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Describe: car\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"A four-wheeled motor vehicle with windows, doors, headlights, and a metal body used for transportation on roads.\"},\n",
        "            {\"role\": \"user\", \"content\": \"Describe: sunflower\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"A tall plant with a large circular flower head containing yellow petals surrounding a brown seed-filled center.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Describe: {cls_name}\"},\n",
        "        ]\n",
        "        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, prompt_style: str = 'twoshot', force_rebuild: bool = False) -> Tuple[Tensor, Dict[str, str]]:\n",
        "        cache_path = self._cache_path(prompt_style)\n",
        "        desc_path = self._descriptions_path()\n",
        "\n",
        "        if cache_path.exists() and desc_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading Qwen cache: {cache_path}\")\n",
        "            embeddings = torch.load(cache_path, weights_only=True)\n",
        "            descriptions = torch.load(desc_path, weights_only=True)\n",
        "            return embeddings, descriptions\n",
        "\n",
        "        print(f\"Building Qwen cache: {cache_path}\")\n",
        "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(self.hf_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(self.hf_path, torch_dtype=torch.float16, device_map=self.device, trust_remote_code=True)\n",
        "        model.eval()\n",
        "\n",
        "        embeddings, descriptions = [], {}\n",
        "        for cls in tqdm(CIFAR100_CLASSES, desc=\"Qwen (twoshot)\"):\n",
        "            prompt = self._build_twoshot_prompt(tokenizer, cls)\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "            outputs = model.generate(**inputs, max_new_tokens=50, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n",
        "            generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
        "            description = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "            descriptions[cls] = description\n",
        "\n",
        "            desc_inputs = tokenizer(description, return_tensors=\"pt\").to(self.device)\n",
        "            desc_outputs = model(**desc_inputs, output_hidden_states=True, return_dict=True)\n",
        "            emb = desc_outputs.hidden_states[-1][0, -1, :].cpu().float()\n",
        "            embeddings.append(emb)\n",
        "\n",
        "        embeddings = torch.stack(embeddings, dim=0)\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        torch.save(embeddings, cache_path)\n",
        "        torch.save(descriptions, desc_path)\n",
        "        print(f\"Saved Qwen: {cache_path} ({embeddings.shape})\")\n",
        "\n",
        "        return embeddings, descriptions\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# T5 TEXT CACHER\n",
        "# =============================================================================\n",
        "\n",
        "class T5TextCacher:\n",
        "    \"\"\"Cache T5 encoder hidden states for class descriptions.\"\"\"\n",
        "\n",
        "    T5_MODELS = {\n",
        "        't5_small': {'hf_path': 'google-t5/t5-small', 'dim': 512},\n",
        "        't5_base': {'hf_path': 'google-t5/t5-base', 'dim': 768},\n",
        "        't5_large': {'hf_path': 'google-t5/t5-large', 'dim': 1024},\n",
        "    }\n",
        "\n",
        "    def __init__(self, model_name: str = 't5_base', cache_dir: str = './encoder_cache', device: str = 'cuda'):\n",
        "        self.model_name = model_name\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.device = device\n",
        "        config = self.T5_MODELS.get(model_name)\n",
        "        if config is None:\n",
        "            raise ValueError(f\"Unknown model: {model_name}\")\n",
        "        self.hf_path = config['hf_path']\n",
        "        self.dim = config['dim']\n",
        "        print(f\"T5TextCacher: {model_name} ({self.dim}d)\")\n",
        "\n",
        "    @property\n",
        "    def embed_dim(self) -> int:\n",
        "        return self.dim\n",
        "\n",
        "    def _cache_path(self) -> Path:\n",
        "        return self.cache_dir / 'cifar100' / f\"{self.model_name}_from_qwen_descriptions.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, descriptions: Dict[str, str], force_rebuild: bool = False) -> Tensor:\n",
        "        cache_path = self._cache_path()\n",
        "        if cache_path.exists() and not force_rebuild:\n",
        "            print(f\"Loading T5 cache: {cache_path}\")\n",
        "            return torch.load(cache_path, weights_only=True)\n",
        "\n",
        "        print(f\"Building T5 cache: {cache_path}\")\n",
        "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        from transformers import T5EncoderModel, T5Tokenizer\n",
        "        tokenizer = T5Tokenizer.from_pretrained(self.hf_path)\n",
        "        model = T5EncoderModel.from_pretrained(self.hf_path, torch_dtype=torch.float16).to(self.device)\n",
        "        model.eval()\n",
        "\n",
        "        embeddings = []\n",
        "        for cls in tqdm(CIFAR100_CLASSES, desc=\"T5 encoding\"):\n",
        "            inputs = tokenizer(descriptions[cls], return_tensors=\"pt\", max_length=128, truncation=True, padding='max_length').to(self.device)\n",
        "            outputs = model(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
        "            hidden = outputs.last_hidden_state\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1)\n",
        "            pooled = (hidden * mask).sum(dim=1) / mask.sum(dim=1)\n",
        "            embeddings.append(pooled.cpu().float().squeeze(0))\n",
        "\n",
        "        embeddings = torch.stack(embeddings, dim=0)\n",
        "        del model, tokenizer\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        torch.save(embeddings, cache_path)\n",
        "        print(f\"Saved T5: {cache_path} ({embeddings.shape})\")\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DUAL VISION CACHING\n",
        "# =============================================================================\n",
        "\n",
        "from geofractal.router.components.encoder_data_component import MultiVisionEncode\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_dual_vision_cache(\n",
        "    encoder_names: List[str], split: str, dataset_name: str = 'cifar100',\n",
        "    cache_dir: str = './encoder_cache', device: str = 'cuda',\n",
        ") -> Tuple[List[Tensor], List[int]]:\n",
        "    latents_list, dims = [], []\n",
        "\n",
        "    for encoder_name in encoder_names:\n",
        "        cache_path = Path(cache_dir) / dataset_name / f\"{encoder_name}_{split}.pt\"\n",
        "\n",
        "        if cache_path.exists():\n",
        "            print(f\"Loading vision cache: {cache_path}\")\n",
        "            latents = torch.load(cache_path, weights_only=True)\n",
        "            latents_list.append(latents)\n",
        "            dims.append(latents.shape[-1])\n",
        "            continue\n",
        "\n",
        "        print(f\"Building vision cache: {cache_path}\")\n",
        "        cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        encoder = MultiVisionEncode(encoders=[encoder_name], dataset_name=dataset_name, device=device, cache_enabled=False, concatenate=True, pool_output=True)\n",
        "        dim = encoder.combined_dim\n",
        "        dims.append(dim)\n",
        "\n",
        "        raw_dataset = datasets.CIFAR100('./data', train=(split == 'train'), download=True, transform=transforms.ToTensor())\n",
        "        latents = []\n",
        "        batch_size = 64\n",
        "\n",
        "        for i in tqdm(range(0, len(raw_dataset), batch_size), desc=f\"Encoding {encoder_name} {split}\"):\n",
        "            batch_images = [raw_dataset[j][0] for j in range(i, min(i + batch_size, len(raw_dataset)))]\n",
        "            batch_tensor = torch.stack(batch_images)\n",
        "            z = encoder.encode(batch_tensor)\n",
        "            latents.append(z.cpu().float())\n",
        "\n",
        "        latents = torch.cat(latents, dim=0)\n",
        "        torch.save(latents, cache_path)\n",
        "        latents_list.append(latents)\n",
        "        encoder.unload_all()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return latents_list, dims\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class DualCachedDataset(Dataset):\n",
        "    def __init__(self, base: Dataset, v1: Tensor, v2: Tensor):\n",
        "        self.base, self.v1, self.v2 = base, v1, v2\n",
        "    def __len__(self):\n",
        "        return len(self.base)\n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.base[idx]\n",
        "        return img, self.v1[idx], self.v2[idx], label\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, dim: int, patch_size: int = 4, img_size: int = 32):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.num_patches, dim) * 0.02)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return self.norm(x + self.pos_embed)\n",
        "\n",
        "\n",
        "class TextSequenceExpander(nn.Module):\n",
        "    def __init__(self, text_dim: int, dim: int, seq_len: int = 8):\n",
        "        super().__init__()\n",
        "        self.seq_len = seq_len\n",
        "        self.dim = dim\n",
        "        self.proj = nn.Sequential(nn.LayerNorm(text_dim), nn.Linear(text_dim, dim * seq_len), nn.GELU())\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, seq_len, dim) * 0.02)\n",
        "        self.refine = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.proj(x).view(-1, self.seq_len, self.dim)\n",
        "        return self.refine(x + self.pos_embed)\n",
        "\n",
        "\n",
        "class DualEncoderFusion(nn.Module):\n",
        "    def __init__(self, name: str, dim1: int, dim2: int, output_dim: int):\n",
        "        super().__init__()\n",
        "        self.proj1 = nn.Sequential(nn.LayerNorm(dim1), nn.Linear(dim1, output_dim), nn.GELU())\n",
        "        self.proj2 = nn.Sequential(nn.LayerNorm(dim2), nn.Linear(dim2, output_dim), nn.GELU())\n",
        "        self.fusion = AdaptiveFusion(f'{name}_adaptive', num_inputs=2, in_features=output_dim)\n",
        "\n",
        "    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:\n",
        "        return self.fusion(self.proj1(x1), self.proj2(x2))\n",
        "\n",
        "\n",
        "class MultiOpinionClassifier(nn.Module):\n",
        "    \"\"\"Concatenates multiple opinions and classifies.\"\"\"\n",
        "    def __init__(self, num_opinions: int, dim: int, num_classes: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(num_opinions * dim),\n",
        "            nn.Linear(num_opinions * dim, dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim * 2, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, *opinions: Tensor) -> Tensor:\n",
        "        return self.classifier(torch.cat(opinions, dim=-1))\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRI-COLLECTIVE ROUTER (ADJACENT TRAINING)\n",
        "# =============================================================================\n",
        "\n",
        "class TriCollectiveRouterAdjacent(BaseRouter):\n",
        "    \"\"\"Tri-Collective with separate parameter groups for adjacent training.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int = 256,\n",
        "        patch_size: int = 4,\n",
        "        num_classes: int = 100,\n",
        "        vision_dim_1: int = 1536,\n",
        "        vision_dim_2: int = 768,\n",
        "        text_dim_1: int = 1536,\n",
        "        text_dim_2: int = 768,\n",
        "        text_seq_len: int = 8,\n",
        "        tower_depth: int = 2,\n",
        "        num_heads: int = 4,\n",
        "        fingerprint_dim: int = 64,\n",
        "        text_latents_qwen: Tensor = None,\n",
        "        text_latents_t5: Tensor = None,\n",
        "    ):\n",
        "        super().__init__('tri_collective_adjacent', strict=False)\n",
        "\n",
        "        if text_latents_qwen is None or text_latents_t5 is None:\n",
        "            raise ValueError(\"Both text latents required\")\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.dim = dim\n",
        "\n",
        "        self.objects['config'] = {\n",
        "            'dim': dim, 'patch_size': patch_size, 'num_classes': num_classes,\n",
        "            'vision_dims': [vision_dim_1, vision_dim_2],\n",
        "            'text_dims': [text_dim_1, text_dim_2],\n",
        "        }\n",
        "\n",
        "        # Text latents (frozen anchors)\n",
        "        self.register_buffer('text_latents_qwen', text_latents_qwen.float())\n",
        "        self.register_buffer('text_latents_t5', text_latents_t5.float())\n",
        "\n",
        "        # Tower configs - 8 towers (4 geometries × pos/neg)\n",
        "        tower_configs = preset_pos_neg_pairs(['cantor', 'beatrix', 'helix', 'simplex'])\n",
        "\n",
        "        # === VISION PATHWAY ===\n",
        "        self.attach('vision_fusion', DualEncoderFusion('vision_dual', vision_dim_1, vision_dim_2, dim))\n",
        "        self.attach('patch_embed', PatchEmbed(dim, patch_size))\n",
        "        self.attach('vision_collective', build_tower_collective(\n",
        "            configs=tower_configs, dim=dim, default_depth=tower_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name='vision_specialist',\n",
        "        ))\n",
        "        self.attach('vision_classifier', nn.Sequential(\n",
        "            nn.LayerNorm(dim), nn.Linear(dim, dim), nn.GELU(),\n",
        "            nn.Dropout(0.1), nn.Linear(dim, num_classes),\n",
        "        ))\n",
        "\n",
        "        # === TEXT PATHWAY ===\n",
        "        self.attach('text_fusion', DualEncoderFusion('text_dual', text_dim_1, text_dim_2, dim))\n",
        "        self.attach('text_expander', TextSequenceExpander(dim, dim, text_seq_len))\n",
        "        self.attach('text_collective', build_tower_collective(\n",
        "            configs=tower_configs, dim=dim, default_depth=tower_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name='text_specialist',\n",
        "        ))\n",
        "        self.attach('text_anchor_proj', nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim)))\n",
        "\n",
        "        # === SHARED PATHWAY ===\n",
        "        self.attach('shared_collective', build_tower_collective(\n",
        "            configs=tower_configs, dim=dim, default_depth=tower_depth,\n",
        "            num_heads=num_heads, fingerprint_dim=fingerprint_dim,\n",
        "            fusion_type='adaptive', name='shared_bridge',\n",
        "        ))\n",
        "\n",
        "        # === FUSION / FINAL STAGE ===\n",
        "        self.attach('multi_opinion_classifier', MultiOpinionClassifier(\n",
        "            num_opinions=5, dim=dim, num_classes=num_classes\n",
        "        ))\n",
        "        self.attach('vision_align_proj', nn.Linear(dim, dim))\n",
        "        self.attach('text_align_proj', nn.Linear(dim, dim))\n",
        "\n",
        "    def analyze_collectives(self):\n",
        "        \"\"\"Pre-analyze all WideRouter collectives for compile safety.\"\"\"\n",
        "        print(\"Analyzing collective structures...\")\n",
        "        self['vision_collective'].analyze_structure()\n",
        "        self['text_collective'].analyze_structure()\n",
        "        self['shared_collective'].analyze_structure()\n",
        "        print(\"  ✓ All collectives analyzed\")\n",
        "\n",
        "    def get_param_groups(self) -> Dict[str, List[nn.Parameter]]:\n",
        "        \"\"\"Return separate parameter groups for adjacent training.\"\"\"\n",
        "        return {\n",
        "            'vision': list(self['vision_fusion'].parameters()) +\n",
        "                      list(self['patch_embed'].parameters()) +\n",
        "                      list(self['vision_collective'].parameters()) +\n",
        "                      list(self['vision_classifier'].parameters()),\n",
        "            'text': list(self['text_fusion'].parameters()) +\n",
        "                    list(self['text_expander'].parameters()) +\n",
        "                    list(self['text_collective'].parameters()) +\n",
        "                    list(self['text_anchor_proj'].parameters()),\n",
        "            'shared': list(self['shared_collective'].parameters()),\n",
        "            'fusion': list(self['multi_opinion_classifier'].parameters()) +\n",
        "                      list(self['vision_align_proj'].parameters()) +\n",
        "                      list(self['text_align_proj'].parameters()),\n",
        "        }\n",
        "\n",
        "    def _get_fused_text_anchors(self, device: torch.device) -> Tensor:\n",
        "        qwen = self.text_latents_qwen.to(device)\n",
        "        t5 = self.text_latents_t5.to(device)\n",
        "        return self['text_fusion'](qwen, t5)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        images: Tensor,\n",
        "        vision_latents_1: Tensor,\n",
        "        vision_latents_2: Tensor,\n",
        "        labels: Optional[Tensor] = None,\n",
        "    ) -> Dict[str, Tensor]:\n",
        "        B = images.shape[0]\n",
        "        device = images.device\n",
        "\n",
        "        # === VISION PATHWAY ===\n",
        "        expert_fused = self['vision_fusion'](vision_latents_1, vision_latents_2)\n",
        "        v_patches = self['patch_embed'](images)\n",
        "        v_opinion = self['vision_collective'](v_patches).fused\n",
        "        v_shared = self['shared_collective'](v_patches).fused\n",
        "        vision_logits = self['vision_classifier'](v_opinion + expert_fused)\n",
        "\n",
        "        # === TEXT PATHWAY ===\n",
        "        text_anchors = self._get_fused_text_anchors(device)\n",
        "        all_text_seq = self['text_expander'](text_anchors)\n",
        "        t_opinion_all = self['text_collective'](all_text_seq).fused\n",
        "        t_shared_all = self['shared_collective'](all_text_seq).fused\n",
        "\n",
        "        text_anchors_proj = self['text_anchor_proj']((t_opinion_all + t_shared_all) * 0.5)\n",
        "        text_norm = F.normalize(text_anchors_proj, dim=-1)\n",
        "        v_for_sim = F.normalize(v_opinion, dim=-1)\n",
        "        text_sim_logits = torch.matmul(v_for_sim, text_norm.T) * 10.0\n",
        "\n",
        "        # === ALIGNMENT ===\n",
        "        vision_aligned = self['vision_align_proj'](v_opinion)\n",
        "        text_aligned = self['text_align_proj'](text_anchors[labels]) if labels is not None else None\n",
        "\n",
        "        # === MULTI-OPINION FUSION ===\n",
        "        t_opinion_ctx = t_opinion_all.mean(dim=0, keepdim=True).expand(B, -1)\n",
        "        t_shared_ctx = t_shared_all.mean(dim=0, keepdim=True).expand(B, -1)\n",
        "        final_logits = self['multi_opinion_classifier'](\n",
        "            v_opinion, v_shared, t_opinion_ctx, t_shared_ctx, expert_fused\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'logits': final_logits,\n",
        "            'vision_logits': vision_logits,\n",
        "            'text_sim_logits': text_sim_logits,\n",
        "            'vision_aligned': vision_aligned,\n",
        "            'text_aligned': text_aligned,\n",
        "            'v_opinion': v_opinion,\n",
        "            'v_shared': v_shared,\n",
        "            't_opinion_all': t_opinion_all,\n",
        "            't_shared_all': t_shared_all,\n",
        "            'expert_fused': expert_fused,\n",
        "            'text_anchors': text_anchors,\n",
        "        }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch_adjacent(\n",
        "    router: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizers: Dict[str, torch.optim.Optimizer],\n",
        "    device: torch.device,\n",
        "    mse_weight: float = 0.5,\n",
        "    max_grad_norm: float = 1.0,\n",
        ") -> Dict[str, float]:\n",
        "    router.train()\n",
        "    metrics = {k: 0. for k in ['loss_total', 'loss_vision', 'loss_text', 'loss_final', 'loss_mse', 'acc_vision', 'acc_text', 'acc_final', 'n_samples']}\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for img, v1, v2, label in pbar:\n",
        "        img = img.to(device, dtype=torch.float32)\n",
        "        v1 = v1.to(device, dtype=torch.float32)\n",
        "        v2 = v2.to(device, dtype=torch.float32)\n",
        "        label = label.to(device)\n",
        "        B = label.size(0)\n",
        "\n",
        "        for opt in optimizers.values():\n",
        "            opt.zero_grad()\n",
        "\n",
        "        out = router(img, v1, v2, labels=label)\n",
        "\n",
        "        ce_vision = F.cross_entropy(out['vision_logits'], label)\n",
        "        ce_text = F.cross_entropy(out['text_sim_logits'], label)\n",
        "        ce_final = F.cross_entropy(out['logits'], label)\n",
        "        mse_align = F.mse_loss(out['vision_aligned'], out['text_aligned']) if out['text_aligned'] is not None else torch.tensor(0., device=device)\n",
        "\n",
        "        loss_total = ce_vision + ce_text + ce_final + mse_weight * mse_align\n",
        "        loss_total.backward()\n",
        "\n",
        "        for opt in optimizers.values():\n",
        "            params = [p for g in opt.param_groups for p in g['params']]\n",
        "            torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "            opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            metrics['loss_total'] += loss_total.item() * B\n",
        "            metrics['loss_vision'] += ce_vision.item() * B\n",
        "            metrics['loss_text'] += ce_text.item() * B\n",
        "            metrics['loss_final'] += ce_final.item() * B\n",
        "            metrics['loss_mse'] += mse_align.item() * B\n",
        "            metrics['acc_vision'] += (out['vision_logits'].argmax(-1) == label).sum().item()\n",
        "            metrics['acc_text'] += (out['text_sim_logits'].argmax(-1) == label).sum().item()\n",
        "            metrics['acc_final'] += (out['logits'].argmax(-1) == label).sum().item()\n",
        "            metrics['n_samples'] += B\n",
        "\n",
        "        pbar.set_postfix({'v': f\"{100*metrics['acc_vision']/metrics['n_samples']:.1f}%\", 't': f\"{100*metrics['acc_text']/metrics['n_samples']:.1f}%\", 'f': f\"{100*metrics['acc_final']/metrics['n_samples']:.1f}%\"})\n",
        "\n",
        "    n = metrics['n_samples']\n",
        "    return {k: v/n for k, v in metrics.items() if k != 'n_samples'}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router: nn.Module, loader: DataLoader, device: torch.device) -> Dict[str, float]:\n",
        "    router.eval()\n",
        "    correct_v, correct_t, correct_f, total = 0, 0, 0, 0\n",
        "    for img, v1, v2, label in loader:\n",
        "        img, v1, v2, label = img.to(device), v1.to(device), v2.to(device), label.to(device)\n",
        "        out = router(img, v1, v2)\n",
        "        correct_v += (out['vision_logits'].argmax(-1) == label).sum().item()\n",
        "        correct_t += (out['text_sim_logits'].argmax(-1) == label).sum().item()\n",
        "        correct_f += (out['logits'].argmax(-1) == label).sum().item()\n",
        "        total += label.size(0)\n",
        "    return {'acc_vision': correct_v/total, 'acc_text': correct_t/total, 'acc_final': correct_f/total}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 Tri-Collective Router (ADJACENT TRAINING)\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Vision: DINOv3-Large + DINOv3-ViT-B-16 → AdaptiveFusion\")\n",
        "    print(\"Text:   Qwen 1.5B + T5-Base → AdaptiveFusion\")\n",
        "    print(\"Training: Separate optimizers (vision, text, shared, fusion)\")\n",
        "    print(\"Loss: CE_vision + CE_text + CE_final + MSE_align\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Config\n",
        "    BATCH, EPOCHS, DIM = 128, 60, 256\n",
        "    PATCH_SIZE, TEXT_SEQ_LEN = 4, 8\n",
        "    MSE_WEIGHT, LR, LR_MIN, WARMUP = 0.5, 1e-3, 1e-5, 5\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    VISION_ENCODER_1 = 'dinov3_large'\n",
        "    VISION_ENCODER_2 = 'dinov3_vitb16'\n",
        "    TEXT_ENCODER_QWEN = 'qwen2.5_1.5b_instruct'\n",
        "    TEXT_ENCODER_T5 = 't5_base'\n",
        "\n",
        "    print(f\"\\nDevice: {DEVICE}, Compile: {COMPILE}\")\n",
        "    print(f\"Vision: {VISION_ENCODER_1} + {VISION_ENCODER_2}\")\n",
        "    print(f\"Text: {TEXT_ENCODER_QWEN} + {TEXT_ENCODER_T5}\")\n",
        "\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    train_tf = transforms.Compose([transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(), transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "    test_tf = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "    train_ds = datasets.CIFAR100('./data', train=True, download=True, transform=train_tf)\n",
        "    test_ds = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    # === CACHING ===\n",
        "    print(\"\\n--- Vision Caching ---\")\n",
        "    [train_v1, train_v2], [vdim1, vdim2] = build_dual_vision_cache([VISION_ENCODER_1, VISION_ENCODER_2], 'train', device=str(DEVICE))\n",
        "    [test_v1, test_v2], _ = build_dual_vision_cache([VISION_ENCODER_1, VISION_ENCODER_2], 'test', device=str(DEVICE))\n",
        "    train_v1, train_v2, test_v1, test_v2 = train_v1.float(), train_v2.float(), test_v1.float(), test_v2.float()\n",
        "    print(f\"Vision dims: {vdim1} + {vdim2}\")\n",
        "\n",
        "    print(\"\\n--- Text Caching ---\")\n",
        "    qwen_cacher = QwenTextCacher(TEXT_ENCODER_QWEN, device=str(DEVICE))\n",
        "    text_qwen, descriptions = qwen_cacher.build_cache(prompt_style='twoshot')\n",
        "    t5_cacher = T5TextCacher(TEXT_ENCODER_T5, device=str(DEVICE))\n",
        "    text_t5 = t5_cacher.build_cache(descriptions).float()\n",
        "    text_qwen = text_qwen.float()\n",
        "    print(f\"Text dims: Qwen={text_qwen.shape[-1]}, T5={text_t5.shape[-1]}\")\n",
        "\n",
        "    train_loader = DataLoader(DualCachedDataset(train_ds, train_v1, train_v2), BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(DualCachedDataset(test_ds, test_v1, test_v2), BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    # === BUILD ROUTER ===\n",
        "    print(\"\\n--- Building Router ---\")\n",
        "    router = TriCollectiveRouterAdjacent(\n",
        "        dim=DIM, patch_size=PATCH_SIZE,\n",
        "        vision_dim_1=vdim1, vision_dim_2=vdim2,\n",
        "        text_dim_1=text_qwen.shape[-1], text_dim_2=text_t5.shape[-1],\n",
        "        text_seq_len=TEXT_SEQ_LEN, text_latents_qwen=text_qwen, text_latents_t5=text_t5,\n",
        "        tower_depth=2, num_heads=4,\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal params: {params:,}\")\n",
        "\n",
        "    # === GET PARAM GROUPS BEFORE COMPILE ===\n",
        "    param_groups = router.get_param_groups()\n",
        "\n",
        "    optimizers = {name: torch.optim.AdamW(params, lr=LR, weight_decay=0.05) for name, params in param_groups.items()}\n",
        "    print(f\"Optimizers: {list(optimizers.keys())}\")\n",
        "    for name, opt in optimizers.items():\n",
        "        print(f\"  {name}: {sum(p.numel() for g in opt.param_groups for p in g['params']):,} params\")\n",
        "\n",
        "    # === ANALYZE AND COMPILE (WideRouter pattern) ===\n",
        "    if COMPILE:\n",
        "        print(\"\\nPre-analyzing collectives and compiling...\")\n",
        "        # 1. Analyze structure on each WideRouter collective BEFORE compile\n",
        "        router.analyze_collectives()\n",
        "        # 2. Now compile the entire router\n",
        "        router.compile(mode=\"default\")\n",
        "        print(\"  ✓ Router compiled\")\n",
        "\n",
        "    def lr_lambda(epoch):\n",
        "        if epoch < WARMUP: return (epoch + 1) / WARMUP\n",
        "        return LR_MIN/LR + (1 - LR_MIN/LR) * 0.5 * (1 + math.cos(math.pi * (epoch - WARMUP) / (EPOCHS - WARMUP)))\n",
        "\n",
        "    schedulers = {name: torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda) for name, opt in optimizers.items()}\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"Loss: CE_vision + CE_text + CE_final + {MSE_WEIGHT}×MSE_align\")\n",
        "    print(f\"LR: {LR} → {LR_MIN} (cosine, warmup={WARMUP})\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_v, best_t, best_f = 0., 0., 0.\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        lr = optimizers['vision'].param_groups[0]['lr']\n",
        "\n",
        "        train_m = train_epoch_adjacent(router, train_loader, optimizers, DEVICE, mse_weight=MSE_WEIGHT)\n",
        "        for s in schedulers.values(): s.step()\n",
        "        test_m = evaluate(router, test_loader, DEVICE)\n",
        "\n",
        "        is_best_v, is_best_t, is_best_f = test_m['acc_vision'] > best_v, test_m['acc_text'] > best_t, test_m['acc_final'] > best_f\n",
        "        best_v, best_t, best_f = max(best_v, test_m['acc_vision']), max(best_t, test_m['acc_text']), max(best_f, test_m['acc_final'])\n",
        "\n",
        "        print(f\"E{epoch:02d} | lr={lr:.6f} | L: {train_m['loss_total']:.3f} (v={train_m['loss_vision']:.3f} t={train_m['loss_text']:.3f} f={train_m['loss_final']:.3f} m={train_m['loss_mse']:.3f}) | \"\n",
        "              f\"Train: v={100*train_m['acc_vision']:.1f} t={100*train_m['acc_text']:.1f} f={100*train_m['acc_final']:.1f} | \"\n",
        "              f\"Test: v={100*test_m['acc_vision']:.1f}%{'*' if is_best_v else ''} t={100*test_m['acc_text']:.1f}%{'^' if is_best_t else ''} f={100*test_m['acc_final']:.1f}%{'!' if is_best_f else ''} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Vision: {100*best_v:.2f}%  |  Best Text: {100*best_t:.2f}%  |  Best Final: {100*best_f:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QdvRL76-A4oA",
        "outputId": "112f389e-06a7-4124-a354-5f7b08469048"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 Tri-Collective Router (ADJACENT TRAINING)\n",
            "================================================================================\n",
            "Vision: DINOv3-Large + DINOv3-ViT-B-16 → AdaptiveFusion\n",
            "Text:   Qwen 1.5B + T5-Base → AdaptiveFusion\n",
            "Training: Separate optimizers (vision, text, shared, fusion)\n",
            "Loss: CE_vision + CE_text + CE_final + MSE_align\n",
            "================================================================================\n",
            "\n",
            "Device: cuda, Compile: True\n",
            "Vision: dinov3_large + dinov3_vitb16\n",
            "Text: qwen2.5_1.5b_instruct + t5_base\n",
            "\n",
            "--- Vision Caching ---\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_large_train.pt\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_vitb16_train.pt\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_large_test.pt\n",
            "Loading vision cache: encoder_cache/cifar100/dinov3_vitb16_test.pt\n",
            "Vision dims: 1536 + 768\n",
            "\n",
            "--- Text Caching ---\n",
            "QwenTextCacher: qwen2.5_1.5b_instruct (1536d)\n",
            "Loading Qwen cache: encoder_cache/cifar100/qwen2.5_1.5b_instruct_twoshot_class_embeddings.pt\n",
            "T5TextCacher: t5_base (768d)\n",
            "Loading T5 cache: encoder_cache/cifar100/t5_base_from_qwen_descriptions.pt\n",
            "Text dims: Qwen=1536, T5=768\n",
            "\n",
            "--- Building Router ---\n",
            "\n",
            "Total params: 42,764,421\n",
            "Optimizers: ['vision', 'text', 'shared', 'fusion']\n",
            "  vision: 14,003,406 params\n",
            "  text: 14,543,466 params\n",
            "  shared: 13,270,505 params\n",
            "  fusion: 947,044 params\n",
            "\n",
            "Pre-analyzing collectives and compiling...\n",
            "Analyzing collective structures...\n",
            "  ✓ All collectives analyzed\n",
            "  ✓ Router compiled\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Loss: CE_vision + CE_text + CE_final + 0.5×MSE_align\n",
            "LR: 0.001 → 1e-05 (cosine, warmup=5)\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.000200 | L: 11.680 (v=3.806 t=4.014 f=3.855 m=0.009) | Train: v=12.7 t=7.5 f=10.6 | Test: v=19.9%* t=11.2%^ f=18.7%! | 564.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.000400 | L: 9.905 (v=3.104 t=3.640 f=3.159 m=0.005) | Train: v=23.0 t=12.9 f=21.4 | Test: v=28.7%* t=15.9%^ f=28.3%! | 67.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.000600 | L: 9.060 (v=2.810 t=3.402 f=2.847 m=0.003) | Train: v=28.1 t=16.8 f=27.4 | Test: v=31.2%* t=18.8%^ f=30.8%! | 67.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.000800 | L: 8.609 (v=2.658 t=3.260 f=2.690 m=0.002) | Train: v=31.2 t=19.6 f=30.4 | Test: v=33.4%* t=21.0%^ f=32.8%! | 74.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.001000 | L: 8.367 (v=2.575 t=3.186 f=2.606 m=0.001) | Train: v=32.9 t=21.3 f=32.1 | Test: v=34.1%* t=22.6%^ f=33.0%! | 69.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.001000 | L: 7.991 (v=2.448 t=3.084 f=2.459 m=0.000) | Train: v=35.5 t=23.0 f=35.3 | Test: v=37.3%* t=24.6%^ f=37.2%! | 69.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.000999 | L: 7.610 (v=2.320 t=2.972 f=2.318 m=0.000) | Train: v=38.3 t=25.2 f=38.3 | Test: v=38.9%* t=26.3%^ f=37.6%! | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.000997 | L: 7.335 (v=2.229 t=2.893 f=2.213 m=0.000) | Train: v=40.2 t=26.9 f=40.6 | Test: v=40.8%* t=26.9%^ f=40.2%! | 69.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.000993 | L: 7.082 (v=2.147 t=2.822 f=2.113 m=0.000) | Train: v=42.1 t=28.4 f=42.8 | Test: v=42.6%* t=29.6%^ f=42.2%! | 68.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.000987 | L: 6.823 (v=2.059 t=2.750 f=2.014 m=0.000) | Train: v=44.0 t=29.7 f=44.7 | Test: v=42.2% t=29.4% f=42.0% | 71.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.000980 | L: 6.600 (v=1.989 t=2.684 f=1.927 m=0.000) | Train: v=45.7 t=31.2 f=46.9 | Test: v=43.6%* t=32.3%^ f=43.5%! | 68.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.000971 | L: 6.390 (v=1.919 t=2.623 f=1.848 m=0.000) | Train: v=47.3 t=32.3 f=48.6 | Test: v=46.0%* t=33.9%^ f=45.9%! | 67.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.000961 | L: 6.184 (v=1.855 t=2.571 f=1.759 m=0.000) | Train: v=48.6 t=33.4 f=50.5 | Test: v=45.9% t=33.8% f=45.7% | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.000949 | L: 6.023 (v=1.800 t=2.526 f=1.697 m=0.000) | Train: v=50.0 t=34.5 f=52.1 | Test: v=46.7%* t=34.0%^ f=46.7%! | 67.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.000936 | L: 5.827 (v=1.740 t=2.467 f=1.620 m=0.000) | Train: v=51.4 t=35.7 f=53.8 | Test: v=47.2%* t=36.0%^ f=46.7% | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.000921 | L: 5.655 (v=1.684 t=2.424 f=1.547 m=0.000) | Train: v=52.9 t=36.9 f=55.4 | Test: v=49.1%* t=38.0%^ f=48.0%! | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.000905 | L: 5.488 (v=1.630 t=2.375 f=1.483 m=0.000) | Train: v=54.1 t=38.0 f=57.2 | Test: v=48.7% t=37.5% f=48.6%! | 71.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.000888 | L: 5.329 (v=1.583 t=2.333 f=1.413 m=0.000) | Train: v=55.2 t=39.0 f=59.0 | Test: v=49.7%* t=39.1%^ f=49.0%! | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.000870 | L: 5.168 (v=1.531 t=2.287 f=1.350 m=0.000) | Train: v=56.4 t=39.9 f=60.5 | Test: v=49.8%* t=39.6%^ f=49.7%! | 67.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.000850 | L: 5.011 (v=1.485 t=2.248 f=1.279 m=0.000) | Train: v=57.5 t=40.9 f=62.5 | Test: v=50.5%* t=39.2% f=49.9%! | 67.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | lr=0.000829 | L: 4.867 (v=1.437 t=2.203 f=1.226 m=0.000) | Train: v=58.6 t=42.1 f=63.4 | Test: v=51.7%* t=41.4%^ f=51.5%! | 67.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | lr=0.000807 | L: 4.739 (v=1.398 t=2.171 f=1.171 m=0.000) | Train: v=59.7 t=43.1 f=65.2 | Test: v=52.1%* t=42.1%^ f=50.9% | 67.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | lr=0.000784 | L: 4.601 (v=1.353 t=2.134 f=1.114 m=0.000) | Train: v=61.0 t=44.0 f=66.7 | Test: v=52.3%* t=42.9%^ f=51.3% | 67.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | lr=0.000761 | L: 4.457 (v=1.306 t=2.097 f=1.054 m=0.000) | Train: v=62.2 t=44.6 f=68.0 | Test: v=52.2% t=43.2%^ f=51.1% | 71.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E25 | lr=0.000736 | L: 4.321 (v=1.262 t=2.060 f=0.999 m=0.000) | Train: v=63.1 t=45.8 f=69.6 | Test: v=53.0%* t=43.5%^ f=51.0% | 68.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E26 | lr=0.000711 | L: 4.210 (v=1.225 t=2.027 f=0.958 m=0.000) | Train: v=64.0 t=46.4 f=70.9 | Test: v=53.3%* t=45.1%^ f=52.2%! | 67.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E27 | lr=0.000685 | L: 4.084 (v=1.186 t=1.996 f=0.903 m=0.000) | Train: v=65.1 t=47.6 f=72.2 | Test: v=53.5%* t=45.9%^ f=53.0%! | 68.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E28 | lr=0.000658 | L: 3.952 (v=1.145 t=1.958 f=0.849 m=0.000) | Train: v=65.9 t=48.6 f=73.7 | Test: v=54.3%* t=46.6%^ f=52.7% | 67.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E29 | lr=0.000631 | L: 3.824 (v=1.100 t=1.919 f=0.805 m=0.000) | Train: v=67.5 t=49.4 f=75.0 | Test: v=54.3%* t=47.4%^ f=53.2%! | 68.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E30 | lr=0.000603 | L: 3.706 (v=1.061 t=1.885 f=0.760 m=0.000) | Train: v=68.3 t=50.2 f=76.1 | Test: v=54.1% t=46.6% f=53.2% | 69.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E31 | lr=0.000575 | L: 3.581 (v=1.021 t=1.849 f=0.712 m=0.000) | Train: v=69.5 t=51.4 f=77.6 | Test: v=54.9%* t=47.5%^ f=53.3%! | 72.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E32 | lr=0.000547 | L: 3.470 (v=0.979 t=1.818 f=0.672 m=0.000) | Train: v=70.6 t=52.1 f=78.7 | Test: v=54.6% t=47.6%^ f=53.5%! | 68.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2089902762.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2089902762.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    693\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'vision'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m         \u001b[0mtrain_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_adjacent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMSE_WEIGHT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    696\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mschedulers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mtest_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2089902762.py\u001b[0m in \u001b[0;36mtrain_epoch_adjacent\u001b[0;34m(router, loader, optimizers, device, mse_weight, max_grad_norm)\u001b[0m\n\u001b[1;32m    556\u001b[0m             \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    245\u001b[0m             )\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             adam(\n\u001b[0m\u001b[1;32m    248\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mmaybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mdisabled_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_fallback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    954\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_multi_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[0m\n\u001b[1;32m    611\u001b[0m     \u001b[0;31m# TODO: Support nonzero-dim Tensor betas, see #147921\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m     grouped_tensors = Optimizer._group_tensors_by_device_and_dtype(\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_steps\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: ignore[list-item]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_group_tensors_by_device_and_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_indices\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[return-value, arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_patch_step_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_foreach_utils.py\u001b[0m in \u001b[0;36m_group_tensors_by_device_and_dtype\u001b[0;34m(tensorlistlist, with_indices)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mwith_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m ) -> dict[tuple[torch.device, torch.dtype], tuple[TensorListList, Indices]]:\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_group_tensors_by_device_and_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensorlistlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# beatrix block 2 v7"
      ],
      "metadata": {
        "id": "xklzKU5GivaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "CIFAR-100 Dual-Pathway with Fusion Towers (WideRouter)\n",
        "======================================================\n",
        "\n",
        "FIXED:\n",
        "- No double-fusing. Use collective.fused directly.\n",
        "- Proper vision cache building with AutoProcessor\n",
        "- Experts at full strength (no dropout/scale)\n",
        "\n",
        "Vision Pathway (CE_v):\n",
        "    Expert 1: DINOv3-Large → proj → [B, dim]\n",
        "    Expert 2: ViT-B → proj → [B, dim]\n",
        "    Collectives: conv.fused + trans.fused + wide.fused → tower_fused\n",
        "    → Classifier([tower_fused, expert_1, expert_2]) = 3 opinions, 67% expert\n",
        "\n",
        "Text Pathway (CE_t):\n",
        "    Same structure → class embeddings [100, dim]\n",
        "    CLIP-style similarity with vision\n",
        "\n",
        "Fusion Pathway (MSE):\n",
        "    Vision + text → Fusion Collective → MSE alignment\n",
        "\n",
        "Optimizer: SGD(0.1) x30 → SGD(0.001) x10\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['TORCH_LOGS'] = '-all'\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch import Tensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from typing import Optional, Dict, List, Tuple\n",
        "from pathlib import Path\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch._dynamo as dynamo\n",
        "dynamo.config.verbose = False\n",
        "dynamo.config.suppress_errors = False\n",
        "\n",
        "from geofractal.router.wide_router import WideRouter\n",
        "from geofractal.router.components.fusion_component import AdaptiveFusion\n",
        "from geofractal.router.prefab.geometric_tower_builder import (\n",
        "    TowerConfig, build_tower_collective, preset_pos_neg_pairs,\n",
        ")\n",
        "from geofractal.router.prefab.geometric_conv_tower_builder import (\n",
        "    build_conv_collective, preset_conv_pos_neg,\n",
        ")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# CIFAR-100 CLASSES\n",
        "# =============================================================================\n",
        "\n",
        "CIFAR100_CLASSES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "CONV_TYPES = ['dilated', 'frequency', 'color_pattern', 'coarse_fine',\n",
        "              'bottleneck', 'squeeze_excite', 'inverted_bottleneck', 'spatial_attention']\n",
        "\n",
        "# HuggingFace paths for vision encoders\n",
        "VISION_HF_PATHS = {\n",
        "    'dinov3_large': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "    'dinov3_vitb16': 'facebook/dinov3-vitb16-pretrain-lvd1689m',\n",
        "    'dinov3_vitl16': 'facebook/dinov3-vitl16-pretrain-lvd1689m',\n",
        "}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TEXT CACHERS\n",
        "# =============================================================================\n",
        "\n",
        "class QwenTextCacher:\n",
        "    def __init__(self, model_name='qwen2.5_1.5b_instruct', cache_dir='./encoder_cache', device='cuda'):\n",
        "        self.model_name, self.cache_dir, self.device = model_name, Path(cache_dir), device\n",
        "        self.hf_path, self.dim = 'Qwen/Qwen2.5-1.5B-Instruct', 1536\n",
        "        print(f\"QwenTextCacher: {model_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self): return self.cache_dir / 'cifar100' / f\"{self.model_name}_twoshot_class_embeddings.pt\"\n",
        "    def _desc_path(self): return self.cache_dir / 'cifar100' / f\"{self.model_name}_twoshot_descriptions.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, force=False):\n",
        "        if self._cache_path().exists() and self._desc_path().exists() and not force:\n",
        "            print(f\"Loading Qwen cache: {self._cache_path()}\")\n",
        "            return torch.load(self._cache_path(), weights_only=True), torch.load(self._desc_path(), weights_only=True)\n",
        "\n",
        "        print(\"Building Qwen cache...\")\n",
        "        self._cache_path().parent.mkdir(parents=True, exist_ok=True)\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        tok = AutoTokenizer.from_pretrained(self.hf_path, trust_remote_code=True)\n",
        "        model = AutoModelForCausalLM.from_pretrained(self.hf_path, torch_dtype=torch.float16, device_map=self.device, trust_remote_code=True).eval()\n",
        "\n",
        "        def prompt(cls):\n",
        "            return tok.apply_chat_template([\n",
        "                {\"role\": \"system\", \"content\": \"You describe objects in exactly one sentence. Be specific about visual features.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Describe: car\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"A four-wheeled motor vehicle with windows, doors, headlights, and a metal body used for transportation on roads.\"},\n",
        "                {\"role\": \"user\", \"content\": \"Describe: sunflower\"},\n",
        "                {\"role\": \"assistant\", \"content\": \"A tall plant with a large circular flower head containing yellow petals surrounding a brown seed-filled center.\"},\n",
        "                {\"role\": \"user\", \"content\": f\"Describe: {cls.replace('_', ' ')}\"},\n",
        "            ], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        embs, descs = [], {}\n",
        "        for cls in tqdm(CIFAR100_CLASSES, desc=\"Qwen\"):\n",
        "            inp = tok(prompt(cls), return_tensors=\"pt\").to(self.device)\n",
        "            out = model.generate(**inp, max_new_tokens=50, do_sample=False, pad_token_id=tok.eos_token_id)\n",
        "            desc = tok.decode(out[0, inp['input_ids'].shape[1]:], skip_special_tokens=True).strip()\n",
        "            descs[cls] = desc\n",
        "            dinp = tok(desc, return_tensors=\"pt\").to(self.device)\n",
        "            dout = model(**dinp, output_hidden_states=True)\n",
        "            embs.append(dout.hidden_states[-1][0, -1, :].cpu().float())\n",
        "\n",
        "        embs = torch.stack(embs)\n",
        "        del model, tok; torch.cuda.empty_cache()\n",
        "        torch.save(embs, self._cache_path()); torch.save(descs, self._desc_path())\n",
        "        return embs, descs\n",
        "\n",
        "\n",
        "class T5TextCacher:\n",
        "    def __init__(self, model_name='t5_base', cache_dir='./encoder_cache', device='cuda'):\n",
        "        self.model_name, self.cache_dir, self.device = model_name, Path(cache_dir), device\n",
        "        self.hf_path, self.dim = 'google-t5/t5-base', 768\n",
        "        print(f\"T5TextCacher: {model_name} ({self.dim}d)\")\n",
        "\n",
        "    def _cache_path(self): return self.cache_dir / 'cifar100' / f\"{self.model_name}_from_qwen_descriptions.pt\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def build_cache(self, descs, force=False):\n",
        "        if self._cache_path().exists() and not force:\n",
        "            print(f\"Loading T5 cache: {self._cache_path()}\")\n",
        "            return torch.load(self._cache_path(), weights_only=True)\n",
        "\n",
        "        print(\"Building T5 cache...\")\n",
        "        from transformers import T5EncoderModel, T5Tokenizer\n",
        "        tok = T5Tokenizer.from_pretrained(self.hf_path)\n",
        "        model = T5EncoderModel.from_pretrained(self.hf_path, torch_dtype=torch.float16).to(self.device).eval()\n",
        "\n",
        "        embs = []\n",
        "        for cls in tqdm(CIFAR100_CLASSES, desc=\"T5\"):\n",
        "            inp = tok(descs[cls], return_tensors=\"pt\", max_length=128, truncation=True, padding='max_length').to(self.device)\n",
        "            out = model(**inp)\n",
        "            mask = inp['attention_mask'].unsqueeze(-1)\n",
        "            embs.append(((out.last_hidden_state * mask).sum(1) / mask.sum(1)).cpu().float().squeeze(0))\n",
        "\n",
        "        embs = torch.stack(embs)\n",
        "        del model, tok; torch.cuda.empty_cache()\n",
        "        torch.save(embs, self._cache_path())\n",
        "        return embs\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# VISION CACHING - FIXED WITH PROPER PROCESSOR\n",
        "# =============================================================================\n",
        "\n",
        "@torch.no_grad()\n",
        "def build_vision_cache(encoders, split, device='cuda'):\n",
        "    \"\"\"Build vision cache with proper processor handling.\"\"\"\n",
        "    from transformers import AutoModel, AutoProcessor\n",
        "\n",
        "    latents, dims = [], []\n",
        "    for enc in encoders:\n",
        "        path = Path('./encoder_cache/cifar100') / f\"{enc}_{split}.pt\"\n",
        "        if path.exists():\n",
        "            print(f\"Loading: {path}\")\n",
        "            lat = torch.load(path, weights_only=True)\n",
        "            latents.append(lat); dims.append(lat.shape[-1])\n",
        "            continue\n",
        "\n",
        "        print(f\"Building: {path}\")\n",
        "        path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Get HF path\n",
        "        hf_path = VISION_HF_PATHS.get(enc)\n",
        "        if not hf_path:\n",
        "            raise ValueError(f\"Unknown encoder: {enc}. Available: {list(VISION_HF_PATHS.keys())}\")\n",
        "\n",
        "        # Load model AND processor\n",
        "        model = AutoModel.from_pretrained(hf_path).to(device).eval()\n",
        "        processor = AutoProcessor.from_pretrained(hf_path)\n",
        "\n",
        "        dim = model.config.hidden_size\n",
        "        dims.append(dim)\n",
        "        print(f\"  {enc}: {sum(p.numel() for p in model.parameters()):,} params, dim={dim}\")\n",
        "\n",
        "        # Load dataset (NO transform - need PIL images for processor)\n",
        "        ds = datasets.CIFAR100('./data', train=(split=='train'), download=True)\n",
        "\n",
        "        all_feats = []\n",
        "        batch_size = 64\n",
        "\n",
        "        for i in tqdm(range(0, len(ds), batch_size), desc=f\"{enc} {split}\"):\n",
        "            # Get PIL images directly\n",
        "            pil_images = [ds[j][0] for j in range(i, min(i+batch_size, len(ds)))]\n",
        "\n",
        "            # Use processor for proper preprocessing\n",
        "            inputs = processor(images=pil_images, return_tensors='pt')\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            output = model(**inputs)\n",
        "            feats = output.pooler_output\n",
        "            all_feats.append(feats.cpu())\n",
        "\n",
        "        lat = torch.cat(all_feats, dim=0)\n",
        "        torch.save(lat, path)\n",
        "        latents.append(lat)\n",
        "\n",
        "        del model, processor\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"  ✓ Saved: {path}\")\n",
        "\n",
        "    return latents, dims\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET\n",
        "# =============================================================================\n",
        "\n",
        "class DualCachedDataset(Dataset):\n",
        "    def __init__(self, base, v1, v2):\n",
        "        self.base, self.v1, self.v2 = base, v1, v2\n",
        "    def __len__(self): return len(self.base)\n",
        "    def __getitem__(self, i):\n",
        "        img, lbl = self.base[i]\n",
        "        return img, self.v1[i], self.v2[i], lbl\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPONENTS\n",
        "# =============================================================================\n",
        "\n",
        "class ExpertProj(nn.Module):\n",
        "    \"\"\"Project expert latents - FULL STRENGTH (no dropout, no scale).\"\"\"\n",
        "    def __init__(self, in_dim, out_dim, bottleneck=128):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(in_dim),\n",
        "            nn.Linear(in_dim, bottleneck),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(bottleneck, out_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, dim, patch=4):\n",
        "        super().__init__()\n",
        "        self.n = (32 // patch) ** 2\n",
        "        self.proj = nn.Conv2d(3, dim, patch, patch)\n",
        "        self.pos = nn.Parameter(torch.randn(1, self.n, dim) * 0.02)\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "    def forward(self, x): return self.norm(self.proj(x).flatten(2).transpose(1,2) + self.pos)\n",
        "\n",
        "\n",
        "class TextExpander(nn.Module):\n",
        "    def __init__(self, dim, seq=8):\n",
        "        super().__init__()\n",
        "        self.seq, self.dim = seq, dim\n",
        "        self.proj = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim*seq), nn.GELU())\n",
        "        self.pos = nn.Parameter(torch.randn(1, seq, dim) * 0.02)\n",
        "        self.refine = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim))\n",
        "    def forward(self, x): return self.refine(self.proj(x).view(-1, self.seq, self.dim) + self.pos)\n",
        "\n",
        "\n",
        "class ThreeOpinionClassifier(nn.Module):\n",
        "    \"\"\"Classifier for 3 opinions: [tower_fused, expert_1, expert_2] - matches working model.\"\"\"\n",
        "    def __init__(self, dim, n_cls):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),\n",
        "            nn.Linear(dim * 3, dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(dim, n_cls),\n",
        "        )\n",
        "    def forward(self, tower_fused, expert_1, expert_2):\n",
        "        return self.net(torch.cat([tower_fused, expert_1, expert_2], dim=-1))\n",
        "\n",
        "\n",
        "class FusionProj(nn.Module):\n",
        "    def __init__(self, in_dim, dim, seq_len=8):\n",
        "        super().__init__()\n",
        "        self.seq_len, self.dim = seq_len, dim\n",
        "        self.proj = nn.Sequential(\n",
        "            nn.LayerNorm(in_dim),\n",
        "            nn.Linear(in_dim, dim * seq_len),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "        self.pos = nn.Parameter(torch.randn(1, seq_len, dim) * 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        return self.proj(x).view(B, self.seq_len, self.dim) + self.pos\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# DUAL-PATHWAY ROUTER - FIXED: NO DOUBLE FUSING\n",
        "# =============================================================================\n",
        "\n",
        "class DualPathwayFusionRouter(WideRouter):\n",
        "    \"\"\"\n",
        "    FIXED: Use collective.fused directly, no double-fusing.\n",
        "\n",
        "    Vision: conv.fused + trans.fused + wide.fused → tower_fused\n",
        "            [tower_fused, expert_1, expert_2] → Classifier (67% expert)\n",
        "\n",
        "    Text: Same structure → CLIP similarity\n",
        "\n",
        "    Fusion: 6 opinions → collective → MSE\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=256, patch=4, n_cls=100, vd1=1536, vd2=768, td1=1536, td2=768,\n",
        "                 text_qwen=None, text_t5=None):\n",
        "        super().__init__('dual_pathway_fusion', strict=False, auto_discover=False)\n",
        "        self.dim, self.n_cls = dim, n_cls\n",
        "        spatial = 32 // patch\n",
        "\n",
        "        self.register_buffer('text_qwen', text_qwen.float())\n",
        "        self.register_buffer('text_t5', text_t5.float())\n",
        "\n",
        "        # === VISION PATHWAY ===\n",
        "        # Experts: SEPARATE, FULL STRENGTH\n",
        "        self.attach('v_expert_1', ExpertProj(vd1, dim))\n",
        "        self.attach('v_expert_2', ExpertProj(vd2, dim))\n",
        "\n",
        "        # Patch embedding\n",
        "        self.attach('v_patch', PatchEmbed(dim, patch))\n",
        "\n",
        "        # Tower collectives (each fuses internally via AdaptiveFusion)\n",
        "        self.attach('v_conv', build_conv_collective(\n",
        "            preset_conv_pos_neg(CONV_TYPES), dim=dim, default_depth=2,\n",
        "            fingerprint_dim=64, spatial_size=spatial, name='v_conv'\n",
        "        ))\n",
        "\n",
        "        v_trans_cfg = preset_pos_neg_pairs(['beatrix', 'beatrix', 'beatrix', 'beatrix'])\n",
        "        for i, c in enumerate(v_trans_cfg):\n",
        "            c.name = f'beatrix_{i//2}_{[\"pos\",\"neg\"][i%2]}'\n",
        "        self.attach('v_trans', build_tower_collective(\n",
        "            v_trans_cfg, dim=dim, default_depth=2, num_heads=4,\n",
        "            fingerprint_dim=64, fusion_type='adaptive', name='v_trans'\n",
        "        ))\n",
        "\n",
        "        self.attach('v_wide', build_conv_collective(\n",
        "            preset_conv_pos_neg(['wide_resnet']), dim=dim, default_depth=2,\n",
        "            fingerprint_dim=64, spatial_size=spatial, name='v_wide'\n",
        "        ))\n",
        "\n",
        "        # Fuse 3 collective outputs → 1 tower opinion\n",
        "        self.attach('v_tower_fusion', AdaptiveFusion('v_tower_af', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # Classifier: 3 opinions [tower_fused, expert_1, expert_2] = 67% expert\n",
        "        self.attach('v_classifier', ThreeOpinionClassifier(dim, n_cls))\n",
        "\n",
        "        # === TEXT PATHWAY ===\n",
        "        self.attach('t_expert_1', ExpertProj(td1, dim))\n",
        "        self.attach('t_expert_2', ExpertProj(td2, dim))\n",
        "        self.attach('t_expand', TextExpander(dim))\n",
        "\n",
        "        self.attach('t_conv', build_conv_collective(\n",
        "            preset_conv_pos_neg(CONV_TYPES), dim=dim, default_depth=2,\n",
        "            fingerprint_dim=64, spatial_size=8, name='t_conv'\n",
        "        ))\n",
        "\n",
        "        t_trans_cfg = preset_pos_neg_pairs(['beatrix', 'beatrix', 'beatrix', 'beatrix'])\n",
        "        for i, c in enumerate(t_trans_cfg):\n",
        "            c.name = f'beatrix_{i//2}_{[\"pos\",\"neg\"][i%2]}'\n",
        "        self.attach('t_trans', build_tower_collective(\n",
        "            t_trans_cfg, dim=dim, default_depth=2, num_heads=4,\n",
        "            fingerprint_dim=64, fusion_type='adaptive', name='t_trans'\n",
        "        ))\n",
        "\n",
        "        self.attach('t_wide', build_conv_collective(\n",
        "            preset_conv_pos_neg(['wide_resnet']), dim=dim, default_depth=2,\n",
        "            fingerprint_dim=64, spatial_size=8, name='t_wide'\n",
        "        ))\n",
        "\n",
        "        # Fuse 3 collective outputs → 1 tower opinion\n",
        "        self.attach('t_tower_fusion', AdaptiveFusion('t_tower_af', num_inputs=3, in_features=dim))\n",
        "\n",
        "        # Text class projection: 3 opinions → class embedding\n",
        "        self.attach('t_class_proj', nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),\n",
        "            nn.Linear(dim * 3, dim),\n",
        "        ))\n",
        "\n",
        "        # Vision pooling for CLIP similarity\n",
        "        self.attach('v_pool_proj', nn.Sequential(\n",
        "            nn.LayerNorm(dim * 3),\n",
        "            nn.Linear(dim * 3, dim),\n",
        "        ))\n",
        "\n",
        "        # Learnable temperature\n",
        "        self.attach('logit_scale', nn.Parameter(torch.ones([]) * 2.6592))\n",
        "\n",
        "        # === FUSION PATHWAY ===\n",
        "        fusion_cfg = preset_pos_neg_pairs(['beatrix', 'beatrix', 'beatrix', 'beatrix'])\n",
        "        for i, c in enumerate(fusion_cfg):\n",
        "            c.name = f'fusion_beatrix_{i//2}_{[\"pos\",\"neg\"][i%2]}'\n",
        "        self.attach('f_trans', build_tower_collective(\n",
        "            fusion_cfg, dim=dim, default_depth=2, num_heads=4,\n",
        "            fingerprint_dim=64, fusion_type='adaptive', name='fusion_trans'\n",
        "        ))\n",
        "\n",
        "        self.attach('fusion_proj', FusionProj(6 * dim, dim, seq_len=8))\n",
        "        self.attach('v_aligner', nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim)))\n",
        "        self.attach('t_aligner', nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, dim)))\n",
        "\n",
        "        self._collective_names = ['v_conv', 'v_trans', 'v_wide', 't_conv', 't_trans', 't_wide', 'f_trans']\n",
        "\n",
        "    def analyze_all_collectives(self):\n",
        "        print(\"Analyzing collectives...\")\n",
        "        for name in self._collective_names:\n",
        "            collective = self[name]\n",
        "            if hasattr(collective, 'analyze_structure'):\n",
        "                collective.analyze_structure()\n",
        "                print(f\"  ✓ {name}: {len(collective.tower_names)} towers\")\n",
        "        print(\"  ✓ All collectives analyzed\")\n",
        "\n",
        "    def _build_text_class_embeddings(self, device):\n",
        "        \"\"\"Build class embeddings using collective FUSED outputs.\"\"\"\n",
        "        # Experts: FULL STRENGTH\n",
        "        t_exp_1 = self['t_expert_1'](self.text_qwen.to(device))  # [100, dim]\n",
        "        t_exp_2 = self['t_expert_2'](self.text_t5.to(device))    # [100, dim]\n",
        "\n",
        "        # Expand for towers\n",
        "        t_seq = self['t_expand'](t_exp_1)  # [100, 8, dim]\n",
        "\n",
        "        # Get collective FUSED outputs (no double-fusing!)\n",
        "        t_conv_fused, _ = self['t_conv'](t_seq)        # [100, dim]\n",
        "        t_trans_out = self['t_trans'](t_seq)\n",
        "        t_trans_fused = t_trans_out.fused              # [100, dim]\n",
        "        t_wide_fused, _ = self['t_wide'](t_seq)        # [100, dim]\n",
        "\n",
        "        # Fuse 3 collective outputs → 1 tower opinion\n",
        "        t_tower_fused = self['t_tower_fusion'](t_conv_fused, t_trans_fused, t_wide_fused)\n",
        "\n",
        "        # Combine 3 opinions → class embedding\n",
        "        t_combined = torch.cat([t_tower_fused, t_exp_1, t_exp_2], dim=-1)  # [100, 3*dim]\n",
        "        t_class_emb = self['t_class_proj'](t_combined)\n",
        "        t_class_emb = F.normalize(t_class_emb, dim=-1)\n",
        "\n",
        "        return t_class_emb, t_exp_1, t_exp_2, t_tower_fused\n",
        "\n",
        "    def forward(self, img, v1, v2, labels=None):\n",
        "        B, device = img.shape[0], img.device\n",
        "\n",
        "        # === VISION PATHWAY ===\n",
        "        # Experts: SEPARATE, FULL STRENGTH\n",
        "        v_exp_1 = self['v_expert_1'](v1)  # [B, dim]\n",
        "        v_exp_2 = self['v_expert_2'](v2)  # [B, dim]\n",
        "\n",
        "        # Patches\n",
        "        v_patches = self['v_patch'](img)  # [B, 64, dim]\n",
        "\n",
        "        # Get collective FUSED outputs (no double-fusing!)\n",
        "        v_conv_fused, _ = self['v_conv'](v_patches)     # [B, dim]\n",
        "        v_trans_out = self['v_trans'](v_patches)\n",
        "        v_trans_fused = v_trans_out.fused               # [B, dim]\n",
        "        v_wide_fused, _ = self['v_wide'](v_patches)     # [B, dim]\n",
        "\n",
        "        # Fuse 3 collective outputs → 1 tower opinion\n",
        "        v_tower_fused = self['v_tower_fusion'](v_conv_fused, v_trans_fused, v_wide_fused)\n",
        "\n",
        "        # Classifier: 3 opinions, 67% expert\n",
        "        v_logits = self['v_classifier'](v_tower_fused, v_exp_1, v_exp_2)\n",
        "\n",
        "        # === TEXT PATHWAY (CLIP-style) ===\n",
        "        t_class_emb, t_exp_1, t_exp_2, t_tower_fused = self._build_text_class_embeddings(device)\n",
        "\n",
        "        # Pool vision: 3 opinions → embedding\n",
        "        v_combined = torch.cat([v_tower_fused, v_exp_1, v_exp_2], dim=-1)\n",
        "        v_pooled = self['v_pool_proj'](v_combined)\n",
        "        v_pooled = F.normalize(v_pooled, dim=-1)\n",
        "\n",
        "        # CLIP similarity\n",
        "        logit_scale = self['logit_scale'].exp()\n",
        "        t_logits = logit_scale * (v_pooled @ t_class_emb.T)\n",
        "\n",
        "        # === FUSION PATHWAY ===\n",
        "        if labels is not None:\n",
        "            t_selected = t_class_emb[labels]\n",
        "        else:\n",
        "            t_selected = t_class_emb.mean(0, keepdim=True).expand(B, -1)\n",
        "\n",
        "        # 6 opinions for fusion: 3 vision + 3 text\n",
        "        v_opinions = [v_tower_fused, v_exp_1, v_exp_2]\n",
        "        t_opinions = [t_selected, t_selected, t_selected]\n",
        "\n",
        "        combined = torch.cat(v_opinions + t_opinions, dim=-1)\n",
        "        seq = self['fusion_proj'](combined)\n",
        "\n",
        "        # Use collective's built-in fusion\n",
        "        fusion_out = self['f_trans'](seq)\n",
        "        fusion_fused = fusion_out.fused\n",
        "\n",
        "        v_aligned = self['v_aligner'](fusion_fused)\n",
        "        t_aligned = self['t_aligner'](fusion_fused)\n",
        "\n",
        "        return {\n",
        "            'v_logits': v_logits,\n",
        "            't_logits': t_logits,\n",
        "            'v_aligned': v_aligned,\n",
        "            't_aligned': t_aligned,\n",
        "        }\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# TRAINING\n",
        "# =============================================================================\n",
        "\n",
        "def train_epoch(router, loader, opt, device, mse_w=0.5):\n",
        "    router.train()\n",
        "    m = {k: 0. for k in ['loss', 'ce_v', 'ce_t', 'mse', 'acc_v', 'acc_t', 'n']}\n",
        "\n",
        "    pbar = tqdm(loader, leave=False)\n",
        "    for img, v1, v2, lbl in pbar:\n",
        "        img, v1, v2, lbl = img.to(device), v1.to(device), v2.to(device), lbl.to(device)\n",
        "        B = lbl.size(0)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        out = router(img, v1, v2, lbl)\n",
        "\n",
        "        ce_v = F.cross_entropy(out['v_logits'], lbl)\n",
        "        ce_t = F.cross_entropy(out['t_logits'], lbl)\n",
        "        mse = F.mse_loss(out['v_aligned'], out['t_aligned'])\n",
        "\n",
        "        loss = ce_v + ce_t + mse_w * mse\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(router.parameters(), 1.0)\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            m['loss'] += loss.item() * B\n",
        "            m['ce_v'] += ce_v.item() * B\n",
        "            m['ce_t'] += ce_t.item() * B\n",
        "            m['mse'] += mse.item() * B\n",
        "            m['acc_v'] += (out['v_logits'].argmax(-1) == lbl).sum().item()\n",
        "            m['acc_t'] += (out['t_logits'].argmax(-1) == lbl).sum().item()\n",
        "            m['n'] += B\n",
        "\n",
        "        pbar.set_postfix({\n",
        "            'L': f\"{m['loss']/m['n']:.3f}\",\n",
        "            'v': f\"{100*m['acc_v']/m['n']:.1f}%\",\n",
        "            't': f\"{100*m['acc_t']/m['n']:.1f}%\"\n",
        "        })\n",
        "\n",
        "    n = m['n']\n",
        "    return {k: v/n for k, v in m.items() if k != 'n'}\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(router, loader, device):\n",
        "    router.eval()\n",
        "    cv, ct, n = 0, 0, 0\n",
        "    for img, v1, v2, lbl in loader:\n",
        "        img, v1, v2, lbl = img.to(device), v1.to(device), v2.to(device), lbl.to(device)\n",
        "        out = router(img, v1, v2)\n",
        "        cv += (out['v_logits'].argmax(-1) == lbl).sum().item()\n",
        "        ct += (out['t_logits'].argmax(-1) == lbl).sum().item()\n",
        "        n += lbl.size(0)\n",
        "    return {'acc_v': cv/n, 'acc_t': ct/n}\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# MAIN\n",
        "# =============================================================================\n",
        "\n",
        "def main():\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CIFAR-100 Dual-Pathway - FIXED: PROPER CACHING + NO DOUBLE FUSING\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Vision: conv.fused + trans.fused + wide.fused → tower_fused\")\n",
        "    print(\"        [tower_fused, expert_1, expert_2] → Classifier (67% expert)\")\n",
        "    print(\"Text:   Same structure → CLIP similarity → CE_t\")\n",
        "    print(\"Fusion: 6 opinions → collective.fused → MSE alignment\")\n",
        "    print(\"SGD: 0.1 x30 epochs → 0.001 x10 epochs\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    BATCH, DIM = 128, 256\n",
        "    EPOCHS_HIGH, EPOCHS_LOW = 30, 10\n",
        "    LR_HIGH, LR_LOW = 0.1, 0.001\n",
        "    MSE_W = 0.5\n",
        "    COMPILE = True\n",
        "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(f\"\\nDevice: {DEVICE}, Compile: {COMPILE}\")\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "\n",
        "    train_tf = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4), transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "    test_tf = transforms.Compose([\n",
        "        transforms.ToTensor(), transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR100('./data', train=True, download=True, transform=train_tf)\n",
        "    test_ds = datasets.CIFAR100('./data', train=False, download=True, transform=test_tf)\n",
        "\n",
        "    print(\"\\n--- Vision Caching ---\")\n",
        "    [tr_v1, tr_v2], [vd1, vd2] = build_vision_cache(['dinov3_large', 'dinov3_vitb16'], 'train', str(DEVICE))\n",
        "    [te_v1, te_v2], _ = build_vision_cache(['dinov3_large', 'dinov3_vitb16'], 'test', str(DEVICE))\n",
        "    print(f\"Vision dims: {vd1} + {vd2}\")\n",
        "\n",
        "    print(\"\\n--- Text Caching ---\")\n",
        "    qwen = QwenTextCacher(device=str(DEVICE))\n",
        "    t_qwen, descs = qwen.build_cache()\n",
        "    t5 = T5TextCacher(device=str(DEVICE))\n",
        "    t_t5 = t5.build_cache(descs)\n",
        "    print(f\"Text dims: {t_qwen.shape[-1]} + {t_t5.shape[-1]}\")\n",
        "\n",
        "    train_loader = DataLoader(DualCachedDataset(train_ds, tr_v1.float(), tr_v2.float()), BATCH, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(DualCachedDataset(test_ds, te_v1.float(), te_v2.float()), BATCH, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    print(\"\\n--- Building Router ---\")\n",
        "    router = DualPathwayFusionRouter(\n",
        "        dim=DIM, vd1=vd1, vd2=vd2, td1=t_qwen.shape[-1], td2=t_t5.shape[-1],\n",
        "        text_qwen=t_qwen.float(), text_t5=t_t5.float(),\n",
        "    )\n",
        "    router.network_to(device=DEVICE)\n",
        "\n",
        "    params = sum(p.numel() for p in router.parameters() if p.requires_grad)\n",
        "    print(f\"\\nTotal params: {params:,}\")\n",
        "    print(f\"Vision conv: {router['v_conv'].tower_names}\")\n",
        "    print(f\"Vision trans: {router['v_trans'].tower_names}\")\n",
        "    print(f\"Fusion trans: {router['f_trans'].tower_names}\")\n",
        "    print(f\"\\nExpert ratio: 2/3 = 67%\")\n",
        "    print(f\"Tower signal: conv.fused + trans.fused + wide.fused → AdaptiveFusion → 1 opinion\")\n",
        "\n",
        "    # Get optimizer BEFORE compile\n",
        "    opt = torch.optim.SGD(router.parameters(), lr=LR_HIGH, momentum=0.9, weight_decay=1e-4)\n",
        "\n",
        "    if COMPILE:\n",
        "        print(\"\\nAnalyzing and compiling...\")\n",
        "        router.analyze_all_collectives()\n",
        "        router = router.compile()\n",
        "        print(\"  ✓ Compiled\")\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 80)\n",
        "    print(f\"Phase 1: LR={LR_HIGH} for {EPOCHS_HIGH} epochs\")\n",
        "    print(f\"Phase 2: LR={LR_LOW} for {EPOCHS_LOW} epochs\")\n",
        "    print(f\"Loss: CE_v + CE_t + {MSE_W}×MSE\")\n",
        "    print(\"Markers: * = best vision, ^ = best text\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    best_v, best_t = 0., 0.\n",
        "\n",
        "    for epoch in range(1, EPOCHS_HIGH + EPOCHS_LOW + 1):\n",
        "        if epoch == EPOCHS_HIGH + 1:\n",
        "            print(f\"\\n*** LR DROP: {LR_HIGH} → {LR_LOW} ***\\n\")\n",
        "            for pg in opt.param_groups:\n",
        "                pg['lr'] = LR_LOW\n",
        "\n",
        "        t0 = time.time()\n",
        "        lr = opt.param_groups[0]['lr']\n",
        "\n",
        "        tr = train_epoch(router, train_loader, opt, DEVICE, MSE_W)\n",
        "        te = evaluate(router, test_loader, DEVICE)\n",
        "\n",
        "        is_v, is_t = te['acc_v'] > best_v, te['acc_t'] > best_t\n",
        "        best_v, best_t = max(best_v, te['acc_v']), max(best_t, te['acc_t'])\n",
        "\n",
        "        print(f\"E{epoch:02d} | lr={lr:.4f} | L: {tr['loss']:.3f} (v={tr['ce_v']:.3f} t={tr['ce_t']:.3f} m={tr['mse']:.3f}) | \"\n",
        "              f\"Train: v={100*tr['acc_v']:.1f} t={100*tr['acc_t']:.1f} | \"\n",
        "              f\"Test: v={100*te['acc_v']:.1f}%{'*' if is_v else ''} t={100*te['acc_t']:.1f}%{'^' if is_t else ''} | {time.time()-t0:.1f}s\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"Best Vision: {100*best_v:.2f}%\")\n",
        "    print(f\"Best Text:   {100*best_t:.2f}%\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XOzXtT1SiyqS",
        "outputId": "663d380b-9ee3-444b-f2ce-e3d47716d91e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CIFAR-100 Dual-Pathway - FIXED: PROPER CACHING + NO DOUBLE FUSING\n",
            "================================================================================\n",
            "Vision: conv.fused + trans.fused + wide.fused → tower_fused\n",
            "        [tower_fused, expert_1, expert_2] → Classifier (67% expert)\n",
            "Text:   Same structure → CLIP similarity → CE_t\n",
            "Fusion: 6 opinions → collective.fused → MSE alignment\n",
            "SGD: 0.1 x30 epochs → 0.001 x10 epochs\n",
            "================================================================================\n",
            "\n",
            "Device: cuda, Compile: True\n",
            "\n",
            "--- Vision Caching ---\n",
            "Loading: encoder_cache/cifar100/dinov3_large_train.pt\n",
            "Loading: encoder_cache/cifar100/dinov3_vitb16_train.pt\n",
            "Loading: encoder_cache/cifar100/dinov3_large_test.pt\n",
            "Loading: encoder_cache/cifar100/dinov3_vitb16_test.pt\n",
            "Vision dims: 1536 + 768\n",
            "\n",
            "--- Text Caching ---\n",
            "QwenTextCacher: qwen2.5_1.5b_instruct (1536d)\n",
            "Loading Qwen cache: encoder_cache/cifar100/qwen2.5_1.5b_instruct_twoshot_class_embeddings.pt\n",
            "T5TextCacher: t5_base (768d)\n",
            "Loading T5 cache: encoder_cache/cifar100/t5_base_from_qwen_descriptions.pt\n",
            "Text dims: 1536 + 768\n",
            "\n",
            "--- Building Router ---\n",
            "\n",
            "Total params: 89,811,997\n",
            "Vision conv: ['dilated_pos', 'dilated_neg', 'frequency_pos', 'frequency_neg', 'color_pattern_pos', 'color_pattern_neg', 'coarse_fine_pos', 'coarse_fine_neg', 'bottleneck_pos', 'bottleneck_neg', 'squeeze_excite_pos', 'squeeze_excite_neg', 'inverted_bottleneck_pos', 'inverted_bottleneck_neg', 'spatial_attention_pos', 'spatial_attention_neg']\n",
            "Vision trans: ['beatrix_0_pos', 'beatrix_0_neg', 'beatrix_1_pos', 'beatrix_1_neg', 'beatrix_2_pos', 'beatrix_2_neg', 'beatrix_3_pos', 'beatrix_3_neg']\n",
            "Fusion trans: ['fusion_beatrix_0_pos', 'fusion_beatrix_0_neg', 'fusion_beatrix_1_pos', 'fusion_beatrix_1_neg', 'fusion_beatrix_2_pos', 'fusion_beatrix_2_neg', 'fusion_beatrix_3_pos', 'fusion_beatrix_3_neg']\n",
            "\n",
            "Expert ratio: 2/3 = 67%\n",
            "Tower signal: conv.fused + trans.fused + wide.fused → AdaptiveFusion → 1 opinion\n",
            "\n",
            "Analyzing and compiling...\n",
            "Analyzing collectives...\n",
            "  ✓ v_conv: 16 towers\n",
            "  ✓ v_trans: 8 towers\n",
            "  ✓ v_wide: 2 towers\n",
            "  ✓ t_conv: 16 towers\n",
            "  ✓ t_trans: 8 towers\n",
            "  ✓ t_wide: 2 towers\n",
            "  ✓ f_trans: 8 towers\n",
            "  ✓ All collectives analyzed\n",
            "  ✓ Compiled\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Phase 1: LR=0.1 for 30 epochs\n",
            "Phase 2: LR=0.001 for 10 epochs\n",
            "Loss: CE_v + CE_t + 0.5×MSE\n",
            "Markers: * = best vision, ^ = best text\n",
            "--------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/391 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 390/391 [08:34<00:00,  3.35it/s, L=1.482, v=81.6%, t=82.8%]/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/lowering.py:7242: UserWarning: \n",
            "Online softmax is disabled on the fly since Inductor decides to\n",
            "split the reduction. Cut an issue to PyTorch if this is an\n",
            "important use case and you want to speed it up with online\n",
            "softmax.\n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E01 | lr=0.1000 | L: 1.480 (v=0.796 t=0.678 m=0.011) | Train: v=81.6 t=82.8 | Test: v=88.1%* t=87.6%^ | 1094.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E02 | lr=0.1000 | L: 0.685 (v=0.337 t=0.348 m=0.001) | Train: v=90.4 t=90.3 | Test: v=88.9%* t=88.6%^ | 128.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E03 | lr=0.1000 | L: 0.495 (v=0.248 t=0.247 m=0.001) | Train: v=92.8 t=92.9 | Test: v=88.5% t=89.2%^ | 121.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E04 | lr=0.1000 | L: 0.373 (v=0.188 t=0.185 m=0.000) | Train: v=94.4 t=94.8 | Test: v=89.3%* t=89.4%^ | 121.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E05 | lr=0.1000 | L: 0.289 (v=0.146 t=0.143 m=0.000) | Train: v=95.6 t=95.9 | Test: v=89.4%* t=89.5%^ | 127.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E06 | lr=0.1000 | L: 0.225 (v=0.112 t=0.113 m=0.000) | Train: v=96.6 t=96.9 | Test: v=89.7%* t=89.7%^ | 121.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E07 | lr=0.1000 | L: 0.183 (v=0.089 t=0.094 m=0.000) | Train: v=97.1 t=97.4 | Test: v=89.0% t=89.6% | 126.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E08 | lr=0.1000 | L: 0.154 (v=0.075 t=0.079 m=0.000) | Train: v=97.6 t=97.9 | Test: v=89.3% t=89.5% | 121.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E09 | lr=0.1000 | L: 0.131 (v=0.062 t=0.069 m=0.000) | Train: v=98.0 t=98.2 | Test: v=89.5% t=89.5% | 127.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E10 | lr=0.1000 | L: 0.110 (v=0.051 t=0.059 m=0.000) | Train: v=98.4 t=98.5 | Test: v=89.4% t=89.8%^ | 120.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E11 | lr=0.1000 | L: 0.085 (v=0.038 t=0.047 m=0.000) | Train: v=98.8 t=99.0 | Test: v=89.5% t=89.3% | 127.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E12 | lr=0.1000 | L: 0.075 (v=0.032 t=0.043 m=0.000) | Train: v=99.0 t=99.0 | Test: v=89.6% t=89.7% | 120.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E13 | lr=0.1000 | L: 0.062 (v=0.026 t=0.036 m=0.000) | Train: v=99.2 t=99.2 | Test: v=89.9%* t=89.8%^ | 120.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E14 | lr=0.1000 | L: 0.058 (v=0.024 t=0.034 m=0.000) | Train: v=99.3 t=99.3 | Test: v=89.6% t=89.6% | 126.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E15 | lr=0.1000 | L: 0.044 (v=0.018 t=0.027 m=0.000) | Train: v=99.5 t=99.4 | Test: v=89.8% t=89.6% | 120.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E16 | lr=0.1000 | L: 0.043 (v=0.017 t=0.026 m=0.000) | Train: v=99.5 t=99.5 | Test: v=89.9% t=89.8% | 126.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E17 | lr=0.1000 | L: 0.036 (v=0.013 t=0.022 m=0.000) | Train: v=99.6 t=99.5 | Test: v=89.9% t=89.8%^ | 120.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E18 | lr=0.1000 | L: 0.035 (v=0.013 t=0.022 m=0.000) | Train: v=99.6 t=99.6 | Test: v=89.8% t=89.9%^ | 121.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E19 | lr=0.1000 | L: 0.033 (v=0.011 t=0.021 m=0.000) | Train: v=99.7 t=99.6 | Test: v=89.7% t=89.5% | 126.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E20 | lr=0.1000 | L: 0.043 (v=0.016 t=0.026 m=0.000) | Train: v=99.5 t=99.5 | Test: v=89.6% t=89.6% | 120.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E21 | lr=0.1000 | L: 0.057 (v=0.022 t=0.034 m=0.000) | Train: v=99.3 t=99.2 | Test: v=89.4% t=89.5% | 126.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E22 | lr=0.1000 | L: 0.068 (v=0.028 t=0.040 m=0.000) | Train: v=99.2 t=99.1 | Test: v=89.1% t=88.9% | 120.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E23 | lr=0.1000 | L: 0.074 (v=0.030 t=0.044 m=0.000) | Train: v=99.1 t=99.0 | Test: v=89.7% t=89.5% | 126.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E24 | lr=0.1000 | L: 0.067 (v=0.027 t=0.040 m=0.000) | Train: v=99.2 t=99.1 | Test: v=89.3% t=89.6% | 120.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E25 | lr=0.1000 | L: 0.066 (v=0.026 t=0.040 m=0.000) | Train: v=99.2 t=99.1 | Test: v=89.6% t=89.5% | 120.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E26 | lr=0.1000 | L: 0.053 (v=0.021 t=0.032 m=0.000) | Train: v=99.4 t=99.3 | Test: v=89.7% t=89.8% | 126.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "E27 | lr=0.1000 | L: 0.058 (v=0.023 t=0.035 m=0.000) | Train: v=99.3 t=99.2 | Test: v=89.5% t=89.5% | 121.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-838315576.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-838315576.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE_W\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0mte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-838315576.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(router, loader, opt, device, mse_w)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mce_v\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mce_t\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmse_w\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rebuild cache"
      ],
      "metadata": {
        "id": "uJWvc-_mn5Rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "REBUILD VISION CACHE WITH PROPER PROCESSOR\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from transformers import AutoModel, AutoProcessor\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "def rebuild_cache(encoder_name, hf_path, split='train'):\n",
        "    cache_path = Path(f'./encoder_cache/cifar100/{encoder_name}_{split}.pt')\n",
        "    cache_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Rebuilding {encoder_name} {split} cache\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Load model AND processor\n",
        "    model = AutoModel.from_pretrained(hf_path).to(device).eval()\n",
        "    processor = AutoProcessor.from_pretrained(hf_path)\n",
        "\n",
        "    print(f\"Model: {sum(p.numel() for p in model.parameters()):,} params\")\n",
        "    print(f\"Processor: {type(processor)}\")\n",
        "\n",
        "    # Load dataset\n",
        "    ds = datasets.CIFAR100('./data', train=(split=='train'), download=True)\n",
        "\n",
        "    all_feats = []\n",
        "    batch_size = 64\n",
        "\n",
        "    for i in tqdm(range(0, len(ds), batch_size), desc=f\"{encoder_name} {split}\"):\n",
        "        # Get PIL images (not tensors!)\n",
        "        pil_images = [ds[j][0] for j in range(i, min(i+batch_size, len(ds)))]\n",
        "\n",
        "        # Use processor for proper preprocessing\n",
        "        inputs = processor(images=pil_images, return_tensors='pt')\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(**inputs)\n",
        "            # Use pooler_output\n",
        "            feats = output.pooler_output\n",
        "            all_feats.append(feats.cpu())\n",
        "\n",
        "    all_feats = torch.cat(all_feats, dim=0)\n",
        "    print(f\"Features: {all_feats.shape}\")\n",
        "\n",
        "    # Verify quality\n",
        "    feats_norm = F.normalize(all_feats[:100], dim=-1)\n",
        "    sim = (feats_norm @ feats_norm.T)[~torch.eye(100, dtype=bool)].mean()\n",
        "    print(f\"Inter-sample similarity (first 100): {sim:.4f}\")\n",
        "    print(f\"  (Should be < 0.5 for diverse features)\")\n",
        "\n",
        "    if sim > 0.9:\n",
        "        print(\"⚠️  WARNING: Features look collapsed!\")\n",
        "        return None\n",
        "\n",
        "    # Save\n",
        "    torch.save(all_feats, cache_path)\n",
        "    print(f\"✓ Saved: {cache_path}\")\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return all_feats\n",
        "\n",
        "# Rebuild all caches\n",
        "ENCODERS = {\n",
        "    'dinov3_large': 'facebook/dinov3-convnext-large-pretrain-lvd1689m',\n",
        "    'dinov3_vitb16': 'facebook/dinov3-vitb16-pretrain-lvd1689m',\n",
        "}\n",
        "\n",
        "for enc_name, hf_path in ENCODERS.items():\n",
        "    for split in ['train', 'test']:\n",
        "        rebuild_cache(enc_name, hf_path, split)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"CACHE REBUILD COMPLETE\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXx0v7o-iFvg",
        "outputId": "bedbc5bd-50fb-470d-cd00-6cf0597ddc4f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Rebuilding dinov3_large train cache\n",
            "============================================================\n",
            "Model: 196,230,336 params\n",
            "Processor: <class 'transformers.models.dinov3_vit.image_processing_dinov3_vit_fast.DINOv3ViTImageProcessorFast'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dinov3_large train: 100%|██████████| 782/782 [01:49<00:00,  7.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: torch.Size([50000, 1536])\n",
            "Inter-sample similarity (first 100): 0.1503\n",
            "  (Should be < 0.5 for diverse features)\n",
            "✓ Saved: encoder_cache/cifar100/dinov3_large_train.pt\n",
            "\n",
            "============================================================\n",
            "Rebuilding dinov3_large test cache\n",
            "============================================================\n",
            "Model: 196,230,336 params\n",
            "Processor: <class 'transformers.models.dinov3_vit.image_processing_dinov3_vit_fast.DINOv3ViTImageProcessorFast'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dinov3_large test: 100%|██████████| 157/157 [00:20<00:00,  7.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: torch.Size([10000, 1536])\n",
            "Inter-sample similarity (first 100): 0.1430\n",
            "  (Should be < 0.5 for diverse features)\n",
            "✓ Saved: encoder_cache/cifar100/dinov3_large_test.pt\n",
            "\n",
            "============================================================\n",
            "Rebuilding dinov3_vitb16 train cache\n",
            "============================================================\n",
            "Model: 85,660,416 params\n",
            "Processor: <class 'transformers.models.dinov3_vit.image_processing_dinov3_vit_fast.DINOv3ViTImageProcessorFast'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dinov3_vitb16 train: 100%|██████████| 782/782 [01:15<00:00, 10.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: torch.Size([50000, 768])\n",
            "Inter-sample similarity (first 100): 0.1051\n",
            "  (Should be < 0.5 for diverse features)\n",
            "✓ Saved: encoder_cache/cifar100/dinov3_vitb16_train.pt\n",
            "\n",
            "============================================================\n",
            "Rebuilding dinov3_vitb16 test cache\n",
            "============================================================\n",
            "Model: 85,660,416 params\n",
            "Processor: <class 'transformers.models.dinov3_vit.image_processing_dinov3_vit_fast.DINOv3ViTImageProcessorFast'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "dinov3_vitb16 test: 100%|██████████| 157/157 [00:15<00:00, 10.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: torch.Size([10000, 768])\n",
            "Inter-sample similarity (first 100): 0.0968\n",
            "  (Should be < 0.5 for diverse features)\n",
            "✓ Saved: encoder_cache/cifar100/dinov3_vitb16_test.pt\n",
            "\n",
            "============================================================\n",
            "CACHE REBUILD COMPLETE\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# next"
      ],
      "metadata": {
        "id": "spvnDgDljDpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# qwen tester"
      ],
      "metadata": {
        "id": "LpfgkAp7A5BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Qwen Two-Shot Generation → Encoding\n",
        "====================================\n",
        "1. Generate actual description of each class\n",
        "2. Encode the GENERATED text (the content, not the prompt)\n",
        "3. Compare semantic structure\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "CIFAR100_CLASSES = [\n",
        "    'apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle',\n",
        "    'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel',\n",
        "    'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock',\n",
        "    'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur',\n",
        "    'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster',\n",
        "    'house', 'kangaroo', 'keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion',\n",
        "    'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse',\n",
        "    'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear',\n",
        "    'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine',\n",
        "    'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea',\n",
        "    'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider',\n",
        "    'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank',\n",
        "    'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip',\n",
        "    'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm'\n",
        "]\n",
        "\n",
        "# =============================================================================\n",
        "# LOAD MODEL\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"Loading Qwen 2.5-1.5B-Instruct...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=DEVICE,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded on {DEVICE}\")\n",
        "\n",
        "# =============================================================================\n",
        "# TWO-SHOT PROMPT\n",
        "# =============================================================================\n",
        "\n",
        "def build_twoshot_prompt(class_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Two-shot prompt: show examples first, then ask for target.\n",
        "    Short, factual descriptions of what the thing IS.\n",
        "    \"\"\"\n",
        "    cls_name = class_name.replace('_', ' ')\n",
        "\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You describe objects in exactly one sentence. Be specific about visual features.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Describe: car\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"A four-wheeled motor vehicle with windows, doors, headlights, and a metal body used for transportation on roads.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Describe: sunflower\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": \"A tall plant with a large circular flower head containing yellow petals surrounding a brown seed-filled center.\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Describe: {cls_name}\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    return tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# GENERATE DESCRIPTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Generating descriptions (two-shot)...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "descriptions = {}\n",
        "\n",
        "for cls in tqdm(CIFAR100_CLASSES, desc=\"Generating\"):\n",
        "    prompt = build_twoshot_prompt(cls)\n",
        "\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=False,  # Deterministic\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    # Decode only the NEW tokens (the generation)\n",
        "    generated_ids = outputs[0, inputs['input_ids'].shape[1]:]\n",
        "    description = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    descriptions[cls] = description\n",
        "\n",
        "# Show some examples\n",
        "print(\"\\nSample descriptions:\")\n",
        "for cls in ['apple', 'lion', 'bus', 'oak_tree', 'rocket'][:5]:\n",
        "    print(f\"  {cls}: {descriptions[cls][:100]}...\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# ENCODE DESCRIPTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Encoding generated descriptions...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "@torch.no_grad()\n",
        "def encode_text(text: str) -> torch.Tensor:\n",
        "    \"\"\"Encode text using last token of hidden states.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(DEVICE)\n",
        "    outputs = model(**inputs, output_hidden_states=True)\n",
        "    # Last token of last layer\n",
        "    return outputs.hidden_states[-1][0, -1, :].cpu().float()\n",
        "\n",
        "\n",
        "embeddings_generated = []\n",
        "for cls in tqdm(CIFAR100_CLASSES, desc=\"Encoding\"):\n",
        "    emb = encode_text(descriptions[cls])\n",
        "    embeddings_generated.append(emb)\n",
        "\n",
        "E_generated = torch.stack(embeddings_generated)\n",
        "print(f\"Embeddings shape: {E_generated.shape}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# COMPARE: Last Token (prompt) vs Generated Description\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Comparison: Last Token (prompt) vs Generated Description\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load the last_token embeddings we made before\n",
        "E_last_token = torch.load('encoder_cache/cifar100/qwen2.5_1.5b_instruct_last_token_class_embeddings.pt')\n",
        "\n",
        "def analyze_embeddings(E: torch.Tensor, name: str):\n",
        "    E_norm = F.normalize(E, dim=-1)\n",
        "    sim = torch.mm(E_norm, E_norm.T)\n",
        "    mask = ~torch.eye(100, dtype=torch.bool)\n",
        "    off_diag = sim[mask]\n",
        "\n",
        "    # Effective rank\n",
        "    U, S, V = torch.svd(E)\n",
        "    S_norm = S / S.sum()\n",
        "    entropy = -(S_norm * torch.log(S_norm + 1e-10)).sum()\n",
        "    eff_rank = torch.exp(entropy).item()\n",
        "\n",
        "    print(f\"\\n{name}:\")\n",
        "    print(f\"  Mean sim: {off_diag.mean():.4f} ± {off_diag.std():.4f}\")\n",
        "    print(f\"  Range: [{off_diag.min():.4f}, {off_diag.max():.4f}]\")\n",
        "    print(f\"  Effective rank: {eff_rank:.1f}\")\n",
        "\n",
        "    return sim\n",
        "\n",
        "sim_last = analyze_embeddings(E_last_token, \"Last Token (prompt position)\")\n",
        "sim_gen = analyze_embeddings(E_generated, \"Generated Description\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SEMANTIC SANITY CHECK\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Semantic Sanity Check\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "def get_sim(sim_matrix, cls1, cls2):\n",
        "    i, j = CIFAR100_CLASSES.index(cls1), CIFAR100_CLASSES.index(cls2)\n",
        "    return sim_matrix[i, j].item()\n",
        "\n",
        "pairs = [\n",
        "    ('lion', 'tiger', 'should be HIGH'),\n",
        "    ('lion', 'pickup_truck', 'should be LOW'),\n",
        "    ('apple', 'orange', 'should be HIGH'),\n",
        "    ('apple', 'rocket', 'should be LOW'),\n",
        "    ('bus', 'train', 'should be HIGH'),\n",
        "    ('bus', 'bee', 'should be LOW'),\n",
        "    ('dolphin', 'whale', 'should be HIGH'),\n",
        "    ('dolphin', 'chair', 'should be LOW'),\n",
        "    ('oak_tree', 'pine_tree', 'should be HIGH'),\n",
        "    ('oak_tree', 'telephone', 'should be LOW'),\n",
        "]\n",
        "\n",
        "print(f\"\\n{'Pair':<25} {'Last Token':>12} {'Generated':>12} {'Expected':>15}\")\n",
        "print(\"─\" * 65)\n",
        "\n",
        "for cls1, cls2, expected in pairs:\n",
        "    s_last = get_sim(sim_last, cls1, cls2)\n",
        "    s_gen = get_sim(sim_gen, cls1, cls2)\n",
        "    delta = s_gen - s_last\n",
        "    marker = \"✓\" if (('HIGH' in expected and delta < 0) or ('LOW' in expected and delta < 0)) else \"\"\n",
        "    print(f\"{cls1} ↔ {cls2:<12} {s_last:>12.4f} {s_gen:>12.4f} {expected:>15}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# SEPARATION METRIC\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"Semantic Separation Score\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Calculate HIGH pairs avg vs LOW pairs avg\n",
        "high_pairs = [('lion', 'tiger'), ('apple', 'orange'), ('bus', 'train'),\n",
        "              ('dolphin', 'whale'), ('oak_tree', 'pine_tree')]\n",
        "low_pairs = [('lion', 'pickup_truck'), ('apple', 'rocket'), ('bus', 'bee'),\n",
        "             ('dolphin', 'chair'), ('oak_tree', 'telephone')]\n",
        "\n",
        "for name, sim in [(\"Last Token\", sim_last), (\"Generated\", sim_gen)]:\n",
        "    high_avg = sum(get_sim(sim, a, b) for a, b in high_pairs) / len(high_pairs)\n",
        "    low_avg = sum(get_sim(sim, a, b) for a, b in low_pairs) / len(low_pairs)\n",
        "    separation = high_avg - low_avg\n",
        "    print(f\"{name}: HIGH={high_avg:.4f}, LOW={low_avg:.4f}, separation={separation:.4f}\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALIZATION\n",
        "# =============================================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "\n",
        "# Similarity matrices\n",
        "for i, (name, sim) in enumerate([(\"Last Token\", sim_last), (\"Generated\", sim_gen)]):\n",
        "    ax = axes[0, i]\n",
        "    im = ax.imshow(sim.numpy(), cmap='RdBu_r', vmin=0.5, vmax=1.0)\n",
        "    ax.set_title(f'{name}\\nCosine Similarity')\n",
        "    plt.colorbar(im, ax=ax, fraction=0.046)\n",
        "\n",
        "# Similarity distributions\n",
        "ax = axes[0, 2]\n",
        "mask = ~torch.eye(100, dtype=torch.bool)\n",
        "ax.hist(sim_last[mask].numpy(), bins=50, alpha=0.5, label='Last Token', density=True)\n",
        "ax.hist(sim_gen[mask].numpy(), bins=50, alpha=0.5, label='Generated', density=True)\n",
        "ax.set_xlabel('Cosine Similarity')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Similarity Distribution')\n",
        "ax.legend()\n",
        "\n",
        "# t-SNE\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "for i, (name, E) in enumerate([(\"Last Token\", E_last_token), (\"Generated\", E_generated)]):\n",
        "    ax = axes[1, i]\n",
        "    tsne = TSNE(n_components=2, perplexity=15, random_state=42)\n",
        "    E_2d = tsne.fit_transform(E.numpy())\n",
        "\n",
        "    ax.scatter(E_2d[:, 0], E_2d[:, 1], s=30, alpha=0.7)\n",
        "    for j in range(0, 100, 10):\n",
        "        ax.annotate(CIFAR100_CLASSES[j], (E_2d[j, 0], E_2d[j, 1]), fontsize=6)\n",
        "    ax.set_title(f'{name}\\nt-SNE')\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "# Sample descriptions\n",
        "ax = axes[1, 2]\n",
        "ax.axis('off')\n",
        "sample_text = \"Sample Generated Descriptions:\\n\\n\"\n",
        "for cls in ['lion', 'apple', 'bus', 'dolphin', 'oak_tree']:\n",
        "    desc = descriptions[cls][:80] + \"...\" if len(descriptions[cls]) > 80 else descriptions[cls]\n",
        "    sample_text += f\"• {cls}: {desc}\\n\\n\"\n",
        "ax.text(0.05, 0.95, sample_text, transform=ax.transAxes, fontsize=9,\n",
        "        verticalalignment='top', fontfamily='monospace')\n",
        "ax.set_title('Generated Descriptions')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('qwen_twoshot_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Saved: qwen_twoshot_analysis.png\")\n",
        "\n",
        "# =============================================================================\n",
        "# CLEANUP\n",
        "# =============================================================================\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"✓ Model unloaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LhZwkjP3z203",
        "outputId": "b7e7e5f2-c871-4a2f-940b-1e19c7712aa7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "Loading Qwen 2.5-1.5B-Instruct...\n",
            "======================================================================\n",
            "Model loaded on cuda\n",
            "\n",
            "======================================================================\n",
            "Generating descriptions (two-shot)...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating:   0%|          | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Generating: 100%|██████████| 100/100 [01:36<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample descriptions:\n",
            "  apple: An edible fruit with a round shape, green skin that turns red when ripe, and contains seeds inside....\n",
            "  lion: A majestic feline with a muscular build, thick fur that can range from golden to black, sharp teeth,...\n",
            "  bus: A large enclosed vehicle designed to carry passengers along fixed routes or at regular intervals, ty...\n",
            "  oak_tree: An ancient tree with a sturdy trunk covered in rough bark, wide branches that spread out like arms, ...\n",
            "  rocket: A long cylindrical object propelled by thrust from its engine at the end, designed to travel through...\n",
            "\n",
            "======================================================================\n",
            "Encoding generated descriptions...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding: 100%|██████████| 100/100 [00:03<00:00, 27.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embeddings shape: torch.Size([100, 1536])\n",
            "\n",
            "======================================================================\n",
            "Comparison: Last Token (prompt) vs Generated Description\n",
            "======================================================================\n",
            "\n",
            "Last Token (prompt position):\n",
            "  Mean sim: 0.9189 ± 0.0242\n",
            "  Range: [0.8319, 0.9923]\n",
            "  Effective rank: 40.5\n",
            "\n",
            "Generated Description:\n",
            "  Mean sim: 0.8267 ± 0.1567\n",
            "  Range: [-0.0009, 0.9947]\n",
            "  Effective rank: 46.6\n",
            "\n",
            "======================================================================\n",
            "Semantic Sanity Check\n",
            "======================================================================\n",
            "\n",
            "Pair                        Last Token    Generated        Expected\n",
            "─────────────────────────────────────────────────────────────────\n",
            "lion ↔ tiger              0.9744       0.9501  should be HIGH\n",
            "lion ↔ pickup_truck       0.9089       0.8384   should be LOW\n",
            "apple ↔ orange             0.9774       0.8912  should be HIGH\n",
            "apple ↔ rocket             0.9218       0.8535   should be LOW\n",
            "bus ↔ train              0.9749       0.9461  should be HIGH\n",
            "bus ↔ bee                0.9215       0.8591   should be LOW\n",
            "dolphin ↔ whale              0.9520       0.9701  should be HIGH\n",
            "dolphin ↔ chair              0.9039       0.8466   should be LOW\n",
            "oak_tree ↔ pine_tree          0.9705       0.8121  should be HIGH\n",
            "oak_tree ↔ telephone          0.9136       0.8563   should be LOW\n",
            "\n",
            "======================================================================\n",
            "Semantic Separation Score\n",
            "======================================================================\n",
            "Last Token: HIGH=0.9698, LOW=0.9139, separation=0.0559\n",
            "Generated: HIGH=0.9139, LOW=0.8508, separation=0.0631\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Saved: qwen_twoshot_analysis.png\n",
            "✓ Model unloaded\n"
          ]
        }
      ]
    }
  ]
}