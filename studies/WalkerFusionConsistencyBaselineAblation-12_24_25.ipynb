{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXFr-Ioqkpeh",
        "outputId": "e37684b0-ace6-403e-c0db-f516a542980d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "============================================================\n",
            "CLASSIC BASELINES CONSISTENCY BENCHMARK\n",
            "============================================================\n",
            "Epochs: 20, Batch: 128, Runs: 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  self.setter(val)\n",
            "100%|██████████| 169M/169M [00:05<00:00, 29.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 50000, Test: 10000\n",
            "\n",
            "==================================================\n",
            "  MLP\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=13.08%\n",
            "      Epoch 10: acc=16.91%\n",
            "      Epoch 15: acc=19.62%\n",
            "      Epoch 20: acc=20.82%\n",
            "    -> Best: 20.83%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=13.73%\n",
            "      Epoch 10: acc=14.98%\n",
            "      Epoch 15: acc=18.24%\n",
            "      Epoch 20: acc=20.71%\n",
            "    -> Best: 20.71%\n",
            "  Mean: 20.77% ± 0.06% | Consistency: 0.994\n",
            "\n",
            "==================================================\n",
            "  SMALL_CNN\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=47.19%\n",
            "      Epoch 10: acc=55.97%\n",
            "      Epoch 15: acc=60.78%\n",
            "      Epoch 20: acc=62.40%\n",
            "    -> Best: 62.40%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=46.42%\n",
            "      Epoch 10: acc=56.37%\n",
            "      Epoch 15: acc=60.40%\n",
            "      Epoch 20: acc=61.52%\n",
            "    -> Best: 61.62%\n",
            "  Mean: 62.01% ± 0.39% | Consistency: 0.987\n",
            "\n",
            "==================================================\n",
            "  RESNET18\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=43.79%\n",
            "      Epoch 10: acc=56.45%\n",
            "      Epoch 15: acc=66.29%\n",
            "      Epoch 20: acc=72.99%\n",
            "    -> Best: 73.05%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=41.67%\n",
            "      Epoch 10: acc=54.71%\n",
            "      Epoch 15: acc=66.87%\n",
            "      Epoch 20: acc=72.87%\n",
            "    -> Best: 72.87%\n",
            "  Mean: 72.96% ± 0.09% | Consistency: 0.998\n",
            "\n",
            "==================================================\n",
            "  RESNET34\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=43.25%\n",
            "      Epoch 10: acc=57.26%\n",
            "      Epoch 15: acc=67.92%\n",
            "      Epoch 20: acc=73.64%\n",
            "    -> Best: 73.64%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=37.69%\n",
            "      Epoch 10: acc=57.58%\n",
            "      Epoch 15: acc=67.19%\n",
            "      Epoch 20: acc=73.38%\n",
            "    -> Best: 73.38%\n",
            "  Mean: 73.51% ± 0.13% | Consistency: 0.996\n",
            "\n",
            "==================================================\n",
            "  VIT_TINY\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "    ERROR: Unknown model (vit_tiny_patch4_32)\n",
            "  Run 2/2 (seed=1042)\n",
            "    ERROR: Unknown model (vit_tiny_patch4_32)\n",
            "  Mean: 0.00% ± 0.00% | Consistency: 0.000\n",
            "\n",
            "==================================================\n",
            "  CONVNEXT_TINY\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=8.07%\n",
            "      Epoch 10: acc=7.82%\n",
            "      Epoch 15: acc=7.27%\n",
            "      Epoch 20: acc=7.47%\n",
            "    -> Best: 8.51%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=7.99%\n",
            "      Epoch 10: acc=7.81%\n",
            "      Epoch 15: acc=7.03%\n",
            "      Epoch 20: acc=7.45%\n",
            "    -> Best: 8.26%\n",
            "  Mean: 8.38% ± 0.12% | Consistency: 0.971\n",
            "\n",
            "==================================================\n",
            "  EFFICIENTNET_B0\n",
            "==================================================\n",
            "  Run 1/2 (seed=42)\n",
            "      Epoch  5: acc=8.67%\n",
            "      Epoch 10: acc=13.43%\n",
            "      Epoch 15: acc=15.00%\n",
            "      Epoch 20: acc=15.06%\n",
            "    -> Best: 15.47%\n",
            "  Run 2/2 (seed=1042)\n",
            "      Epoch  5: acc=5.85%\n",
            "      Epoch 10: acc=10.73%\n",
            "      Epoch 15: acc=12.99%\n",
            "      Epoch 20: acc=13.41%\n",
            "    -> Best: 13.53%\n",
            "  Mean: 14.50% ± 0.97% | Consistency: 0.875\n",
            "\n",
            "============================================================\n",
            "RESULTS SUMMARY\n",
            "============================================================\n",
            "           name  params_M  best_mean  best_std  consistency\n",
            "       resnet34 21.328292     73.510     0.130     0.996469\n",
            "       resnet18 11.220132     72.960     0.090     0.997536\n",
            "      small_cnn  1.604196     62.010     0.390     0.987500\n",
            "            mlp  3.722852     20.770     0.060     0.994239\n",
            "efficientnet_b0  4.135648     14.500     0.970     0.874596\n",
            "  convnext_tiny 27.893572      8.385     0.125     0.970623\n",
            "       vit_tiny  0.000000      0.000     0.000     0.000000\n",
            "\n",
            "============================================================\n",
            "COMPARISON CONTEXT\n",
            "============================================================\n",
            "Walker fusion results (from combo ablation):\n",
            "  baseline_walker:      88.07% ± 0.59% | consistency: 0.987\n",
            "  combo_shiva_cosine:   88.62% ± 0.05% | consistency: 0.999\n",
            "  combo_shiva_learned:  88.64% ± 0.05% | consistency: 0.999\n",
            "\n",
            "Note: Walker uses frozen pretrained encoders (no training)\n",
            "      Baselines train from scratch\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Classic Baselines Consistency Benchmark\n",
        "# Industry-standard architectures on CIFAR-100\n",
        "# Same dual-run methodology for fair comparison\n",
        "# =========================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import timm\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from datetime import datetime\n",
        "from typing import Dict, Tuple, List\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -------------------------\n",
        "# Config\n",
        "# -------------------------\n",
        "RESULTS_FILE = f\"classic_baselines_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "EPOCHS = 20  # Standard training length\n",
        "BATCH_SIZE = 128\n",
        "NUM_RUNS = 2\n",
        "NUM_CLASSES = 100\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Data\n",
        "# -------------------------\n",
        "def get_cifar100_loaders():\n",
        "    \"\"\"Standard CIFAR-100 with basic augmentation.\"\"\"\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
        "    ])\n",
        "\n",
        "    train_ds = datasets.CIFAR100(\"./data\", train=True, download=True, transform=train_transform)\n",
        "    test_ds = datasets.CIFAR100(\"./data\", train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Models\n",
        "# -------------------------\n",
        "class SimpleMLP(nn.Module):\n",
        "    \"\"\"Basic MLP baseline.\"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(32 * 32 * 3, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(self.flatten(x))\n",
        "\n",
        "\n",
        "class SmallCNN(nn.Module):\n",
        "    \"\"\"Simple 4-layer CNN.\"\"\"\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 3, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "        )\n",
        "        self.classifier = nn.Linear(512, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "def get_resnet18_cifar():\n",
        "    \"\"\"ResNet-18 adapted for CIFAR (32x32).\"\"\"\n",
        "    model = timm.create_model('resnet18', pretrained=False, num_classes=100)\n",
        "    # Adapt for 32x32: smaller initial conv, no initial maxpool\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_resnet34_cifar():\n",
        "    \"\"\"ResNet-34 adapted for CIFAR.\"\"\"\n",
        "    model = timm.create_model('resnet34', pretrained=False, num_classes=100)\n",
        "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "    model.maxpool = nn.Identity()\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_vit_tiny_cifar():\n",
        "    \"\"\"ViT-Tiny for CIFAR-100.\"\"\"\n",
        "    model = timm.create_model(\n",
        "        'vit_tiny_patch4_32',  # 4x4 patches for 32x32 images\n",
        "        pretrained=False,\n",
        "        num_classes=100,\n",
        "        img_size=32,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_convnext_tiny_cifar():\n",
        "    \"\"\"ConvNeXt-Tiny adapted for CIFAR.\"\"\"\n",
        "    model = timm.create_model('convnext_tiny', pretrained=False, num_classes=100)\n",
        "    # Adapt stem for 32x32\n",
        "    model.stem[0] = nn.Conv2d(3, 96, kernel_size=2, stride=2)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_efficientnet_b0_cifar():\n",
        "    \"\"\"EfficientNet-B0 for CIFAR.\"\"\"\n",
        "    model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=100)\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_deit_tiny_cifar():\n",
        "    \"\"\"DeiT-Tiny for CIFAR-100.\"\"\"\n",
        "    model = timm.create_model(\n",
        "        'deit_tiny_patch16_224',\n",
        "        pretrained=False,\n",
        "        num_classes=100,\n",
        "        img_size=32,\n",
        "        patch_size=4,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "MODELS = {\n",
        "    'mlp': (SimpleMLP, {'lr': 1e-3, 'wd': 1e-4}),\n",
        "    'small_cnn': (SmallCNN, {'lr': 1e-3, 'wd': 1e-4}),\n",
        "    'resnet18': (get_resnet18_cifar, {'lr': 0.1, 'wd': 5e-4, 'scheduler': 'cosine'}),\n",
        "    'resnet34': (get_resnet34_cifar, {'lr': 0.1, 'wd': 5e-4, 'scheduler': 'cosine'}),\n",
        "    'vit_tiny': (get_vit_tiny_cifar, {'lr': 1e-3, 'wd': 0.05, 'scheduler': 'cosine'}),\n",
        "    'convnext_tiny': (get_convnext_tiny_cifar, {'lr': 1e-3, 'wd': 0.05, 'scheduler': 'cosine'}),\n",
        "    'efficientnet_b0': (get_efficientnet_b0_cifar, {'lr': 1e-3, 'wd': 1e-4, 'scheduler': 'cosine'}),\n",
        "}\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_single_run(\n",
        "    model_fn,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        "    config: Dict,\n",
        "    seed: int,\n",
        ") -> Tuple[float, float, List[float]]:\n",
        "    \"\"\"Single training run.\"\"\"\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "\n",
        "    model = model_fn().to(device)\n",
        "\n",
        "    # Count params\n",
        "    num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    # Optimizer\n",
        "    lr = config.get('lr', 1e-3)\n",
        "    wd = config.get('wd', 1e-4)\n",
        "\n",
        "    if config.get('scheduler') == 'cosine':\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=wd)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "    else:\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc = 0.0\n",
        "    epoch_accs = []\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # Train\n",
        "        model.train()\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(imgs)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # Eval\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in test_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                logits = model(imgs)\n",
        "                preds = logits.argmax(dim=-1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        acc = 100.0 * correct / total\n",
        "        epoch_accs.append(acc)\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "\n",
        "        if (epoch + 1) % 5 == 0:\n",
        "            print(f\"      Epoch {epoch+1:2d}: acc={acc:.2f}%\")\n",
        "\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return best_acc, epoch_accs[-1], num_params\n",
        "\n",
        "\n",
        "def benchmark_model(\n",
        "    name: str,\n",
        "    model_fn,\n",
        "    config: Dict,\n",
        "    train_loader,\n",
        "    test_loader,\n",
        ") -> Dict:\n",
        "    \"\"\"Run model multiple times for consistency check.\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"  {name.upper()}\")\n",
        "    print(f\"{'='*50}\")\n",
        "\n",
        "    run_bests = []\n",
        "    run_finals = []\n",
        "    num_params = 0\n",
        "\n",
        "    for run_idx in range(NUM_RUNS):\n",
        "        seed = 42 + run_idx * 1000\n",
        "        print(f\"  Run {run_idx+1}/{NUM_RUNS} (seed={seed})\")\n",
        "\n",
        "        try:\n",
        "            best_acc, final_acc, num_params = train_single_run(\n",
        "                model_fn, train_loader, test_loader, config, seed\n",
        "            )\n",
        "            run_bests.append(best_acc)\n",
        "            run_finals.append(final_acc)\n",
        "            print(f\"    -> Best: {best_acc:.2f}%\")\n",
        "        except Exception as e:\n",
        "            print(f\"    ERROR: {e}\")\n",
        "            run_bests.append(0.0)\n",
        "            run_finals.append(0.0)\n",
        "\n",
        "    # Stats\n",
        "    best_min, best_max = min(run_bests), max(run_bests)\n",
        "    consistency = best_min / best_max if best_max > 0 else 0\n",
        "    best_mean = sum(run_bests) / len(run_bests)\n",
        "    best_std = (sum((x - best_mean)**2 for x in run_bests) / len(run_bests)) ** 0.5\n",
        "\n",
        "    print(f\"  Mean: {best_mean:.2f}% ± {best_std:.2f}% | Consistency: {consistency:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'name': name,\n",
        "        'params': num_params,\n",
        "        'params_M': num_params / 1e6,\n",
        "        'best_mean': best_mean,\n",
        "        'best_std': best_std,\n",
        "        'best_min': best_min,\n",
        "        'best_max': best_max,\n",
        "        'consistency': consistency,\n",
        "        'run1_best': run_bests[0],\n",
        "        'run2_best': run_bests[1] if len(run_bests) > 1 else run_bests[0],\n",
        "        'final_mean': sum(run_finals) / len(run_finals),\n",
        "    }\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def run_baselines():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"CLASSIC BASELINES CONSISTENCY BENCHMARK\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Epochs: {EPOCHS}, Batch: {BATCH_SIZE}, Runs: {NUM_RUNS}\")\n",
        "\n",
        "    train_loader, test_loader = get_cifar100_loaders()\n",
        "    print(f\"Train: {len(train_loader.dataset)}, Test: {len(test_loader.dataset)}\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for name, (model_fn, config) in MODELS.items():\n",
        "        result = benchmark_model(name, model_fn, config, train_loader, test_loader)\n",
        "        results.append(result)\n",
        "\n",
        "    # Save\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(RESULTS_FILE, index=False)\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"RESULTS SUMMARY\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    df_sorted = df.sort_values('best_mean', ascending=False)\n",
        "    print(df_sorted[['name', 'params_M', 'best_mean', 'best_std', 'consistency']].to_string(index=False))\n",
        "\n",
        "    # Compare to walker fusion\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"COMPARISON CONTEXT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Walker fusion results (from combo ablation):\")\n",
        "    print(\"  baseline_walker:      88.07% ± 0.59% | consistency: 0.987\")\n",
        "    print(\"  combo_shiva_cosine:   88.62% ± 0.05% | consistency: 0.999\")\n",
        "    print(\"  combo_shiva_learned:  88.64% ± 0.05% | consistency: 0.999\")\n",
        "    print(\"\\nNote: Walker uses frozen pretrained encoders (no training)\")\n",
        "    print(\"      Baselines train from scratch\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results_df = run_baselines()"
      ]
    }
  ]
}